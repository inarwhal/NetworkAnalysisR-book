[["index.html", "Network Analysis: Integrating Social Network Theory, Method, and Application with R 1 Introduction 1.1 How to Read the Book 1.2 Citations and Use 1.3 Updates and Feedback 1.4 Acknowledgments 1.5 Author Informtion 1.6 Session Information", " Network Analysis: Integrating Social Network Theory, Method, and Application with R Craig Rawlings, Jeffrey A. Smith, James Moody, and Daniel McFarland 1 Introduction p.comment { padding: 10px; border: 1px solid black; margin-left: 25px; border-radius: 5px; } Welcome to the website for Network Analysis: Integrating Social Network Theory, Method, and Application with R. Here you will find the R tutorials that accompany the printed manuscript, which is available through Cambridge University Press. The printed manuscript offers substantive, theoretical and methodological discussions on how to conceptually conduct network analysis. The printed book thus offers the motivation and logic behind asking research questions from a network perspective. These tutorials serve as the practical counterpart, offering detailed examples on how to manipulate, visualize, summarize and analyze network data in R. The tutorials are motivated by substantive problems and include in-depth examples and interpretation. Many, but not all, of the examples are based on adolescents in school, as they serve as a familiar case study useful for drawing out larger, more general themes. 1.1 How to Read the Book The material on this website is meant to be paired with the printed manuscript. It is not an online version of the printed book. A reader would ideally read a chapter in the printed manuscript and then walk through the associated online R tutorials step-by-step. Readers may choose to go through each R tutorial in order or opt to cover specific topics of interest, depending on the goals and experience of the reader. Each tutorial is self-contained, so that more experienced readers could choose to cover the tutorials out of order. For those readers not strictly following the published book, it is important to remember that the R tutorials are numbered to coincide exactly with the chapters in the published manuscript. The book covers a wide range of topics related to network analysis. There are often multiple tutorials associated with a given topic. Topics include: Data Management Missing Data Visualization Ego Networks Dyads and Triads Cohesion and Communities Centrality Positions and Roles Affiliations and Duality Networks and Culture Statistical Network Models Diffusion Social Influence 1.2 Citations and Use You can cite the tutorials on this website as: Rawlings, Craig M., Jeffrey A. Smith, James Moody, and Daniel A. McFarland 2023. Network Analysis: Integrating Social Network Theory, Method, and Application with R. New York: Cambridge University Press. The online R tutorials, like the printed manuscript, is in copyright. No reproduction of any part may take place without the written permission of Cambridge University Press &amp; Assessment (© Craig M. Rawlings, Jeffrey A. Smith, James Moody, and Daniel A. McFarland, 2023). The tutorials are, however, made freely available through this site. 1.3 Updates and Feedback The authors are committed to keeping these chapters as up to date as possible, especially when there are major updates to key packages. The original version of the tutorials were completed and published in August 2023. It is possible that future versions of this online book will include additional tutorials on topics not currently covered. If you find errors or breaks in any of the code you can note them here: https://github.com/JeffreyAlanSmith/Integrated_Network_Science/issues or contact one of the authors directly. You can also find additional functions and data sets used throughout this book on the following github site: https://github.com/JeffreyAlanSmith/Integrated_Network_Science. 1.4 Acknowledgments There are many people we would like to thank for providing feedback and suggestions on these tutorials. Special thanks goes to Robin Gauthier, Sela Harcey and Julia McQuillan for their insightful comments and support, as well as graduate students Gabriel Varela, Tom Wolff, and Joe Quinn for reviews and beta testing. These tutorials have also been taught at various network analysis classes at Duke, Stanford and UNL. The advice and suggestions of our students have greatly strengthened the material presented here. Many of the R tutorials presented in this textbook were built off prior versions developed at Stanford University by Daniel McFarland, Solomon Messing, Michael Nowak, Sean J. Westwood, and Sanne Smith. Chapter 5’s tutorial for NDTV drew on Skye Bender-deMoll’s materials; Chapter 12 on LDA/CA from Love Börjeson and Daniel McFarland; Chapter 13 concerning “ERGM” and “relevant” drew on Carter Butts’ materials; Chapter 15 on SIENA/SAOM drew on ICS materials. Finally, a great many resources from the Duke Network Analysis Center (DNAC) helped us in formulating elements in many of the tutorials. For example, Chapter 4 on missing data imputation drew on DNAC/James Moody and Jeffrey A. Smith’s work, as did Chapter 14 on diffusion. We are grateful to these institutions and individuals for sharing code and helping us formulate applications for each chapter’s theories. 1.5 Author Informtion Craig M. Rawlings is Associate Professor of Sociology at Duke University, where he is affiliated with the Duke Network Analysis Center. His work focuses on the connections between social structures and culture, including belief systems, knowledge, meaning-making processes, and attitude change. His publications have appeared in the American Journal of Sociology, American Sociological Review, Social Forces, Sociological Science, and Poetics. Jeffrey A. Smith is Senior Policy Analyst in Mental Health and Addictions at the Nova Scotia Health Authority. He has done methodological work on network sampling and missing data, as well as more substantive work on network processes, drug use, and health outcomes. His work has been published in the American Sociological Review, Sociological Methodology, Social Networks, and other venues. James Moody is Professor of Sociology at Duke University and focuses on the network foundations of social cohesion and diffusion, using network analysis to help understand topics including racial segregation, disease spread, and the development of scientific disciplines. He has won the Freeman Award for contributions to network analysis and a James S. McDonnel Foundation Complexity Scholars award. Daniel A. McFarland is Professor of Education and (by courtesy) Sociology and Organizational Behavior at Stanford University, where he founded Stanford’s Center for Computational Social Science. His past work studied social network dynamics of communication, relationships, affiliations, and knowledge structures in educational contexts. His current work integrates social network analysis and natural language processing to study the development of scientific knowledge. 1.6 Session Information This version of the book was built using R version 4.3.0. See below for the session information: ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.3.0 (2023-04-21) ## os macOS Ventura 13.2.1 ## system x86_64, darwin20 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/Halifax ## date 2023-08-17 ## pandoc 2.11.4 @ /Applications/RStudio.app/Contents/MacOS/pandoc/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## abind 1.4-5 2016-07-21 [1] CRAN (R 4.3.0) ## animation 2.7 2021-10-07 [1] CRAN (R 4.3.0) ## ape 5.7-1 2023-03-13 [1] CRAN (R 4.3.0) ## askpass 1.1 2019-01-13 [1] CRAN (R 4.3.0) ## backports 1.4.1 2021-12-13 [1] CRAN (R 4.3.0) ## base64 2.0.1 2022-08-19 [1] CRAN (R 4.3.0) ## base64enc 0.1-3 2015-07-28 [1] CRAN (R 4.3.0) ## boot 1.3-28.1 2022-11-22 [1] CRAN (R 4.3.0) ## brio 1.1.3 2021-11-30 [1] CRAN (R 4.3.0) ## broom 1.0.5 2023-06-09 [1] CRAN (R 4.3.0) ## bslib 0.5.0 2023-06-09 [1] CRAN (R 4.3.0) ## cachem 1.0.8 2023-05-01 [1] CRAN (R 4.3.0) ## callr 3.7.3 2022-11-02 [1] CRAN (R 4.3.0) ## car 3.1-2 2023-03-30 [1] CRAN (R 4.3.0) ## carData 3.0-5 2022-01-06 [1] CRAN (R 4.3.0) ## chk 0.9.0 2023-05-27 [1] CRAN (R 4.3.0) ## cli 3.6.1 2023-03-23 [1] CRAN (R 4.3.0) ## coda 0.19-4 2020-09-30 [1] CRAN (R 4.3.0) ## codetools 0.2-19 2023-02-01 [1] CRAN (R 4.3.0) ## colorspace 2.1-0 2023-01-23 [1] CRAN (R 4.3.0) ## cpp11 0.4.3 2022-10-12 [1] CRAN (R 4.3.0) ## crayon 1.5.2 2022-09-29 [1] CRAN (R 4.3.0) ## curl 5.0.1 2023-06-07 [1] CRAN (R 4.3.0) ## DBI 1.1.3 2022-06-18 [1] CRAN (R 4.3.0) ## DEoptimR 1.0-14 2023-06-09 [1] CRAN (R 4.3.0) ## desc 1.4.2 2022-09-08 [1] CRAN (R 4.3.0) ## deSolve 1.35 2023-03-12 [1] CRAN (R 4.3.0) ## diffobj 0.3.5 2021-10-05 [1] CRAN (R 4.3.0) ## digest 0.6.31 2022-12-11 [1] CRAN (R 4.3.0) ## doParallel 1.0.17 2022-02-07 [1] CRAN (R 4.3.0) ## dplyr 1.1.2 2023-04-20 [1] CRAN (R 4.3.0) ## egor 1.23.3 2023-03-16 [1] CRAN (R 4.3.0) ## ellipsis 0.3.2 2021-04-29 [1] CRAN (R 4.3.0) ## EpiModel 2.4.0 2023-06-20 [1] CRAN (R 4.3.0) ## ergm 4.5.0 2023-05-28 [1] CRAN (R 4.3.0) ## ergm.count 4.1.1 2022-05-25 [1] CRAN (R 4.3.0) ## ergm.ego 1.1.0 2023-05-30 [1] CRAN (R 4.3.0) ## ergm.multi 0.2.0 2023-05-30 [1] CRAN (R 4.3.0) ## evaluate 0.21 2023-05-05 [1] CRAN (R 4.3.0) ## fansi 1.0.4 2023-01-22 [1] CRAN (R 4.3.0) ## farver 2.1.1 2022-07-06 [1] CRAN (R 4.3.0) ## fastmap 1.1.1 2023-02-24 [1] CRAN (R 4.3.0) ## fontawesome 0.5.1 2023-04-18 [1] CRAN (R 4.3.0) ## forcats 1.0.0 2023-01-29 [1] CRAN (R 4.3.0) ## foreach 1.5.2 2022-02-02 [1] CRAN (R 4.3.0) ## fs 1.6.2 2023-04-25 [1] CRAN (R 4.3.0) ## generics 0.1.3 2022-07-05 [1] CRAN (R 4.3.0) ## GGally 2.1.2 2021-06-21 [1] CRAN (R 4.3.0) ## ggnetwork 0.5.12 2023-03-06 [1] CRAN (R 4.3.0) ## ggplot2 3.4.2 2023-04-03 [1] CRAN (R 4.3.0) ## ggrepel 0.9.3 2023-02-03 [1] CRAN (R 4.3.0) ## glue 1.6.2 2022-02-24 [1] CRAN (R 4.3.0) ## gtable 0.3.3 2023-03-21 [1] CRAN (R 4.3.0) ## highr 0.10 2022-12-22 [1] CRAN (R 4.3.0) ## hms 1.1.3 2023-03-21 [1] CRAN (R 4.3.0) ## htmltools 0.5.5 2023-03-23 [1] CRAN (R 4.3.0) ## htmlwidgets 1.6.2 2023-03-17 [1] CRAN (R 4.3.0) ## igraph 1.5.0 2023-06-16 [1] CRAN (R 4.3.0) ## intergraph 2.0-2 2016-12-05 [1] CRAN (R 4.3.0) ## isoband 0.2.7 2022-12-20 [1] CRAN (R 4.3.0) ## iterators 1.0.14 2022-02-05 [1] CRAN (R 4.3.0) ## jquerylib 0.1.4 2021-04-26 [1] CRAN (R 4.3.0) ## jsonlite 1.8.5 2023-06-05 [1] CRAN (R 4.3.0) ## knitr 1.43 2023-05-25 [1] CRAN (R 4.3.0) ## labeling 0.4.2 2020-10-20 [1] CRAN (R 4.3.0) ## lattice 0.21-8 2023-04-05 [1] CRAN (R 4.3.0) ## lazyeval 0.2.2 2019-03-15 [1] CRAN (R 4.3.0) ## lifecycle 1.0.3 2022-10-07 [1] CRAN (R 4.3.0) ## lme4 1.1-33 2023-04-25 [1] CRAN (R 4.3.0) ## lpSolveAPI 5.5.2.0-17.9 2022-10-20 [1] CRAN (R 4.3.0) ## magick 2.7.4 2023-03-09 [1] CRAN (R 4.3.0) ## magrittr 2.0.3 2022-03-30 [1] CRAN (R 4.3.0) ## MASS 7.3-60 2023-05-04 [1] CRAN (R 4.3.0) ## MatchIt 4.5.4 2023-06-14 [1] CRAN (R 4.3.0) ## Matrix 1.5-4.1 2023-05-18 [1] CRAN (R 4.3.0) ## MatrixModels 0.5-1 2022-09-11 [1] CRAN (R 4.3.0) ## memoise 2.0.1 2021-11-26 [1] CRAN (R 4.3.0) ## mgcv 1.8-42 2023-03-02 [1] CRAN (R 4.3.0) ## mime 0.12 2021-09-28 [1] CRAN (R 4.3.0) ## minqa 1.2.5 2022-10-19 [1] CRAN (R 4.3.0) ## mitools 2.4 2019-04-26 [1] CRAN (R 4.3.0) ## munsell 0.5.0 2018-06-12 [1] CRAN (R 4.3.0) ## NbClust 3.0.1 2022-05-02 [1] CRAN (R 4.3.0) ## ndtv 0.13.3 2022-11-20 [1] CRAN (R 4.3.0) ## netdiffuseR 1.22.5 2022-12-02 [1] CRAN (R 4.3.0) ## network 1.18.1 2023-01-24 [1] CRAN (R 4.3.0) ## networkD3 0.4 2017-03-18 [1] CRAN (R 4.3.0) ## networkDynamic 0.11.3 2023-02-16 [1] CRAN (R 4.3.0) ## networkLite 1.0.5 2023-03-10 [1] CRAN (R 4.3.0) ## nlme 3.1-162 2023-01-31 [1] CRAN (R 4.3.0) ## nloptr 2.0.3 2022-05-26 [1] CRAN (R 4.3.0) ## nnet 7.3-19 2023-05-03 [1] CRAN (R 4.3.0) ## numDeriv 2016.8-1.1 2019-06-06 [1] CRAN (R 4.3.0) ## openssl 2.0.6 2023-03-09 [1] CRAN (R 4.3.0) ## pbkrtest 0.5.2 2023-01-19 [1] CRAN (R 4.3.0) ## pillar 1.9.0 2023-03-22 [1] CRAN (R 4.3.0) ## pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.3.0) ## pkgload 1.3.2 2022-11-16 [1] CRAN (R 4.3.0) ## plyr 1.8.8 2022-11-11 [1] CRAN (R 4.3.0) ## praise 1.0.0 2015-08-11 [1] CRAN (R 4.3.0) ## prettyunits 1.1.1 2020-01-24 [1] CRAN (R 4.3.0) ## processx 3.8.1 2023-04-18 [1] CRAN (R 4.3.0) ## progress 1.2.2 2019-05-16 [1] CRAN (R 4.3.0) ## ps 1.7.5 2023-04-18 [1] CRAN (R 4.3.0) ## purrr 1.0.1 2023-01-10 [1] CRAN (R 4.3.0) ## quantreg 5.95 2023-04-08 [1] CRAN (R 4.3.0) ## R6 2.5.1 2021-08-19 [1] CRAN (R 4.3.0) ## rappdirs 0.3.3 2021-01-31 [1] CRAN (R 4.3.0) ## rbibutils 2.2.13 2023-01-13 [1] CRAN (R 4.3.0) ## RColorBrewer 1.1-3 2022-04-03 [1] CRAN (R 4.3.0) ## Rcpp 1.0.10 2023-01-22 [1] CRAN (R 4.3.0) ## RcppArmadillo 0.12.4.1.0 2023-06-19 [1] CRAN (R 4.3.0) ## RcppEigen 0.3.3.9.3 2022-11-05 [1] CRAN (R 4.3.0) ## RcppProgress 0.4.2 2020-02-06 [1] CRAN (R 4.3.0) ## Rdpack 2.4 2022-07-20 [1] CRAN (R 4.3.0) ## relevent 1.2-1 2023-01-24 [1] CRAN (R 4.3.0) ## rematch2 2.1.2 2020-05-01 [1] CRAN (R 4.3.0) ## reshape 0.8.9 2022-04-12 [1] CRAN (R 4.3.0) ## rlang 1.1.1 2023-04-28 [1] CRAN (R 4.3.0) ## rle 0.9.2 2020-09-25 [1] CRAN (R 4.3.0) ## rmarkdown 2.22 2023-06-01 [1] CRAN (R 4.3.0) ## robustbase 0.99-0 2023-06-16 [1] CRAN (R 4.3.0) ## rprojroot 2.0.3 2022-04-02 [1] CRAN (R 4.3.0) ## RSiena 1.3.14.1 2023-02-05 [1] CRAN (R 4.3.0) ## sass 0.4.6 2023-05-03 [1] CRAN (R 4.3.0) ## scales 1.2.1 2022-08-20 [1] CRAN (R 4.3.0) ## sna 2.7-1 2023-01-24 [1] CRAN (R 4.3.0) ## SparseM 1.81 2021-02-18 [1] CRAN (R 4.3.0) ## srvyr 1.2.0 2023-02-21 [1] CRAN (R 4.3.0) ## statnet.common 4.9.0 2023-05-24 [1] CRAN (R 4.3.0) ## stringi 1.7.12 2023-01-11 [1] CRAN (R 4.3.0) ## stringr 1.5.0 2022-12-02 [1] CRAN (R 4.3.0) ## survey 4.2-1 2023-05-03 [1] CRAN (R 4.3.0) ## survival 3.5-5 2023-03-12 [1] CRAN (R 4.3.0) ## sys 3.4.2 2023-05-23 [1] CRAN (R 4.3.0) ## tergm 4.2.0 2023-05-30 [1] CRAN (R 4.3.0) ## testthat 3.1.9 2023-06-15 [1] CRAN (R 4.3.0) ## tibble 3.2.1 2023-03-20 [1] CRAN (R 4.3.0) ## tidygraph 1.2.3 2023-02-01 [1] CRAN (R 4.3.0) ## tidyr 1.3.0 2023-01-24 [1] CRAN (R 4.3.0) ## tidyselect 1.2.0 2022-10-10 [1] CRAN (R 4.3.0) ## tinytex 0.45 2023-04-18 [1] CRAN (R 4.3.0) ## trust 0.1-8 2020-01-10 [1] CRAN (R 4.3.0) ## utf8 1.2.3 2023-01-31 [1] CRAN (R 4.3.0) ## vctrs 0.6.3 2023-06-14 [1] CRAN (R 4.3.0) ## viridisLite 0.4.2 2023-05-02 [1] CRAN (R 4.3.0) ## waldo 0.5.1 2023-05-08 [1] CRAN (R 4.3.0) ## withr 2.5.0 2022-03-03 [1] CRAN (R 4.3.0) ## xfun 0.39 2023-04-20 [1] CRAN (R 4.3.0) ## xtable 1.8-4 2019-04-21 [1] CRAN (R 4.3.0) ## yaml 2.3.7 2023-01-23 [1] CRAN (R 4.3.0) ## ## [1] /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library ## ## ────────────────────────────────────────────────────────────────────────────── "],["ch2-intro-R.html", "2 Basic Introduction to R 2.1 Preliminaries 2.2 Packages", " 2 Basic Introduction to R This tutorial offers an introduction to the basics of R. R is a general platform for performing statistical and programming tasks. R offers a wide variety of network functionality, making it possible to do everything from simple network construction and graphing to sophisticated statistical models, all under one platform. Through a series of R tutorials, we will offer R code and examples for each major topic covered in the main text. In this tutorial, we offer a very brief overview of R and R packages, just enough to make sure that everyone can run and follow the tutorials that follow. Note that we do not offer a dedicated tutorial on R programming. All tutorials (except this one) are motivated by a problem related to network analysis. It will be useful to have some basic familiarity with R and its syntax (or be willing to learn as we go along!) but this is not strictly necessary. Each tutorial includes both R code and results, making it easier for readers to follow along, even without detailed knowledge of R. Readers interested in learning R in more depth should consider one of the many introductory R textbooks. 2.1 Preliminaries The first step is to gain access to R, which is free and available on the R website: http://cran.r-project.org/. Simply go to the R website, select the appropriate location and operating system, and follow the instructions to download the base distribution of R. RStudio offers a user friendly environment to run R and is recommended. Once R is opened we can begin to run commands. R commands can be run directly from the console, from the R script editor or from a text editor separate from R. For example, the following does a bit of simple math: 5 + 5 ## [1] 10 The two ## are printed for convenience, and indicate that everything that follows is a result shown in the R console. As another example, here we sum up a series of numbers using a sum() function. sum(5, 5, 4) ## [1] 14 R offers detailed help files for each function. To access help, type ?name_of_function. For example, for help on the sum() function, run: ?sum All lines proceeded by a # are comments and will not run. For example: # This is a comment. R will not recognize this as a command. 2.2 Packages The functionality of R is extended by loading packages that researchers have contributed to the R community. Packages are sets of useful functions that accomplish a set of tasks, such as graphing a network. In this book we will primarily make use of: igraph (Csardi and Nepusz 2006), sna (Butts 2023b), network (Butts 2015), networkDynamic (Butts et al. 2023), ndtv (Bender-deMoll 2022), egor (Krenz et al. 2023), ergm (Handcock et al. 2023), ergm.ego (Krivitsky 2023), ergm.count (Krivitsky 2022), tergm (Krivitsky and Handcock 2023), relevent (Butts 2023a), EpiModel (Jenness, Goodreau, and Morris 2018), netdiffuseR (Vega Yon and Valente 2022), and RSiena (Ripley et al. 2023). igraph, network and sna perform similar tasks related to network construction, graphing and network measurement. sna and network work in tandem while igraph is a fairly stand alone package. Note that igraph and sna/network construct networks in very different manners and there is little overlap in syntax. We will present the code for performing different calculations and tasks in both igraph and sna/network, where appropriate. The other packages, like ergm, tergm, and RSiena, estimate more advanced statistical models and we will cover them later in the book. Each package of interest must be installed and loaded before it can be used. The packages will not be immediately available when R is opened. A package only has to be installed once on a computer, but the package will have to be loaded every time R is restarted. To install all of the packages used in this book, run the following: source(&quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/R/setup.R&quot;) It is also possible to install each package individually as we need them. For example, to install igraph, sna, and network, we would do: install.packages(&quot;igraph&quot;) install.packages(&quot;sna&quot;) install.packages(&quot;network&quot;) Now that we have our packages successfully installed, we can go ahead and load them into R. Here we will load the network package as an example. We can use of all the functions available in that package once it is loaded into R. We load packages by using a library() function. The input is the name of the package, not in quotes. library(network) We can look up all of the functions within a package by using a help() function. For example, let’s look at the functions available in the network package. help(package = network) Note that the package argument is necessary to look up all of the functions. We can also detach a package if we no longer want it loaded. This is sometimes useful if two packages do not play well together. Here we will use a detach() function. detach(package:network) For simplicity, we will assume that the reader has restarted R at the beginning of each tutorial. This will ensure that the code will run without any complications. We will now turn to the basics of data management and network construction in R, covered in Chapter 3. "],["ch3-Network-Data-R.html", "3 Network Data", " 3 Network Data Chapter 3 covers the basics of data management for network data in R. There are two tutorials. The first tutorial covers cross-sectional network data while the second covers dynamic network data. "],["ch3-Cross-Sectional-Network-Data-R.html", "3, Part 1. Cross-Sectional Network Data 3.1 Reading Data into R 3.2 Networks in igraph 3.3 Using the network Package 3.4 Some Key Network Measures", " 3, Part 1. Cross-Sectional Network Data This tutorial covers the basics of dealing with network data in R, focusing on the case of cross-sectional network data (a network observed at one time point). We assume that the reader has installed R, as well as a few key packages. In this tutorial we will use igraph, network, reshape and intergraph. We will learn how to take actual network data, read it into R, and then use the data to construct networks. This will make it possible to plot, summarize and manipulate the network information in a useful manner. All subsequent chapters will assume some basic knowledge of core network construction functions in R. We will end the tutorial by going over how to calculate key summary measures, like density and distance, using the igraph package. These measures offer important building blocks for later chapters, where we will cover topics like diffusion, hierarchy and cohesion, which require a working knowledge of more basic network measures. The data we will use for this tutorial are based on classroom friendships collected by Daniel McFarland. The nodes, or vertices, are students in a classroom. There are 24 students. The survey asked each student to nominate classmates they \"hung out with\" as friends. The survey also asked each student about their gender, race and grade level. We thus have network data, based on the friendships between students, as well as node-level attribute data. 3.1 Reading Data into R The first step is to read the network data into R (it is possible, but not practical, to type out the data within R). In this case, we have stored the network data as a matrix, describing the friendships between each student in the class. The network is binary and directed. The matrix is stored as a CSV file called class555_matrix.csv. The attribute data for the students in this classroom is called class555_attributedata.csv. It contains data on the gender, race and grade for each student in the class. We have also created an edgelist for this network called class555_edgelist.csv. We begin by reading in the data matrix using a read.csv() function. The main argument to the function is file, set to the location of the file we want to read in. In this case, we will read in the file directly from a URL (we could also read in a file saved locally on a computer). The first line below simply defines the URL of interest (defined as url1), while the second line reads in the file using read.csv. url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class555_matrix.csv&quot; class_mat &lt;- read.csv(file = url1) We have now created an object called class_mat, which holds the information read in from the class555_matrix file. Before we take a closer look, let's clean things up a bit. Let's first check on the class of the object we created: class(class_mat) ## [1] &quot;data.frame&quot; We can see that class_mat is a data frame. Data frames are one kind of object in R, akin to traditional data sets that we may find in other programs like Stata or Excel. Data frames allow users to hold different kinds of columns (like numeric values and characters) all together in a single object. Data frames are, however, not conducive to matrix operations (like matrix multiplication). If we want to do matrix multiplication and related operations, we need to work with matrix objects, rather than data frames. Additionally, the main packages in R for network analysis take matrices as inputs. Given this, let's go ahead and turn our object, class_mat, into a matrix. We can turn our data frame into a matrix using the as.matrix() function: class_mat &lt;- as.matrix(class_mat) Let's also add a useful set of row names and column names to the matrix. This will make it easier to interpret the matrix when printed. We will set the row names to be 1 to the number of rows and the column names to be 1 to the number of columns. rownames(class_mat) &lt;- 1:nrow(class_mat) colnames(class_mat) &lt;- 1:ncol(class_mat) Now, let’s take a look at the matrix. Note that R will automatically print the object if no assignment is made, like this: class_mat ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ## 2 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 ## 4 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 7 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 ## 8 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ## 9 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 10 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 ## 11 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 ## 12 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 ## 13 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 14 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 ## 15 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 ## 16 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ## 17 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 ## 18 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 ## 19 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 ## 20 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 ## 21 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 ## 22 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ## 23 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 ## 24 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 We can see that the row names and column names run from 1 to 24, as we set above. The matrix itself consists of 0s and 1s, where a 1 means that i nominates j as a friend and 0 means that i did not nominate j as a friend, where i is the row and j is the column. For example, student 1 nominates students 3, 5, 7, and 21 as friends. Now, let's go ahead and read in the attribute data. Again, we read in the data directly using a URL, setting stringsAsFactors to T (this sets how non-numeric variables are read into R): url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class555_attributedata.csv&quot; class_attributes &lt;- read.csv(file = url2, stringsAsFactors = T) class_attributes ## id gender grade race ## 1 1 Male 12 White ## 2 2 Female 12 White ## 3 3 Female 12 White ## 4 4 Female 12 White ## 5 5 Male 12 White ## 6 6 Female 12 White ## 7 7 Male 11 Black ## 8 8 Male 11 White ## 9 9 Male 11 White ## 10 10 Female 11 White ## 11 11 Female 10 White ## 12 12 Female 10 White ## 13 13 Male 10 White ## 14 14 Male 10 White ## 15 15 Female 10 White ## 16 16 Male 10 White ## 17 17 Female 10 White ## 18 18 Female 10 White ## 19 19 Female 10 White ## 20 20 Female 10 White ## 21 21 Female 10 White ## 22 22 Female 10 White ## 23 23 Female 10 White ## 24 24 Female 10 White This is a simple data frame describing the gender, grade and race of each student in the network. The data frame thus holds the attributes of our nodes. Note that the order of the data frame has to be the same as the order in the matrix. Also note that the first column is the id of each node in the network (this must be consistent with the ids in the edgelist). We can grab particular columns by using a $ command or by calling a particular column in the data frame. Here we look at the first five values for gender: class_attributes$gender[1:5] ## [1] Male Female Female Female Male ## Levels: Female Male Same as above: class_attributes[1:5, &quot;gender&quot;] ## [1] Male Female Female Female Male ## Levels: Female Male The columns in a data frame can take different forms, or classes. For example, let's check on the class of the grade variable. class(class_attributes[, &quot;grade&quot;]) ## [1] &quot;integer&quot; The grade variable is an integer, meaning we can do mathematical operations on it. class_attributes[, &quot;grade&quot;] * 2 ## [1] 24 24 24 24 24 24 22 22 22 22 20 20 20 20 20 20 20 20 20 20 20 20 20 20 Note that our code above does not change the values of grade on the class_attributes data frame (as no assignment was made). If we had wanted to change the values we could do: class_attributes[, \"grade\"] &lt;- class_attributes[, \"grade\"] * 2 Now, let's check on the class of gender. class(class_attributes[, &quot;gender&quot;]) ## [1] &quot;factor&quot; gender is a factor, meaning it is a categorical (i.e., non-numeric) variable with an order to it. In this case Female is first and Male is second: levels(class_attributes[, &quot;gender&quot;]) ## [1] &quot;Female&quot; &quot;Male&quot; Factors do not represent meaningful numbers and thus we cannot do mathematical operations on them. Factors are still useful, however, as we can use them in analyses to make group comparisons (Female versus Male, for example). If we do not need (or want) to have the values ordered, we can turn our factors into characters using an as.character() function (or set stringsAsFactors to F when reading in the data). It is important to note that a matrix cannot hold numeric and character columns at the same time, as all columns must be of the same class (i.e., all numeric or all character). This is the main disadvantage of a matrix in R. And conversely, we can now see the main advantage of a data frame, that we can hold different kinds of columns in one object. 3.2 Networks in igraph Here we will use the igraph package to construct a network out of the friendship data. We will demonstrate how to do the same task using the network package below. 3.2.1 Using a Matrix to Construct the Network We begin by using the matrix representation of the network data. The advantage of turning our raw matrix into a network in the igraph (or network) format is that we can use all of the plotting, measure and statistical functions available within the various network packages. igraph, for example, offers a wide variety of functions to perform network calculations, plotting and simulations. We start by loading the igraph package (Csardi and Nepusz 2006). library(igraph) The function in igraph to create a network from a matrix is graph_from_adjacency_matrix(). The arguments are: adjmatrix = the input matrix mode = directed or undirected The output is an igraph object, which holds all of the network information defined by the input matrix. Here we will create an igraph object called class_netbymatrix using class_mat as input. class_netbymatrix &lt;- graph_from_adjacency_matrix(adjmatrix = class_mat, mode = &quot;directed&quot;) class_netbymatrix ## IGRAPH 59d7f6d DN-- 24 77 -- ## + attr: name (v/c) ## + edges from 59d7f6d (vertex names): ## [1] 1 -&gt;3 1 -&gt;5 1 -&gt;7 1 -&gt;21 2 -&gt;3 2 -&gt;6 3 -&gt;6 3 -&gt;8 3 -&gt;16 3 -&gt;24 ## [11] 4 -&gt;13 4 -&gt;18 7 -&gt;1 7 -&gt;9 7 -&gt;10 7 -&gt;16 8 -&gt;3 8 -&gt;9 8 -&gt;13 9 -&gt;5 ## [21] 9 -&gt;8 10-&gt;6 10-&gt;14 10-&gt;19 10-&gt;20 10-&gt;24 11-&gt;12 11-&gt;15 11-&gt;18 11-&gt;24 ## [31] 12-&gt;11 12-&gt;15 12-&gt;24 13-&gt;8 14-&gt;10 14-&gt;13 14-&gt;19 14-&gt;21 14-&gt;24 15-&gt;10 ## [41] 15-&gt;11 15-&gt;13 15-&gt;14 15-&gt;24 16-&gt;3 16-&gt;5 16-&gt;9 16-&gt;19 17-&gt;8 17-&gt;13 ## [51] 17-&gt;18 17-&gt;23 17-&gt;24 18-&gt;13 18-&gt;17 18-&gt;23 18-&gt;24 19-&gt;14 19-&gt;16 19-&gt;20 ## [61] 19-&gt;21 20-&gt;19 20-&gt;21 20-&gt;24 21-&gt;5 21-&gt;19 21-&gt;20 22-&gt;23 23-&gt;5 23-&gt;13 ## [71] 23-&gt;17 23-&gt;18 24-&gt;6 24-&gt;10 24-&gt;14 24-&gt;15 24-&gt;21 We can see that the igraph object contains lots of useful information, including the size of the network (24), the number of edges (77) and specific edge information. Now, let's map the node attributes we read in above, like gender and race, onto our igraph object. Here we need to use a set_vertex_attr() function. This makes it possible to take an attribute, like gender, and map it to the nodes in the network (so we will know what the gender of each node in the network is). The arguments are: graph = network of interest, as igraph object name = name of node attribute on network to be created value = a vector of node attributes Let's first do gender, adding an attribute called gender to the igraph object, equal to the gender values in class_attributes$gender. class_netbymatrix &lt;- set_vertex_attr(graph = class_netbymatrix, name = &quot;gender&quot;, value = class_attributes$gender) And now we add grade and race to the igraph object. class_netbymatrix &lt;- set_vertex_attr(graph = class_netbymatrix, name = &quot;grade&quot;, value = class_attributes$grade) class_netbymatrix &lt;- set_vertex_attr(graph = class_netbymatrix, name = &quot;race&quot;, value = class_attributes$race) class_netbymatrix ## IGRAPH 59d7f6d DN-- 24 77 -- ## + attr: name (v/c), gender (v/x), grade (v/n), race (v/x) ## + edges from 59d7f6d (vertex names): ## [1] 1 -&gt;3 1 -&gt;5 1 -&gt;7 1 -&gt;21 2 -&gt;3 2 -&gt;6 3 -&gt;6 3 -&gt;8 3 -&gt;16 3 -&gt;24 ## [11] 4 -&gt;13 4 -&gt;18 7 -&gt;1 7 -&gt;9 7 -&gt;10 7 -&gt;16 8 -&gt;3 8 -&gt;9 8 -&gt;13 9 -&gt;5 ## [21] 9 -&gt;8 10-&gt;6 10-&gt;14 10-&gt;19 10-&gt;20 10-&gt;24 11-&gt;12 11-&gt;15 11-&gt;18 11-&gt;24 ## [31] 12-&gt;11 12-&gt;15 12-&gt;24 13-&gt;8 14-&gt;10 14-&gt;13 14-&gt;19 14-&gt;21 14-&gt;24 15-&gt;10 ## [41] 15-&gt;11 15-&gt;13 15-&gt;14 15-&gt;24 16-&gt;3 16-&gt;5 16-&gt;9 16-&gt;19 17-&gt;8 17-&gt;13 ## [51] 17-&gt;18 17-&gt;23 17-&gt;24 18-&gt;13 18-&gt;17 18-&gt;23 18-&gt;24 19-&gt;14 19-&gt;16 19-&gt;20 ## [61] 19-&gt;21 20-&gt;19 20-&gt;21 20-&gt;24 21-&gt;5 21-&gt;19 21-&gt;20 22-&gt;23 23-&gt;5 23-&gt;13 ## [71] 23-&gt;17 23-&gt;18 24-&gt;6 24-&gt;10 24-&gt;14 24-&gt;15 24-&gt;21 We can see that we have now added gender, grade and race as node (or vertex) attributes to the igraph object (looking at the attr line in the output). At this point we have the basic network constructed and can start to analyze, plot, etc. 3.2.2 Using an Edgelist to Construct the Network Now we will do the same task of constructing a network (as an igraph object) but we will use an edgelist as the input data structure. Remember, an edgelist is a data set recording the edges in the network. The basic form is for the first column to be the sender of the tie and the second column to be the receiver. An edgelist contains all of the information of the matrix, except that it does not capture whether there are isolates (with no ties in and no ties out). The first task is to read in the edgelist. As before, we read in the file directly from a URL. url3 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class555_edgelist.csv&quot; class_edges &lt;- read.csv(file = url3) And now let's use the head() function to look at the first six rows of the edgelist. head(class_edges) ## sender receiver weight ## 1 1 3 1 ## 2 1 5 1 ## 3 1 7 1 ## 4 1 21 1 ## 5 2 3 1 ## 6 2 6 1 We see again that student 1 nominates student 3, 5, 7 and 21. Note that in many cases we will have edge attributes, or weights, capturing the strength or type of relationship between i and j. These can be easily represented by extra columns on the edgelist. Here we have included another column called weight. As the relation in this case is binary (friend or no friend) all the weights are the same across edges, equal to 1. In other cases the values would be different, capturing the strength of relationship between i and j. Now, we want to create an igraph object, as before, but this time use the edgelist as input. Here the function is graph_from_data_frame(). The arguments are: d = the edgelist; directed = T or F (true or false). vertices = optional data frame with vertex (i.e, node) attributes Here we create an igraph object called class_netbyedgelist based on the edgelist, class_edges. Note that we do not need to turn the edgelist into a matrix, as the function takes a data frame as input. class_netbyedgelist &lt;- graph_from_data_frame(d = class_edges, directed = T) class_netbyedgelist ## IGRAPH a725539 DNW- 24 77 -- ## + attr: name (v/c), weight (e/n) ## + edges from a725539 (vertex names): ## [1] 1 -&gt;3 1 -&gt;5 1 -&gt;7 1 -&gt;21 2 -&gt;3 2 -&gt;6 3 -&gt;6 3 -&gt;8 3 -&gt;16 3 -&gt;24 ## [11] 4 -&gt;13 4 -&gt;18 7 -&gt;1 7 -&gt;9 7 -&gt;10 7 -&gt;16 8 -&gt;3 8 -&gt;9 8 -&gt;13 9 -&gt;5 ## [21] 9 -&gt;8 10-&gt;6 10-&gt;14 10-&gt;19 10-&gt;20 10-&gt;24 11-&gt;12 11-&gt;15 11-&gt;18 11-&gt;24 ## [31] 12-&gt;11 12-&gt;15 12-&gt;24 13-&gt;8 14-&gt;10 14-&gt;13 14-&gt;19 14-&gt;21 14-&gt;24 15-&gt;10 ## [41] 15-&gt;11 15-&gt;13 15-&gt;14 15-&gt;24 16-&gt;3 16-&gt;5 16-&gt;9 16-&gt;19 17-&gt;8 17-&gt;13 ## [51] 17-&gt;18 17-&gt;23 17-&gt;24 18-&gt;13 18-&gt;17 18-&gt;23 18-&gt;24 19-&gt;14 19-&gt;16 19-&gt;20 ## [61] 19-&gt;21 20-&gt;19 20-&gt;21 20-&gt;24 21-&gt;5 21-&gt;19 21-&gt;20 22-&gt;23 23-&gt;5 23-&gt;13 ## [71] 23-&gt;17 23-&gt;18 24-&gt;6 24-&gt;10 24-&gt;14 24-&gt;15 24-&gt;21 As we can see, igraph will automatically add the edge attributes (here weight) to the igraph object. One advantage of using an edgelist is that it is easy to incorporate node attributes into the igraph object. This simply requires using a vertices argument when specifying the graph_from_data_frame() function. The input to vertices is a data frame of the attributes of each node, here class_attributes. igraph will add every column in the data frame (except the first column, which is assumed to be the ids of the nodes) to the created igraph object. The ids in the attribute data frame must correspond to the ids in the edgelist. Here we will go ahead and redo the command above using a vertices argument. class_netbyedgelist &lt;- graph_from_data_frame(d = class_edges, directed = T, vertices = class_attributes) class_netbyedgelist ## IGRAPH e77deaa DNW- 24 77 -- ## + attr: name (v/c), gender (v/c), grade (v/n), race (v/c), weight (e/n) ## + edges from e77deaa (vertex names): ## [1] 1 -&gt;3 1 -&gt;5 1 -&gt;7 1 -&gt;21 2 -&gt;3 2 -&gt;6 3 -&gt;6 3 -&gt;8 3 -&gt;16 3 -&gt;24 ## [11] 4 -&gt;13 4 -&gt;18 7 -&gt;1 7 -&gt;9 7 -&gt;10 7 -&gt;16 8 -&gt;3 8 -&gt;9 8 -&gt;13 9 -&gt;5 ## [21] 9 -&gt;8 10-&gt;6 10-&gt;14 10-&gt;19 10-&gt;20 10-&gt;24 11-&gt;12 11-&gt;15 11-&gt;18 11-&gt;24 ## [31] 12-&gt;11 12-&gt;15 12-&gt;24 13-&gt;8 14-&gt;10 14-&gt;13 14-&gt;19 14-&gt;21 14-&gt;24 15-&gt;10 ## [41] 15-&gt;11 15-&gt;13 15-&gt;14 15-&gt;24 16-&gt;3 16-&gt;5 16-&gt;9 16-&gt;19 17-&gt;8 17-&gt;13 ## [51] 17-&gt;18 17-&gt;23 17-&gt;24 18-&gt;13 18-&gt;17 18-&gt;23 18-&gt;24 19-&gt;14 19-&gt;16 19-&gt;20 ## [61] 19-&gt;21 20-&gt;19 20-&gt;21 20-&gt;24 21-&gt;5 21-&gt;19 21-&gt;20 22-&gt;23 23-&gt;5 23-&gt;13 ## [71] 23-&gt;17 23-&gt;18 24-&gt;6 24-&gt;10 24-&gt;14 24-&gt;15 24-&gt;21 The attributes will be added to the igraph object in the same order as in the input data frame (important when we extract information from the igraph object; see below). We can check the order of the nodes in our igraph object using: V(class_netbyedgelist)$name ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;13&quot; &quot;14&quot; &quot;15&quot; ## [16] &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; &quot;20&quot; &quot;21&quot; &quot;22&quot; &quot;23&quot; &quot;24&quot; We offer one final note on constructing networks from an edgelist. As discussed above, the edgelist does not contain information about isolates. If there are isolates in the network and we want to construct the network from an edgelist, we need to be careful. One simple solution is to include a vertices argument, with a data frame of node attributes. Even if no attributes are available, including a data frame of just ids will tell igraph how big the network really is, including isolates (who would be missed by the edgelist alone). For example, the following code creates a network with two isolates, with ids of 25 and 26. net &lt;- graph_from_data_frame(d = class_edges, directed = T, vertices = (id = 1:26)) plot(net) More generally, it is good practice to use a vertices argument when using graph_from_data_frame. This ensures that the order of the nodes (in the created object) is set in a clear manner; rather than allow the function to set the order, which can sometimes lead to unexpected results. 3.2.3 Extracting Information from igraph Objects Note that we can get node attributes, like grade, back out of the igraph object using a vertex_attr() function. This is useful as we no longer have to go back to the original data to grab the attributes of the nodes. Importantly, any action taken on the network (like removing isolates) would be reflected in the attributes attached to the network, unlike with the original data (i.e., any removed nodes would still be in the original data frame). This makes it easier to answer substantive questions about network structure and demographic characteristics. The basic arguments for vertex_attr() are graph and name (name of vertex attribute to extract). vertex_attr(graph = class_netbyedgelist, name = &quot;grade&quot;) ## [1] 12 12 12 12 12 12 11 11 11 11 10 10 10 10 10 10 10 10 10 10 10 10 10 10 We can extract the edge attribute, weight, from the igraph object using an edge_attr() function (with name set to the edge attribute of interest): weights &lt;- edge_attr(graph = class_netbyedgelist, name = \"weight\") We can also extract the edgelist and matrix, which is useful for a number of reasons. First, this makes it possible to extract an edgelist in a case where the input was a matrix (or vice versa). Second, the extracted edgelist or matrix will reflect any changes in the network (e.g., removing isolates). Similarly, certain functions will yield igraph objects as part of their output, and it is often useful to transform the igraph objects to matrices or edgelists for additional analyses. To get the edgelist we use an as_edgelist() function. class_edges_temp &lt;- as_edgelist(graph = class_netbyedgelist, names = F) Here, we take a look at the first six rows of the edgelist: head(class_edges_temp) ## [,1] [,2] ## [1,] 1 3 ## [2,] 1 5 ## [3,] 1 7 ## [4,] 1 21 ## [5,] 2 3 ## [6,] 2 6 The matrix can be extracted using the as_adjacency_matrix() function. as_adjacency_matrix(graph = class_netbyedgelist) ## 24 x 24 sparse Matrix of class &quot;dgCMatrix&quot; ## ## 1 . . 1 . 1 . 1 . . . . . . . . . . . . . 1 . . . ## 2 . . 1 . . 1 . . . . . . . . . . . . . . . . . . ## 3 . . . . . 1 . 1 . . . . . . . 1 . . . . . . . 1 ## 4 . . . . . . . . . . . . 1 . . . . 1 . . . . . . ## 5 . . . . . . . . . . . . . . . . . . . . . . . . ## 6 . . . . . . . . . . . . . . . . . . . . . . . . ## 7 1 . . . . . . . 1 1 . . . . . 1 . . . . . . . . ## 8 . . 1 . . . . . 1 . . . 1 . . . . . . . . . . . ## 9 . . . . 1 . . 1 . . . . . . . . . . . . . . . . ## 10 . . . . . 1 . . . . . . . 1 . . . . 1 1 . . . 1 ## 11 . . . . . . . . . . . 1 . . 1 . . 1 . . . . . 1 ## 12 . . . . . . . . . . 1 . . . 1 . . . . . . . . 1 ## 13 . . . . . . . 1 . . . . . . . . . . . . . . . . ## 14 . . . . . . . . . 1 . . 1 . . . . . 1 . 1 . . 1 ## 15 . . . . . . . . . 1 1 . 1 1 . . . . . . . . . 1 ## 16 . . 1 . 1 . . . 1 . . . . . . . . . 1 . . . . . ## 17 . . . . . . . 1 . . . . 1 . . . . 1 . . . . 1 1 ## 18 . . . . . . . . . . . . 1 . . . 1 . . . . . 1 1 ## 19 . . . . . . . . . . . . . 1 . 1 . . . 1 1 . . . ## 20 . . . . . . . . . . . . . . . . . . 1 . 1 . . 1 ## 21 . . . . 1 . . . . . . . . . . . . . 1 1 . . . . ## 22 . . . . . . . . . . . . . . . . . . . . . . 1 . ## 23 . . . . 1 . . . . . . . 1 . . . 1 1 . . . . . . ## 24 . . . . . 1 . . . 1 . . . 1 1 . . . . . 1 . . . Note that the matrix is outputted as a sparse matrix by default, but this behavior can be changed by setting sparse = F. 3.2.4 Using an Adjacency List to Construct the Network As another example, we will walk through how to work with network data stored as an adjacency list. Here, each node is on the rows and the columns capture who that node sends ties to. This is often how survey data will look in its raw form. Let's read in the adjacency list associated with our example network from above: url4 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class555_adjacency_list.csv&quot; class_adjacency &lt;- read.csv(file = url4) class_adjacency ## id Nomination1 Nomination2 Nomination3 Nomination4 Nomination5 ## 1 1 3 5 7 21 NA ## 2 2 3 6 NA NA NA ## 3 3 6 8 16 24 NA ## 4 4 13 18 NA NA NA ## 5 5 NA NA NA NA NA ## 6 6 NA NA NA NA NA ## 7 7 1 9 10 16 NA ## 8 8 3 9 13 NA NA ## 9 9 5 8 NA NA NA ## 10 10 6 14 19 20 24 ## 11 11 12 15 18 24 NA ## 12 12 11 15 24 NA NA ## 13 13 8 NA NA NA NA ## 14 14 10 13 19 21 24 ## 15 15 10 11 13 14 24 ## 16 16 3 5 9 19 NA ## 17 17 8 13 18 23 24 ## 18 18 13 17 23 24 NA ## 19 19 14 16 20 21 NA ## 20 20 19 21 24 NA NA ## 21 21 5 19 20 NA NA ## 22 22 23 NA NA NA NA ## 23 23 5 13 17 18 NA ## 24 24 6 10 14 15 21 The adjacency list has 24 rows, one for each node, and 6 columns. The first column shows the id of the node (here students) and the remaining columns capture who they name as friends. Looking at the first row, we again see that node 1 nominates 3, 5, 7 and 21 as friends. It is often useful to transform our adjacency list into an edgelist (or matrix), as it is easy to construct a network from an edgelist but somewhat more difficult using an adjacency list (although see the function graph_from_adj_list()). Here we will use functions from the reshape package to turn our adjacency list into an edgelist. Let's begin by loading the reshape package. library(reshape) We first need to identify the columns in the adjacency list that correspond to the nomination data. In this case, the columns of interest are: Nomination1, Nomination2,...Nomination5. These are the columns that we need to stack together to form the receiver column in the edgelist. Here, we will use a paste command to create a vector of column names corresponding to the nomination columns. nomination_columns &lt;- paste(&quot;Nomination&quot;, 1:5, sep = &quot;&quot;) nomination_columns ## [1] &quot;Nomination1&quot; &quot;Nomination2&quot; &quot;Nomination3&quot; &quot;Nomination4&quot; &quot;Nomination5&quot; Now we use a reshape() function to turn our data from a 'wide' format to a 'long' format. We thus move from having the nodes as rows to having edges as rows. The basic idea is take the id variable and repeat it (forming the sender column), while stacking the nomination data to form the receiver column. In the reshape() function, we set data to the adjacency list created above (class_adjacency); varying to the columns to stack (nomination_columns); v.names to the name of the variable to be created in the long format; idvar to the id variable for the nodes (id); and direction to long. class_edgelist_byadjacency &lt;- reshape(data = class_adjacency, varying = nomination_columns, v.names = &quot;receiver&quot;, idvar = &quot;id&quot;, direction = &quot;long&quot;) head(class_edgelist_byadjacency) ## id time receiver ## 1.1 1 1 3 ## 2.1 2 1 3 ## 3.1 3 1 6 ## 4.1 4 1 13 ## 5.1 5 1 NA ## 6.1 6 1 NA We can see that the data is beginning to look like an edgelist (running long rather than wide) but there is still some cleaning up to do before we can actually use it. For example, we don't really need that second column (showing the nomination column), so we want to remove it. We also want to add a better set of column names to the data. class_edgelist_byadjacency &lt;- class_edgelist_byadjacency[, -2] colnames(class_edgelist_byadjacency) &lt;- c(&quot;sender&quot;, &quot;receiver&quot;) We also need to take out any NAs in the data, which were simply copied over from the adjacency list. We can use a complete.cases() function to only keep those rows where we have no NA values. which_keep &lt;- complete.cases(class_edgelist_byadjacency) class_edgelist_byadjacency &lt;- class_edgelist_byadjacency[which_keep, ] And let's also reorder the data (although this is not strictly necessary) to match the edgelist read in before. what_order &lt;- order(class_edgelist_byadjacency$sender) class_edgelist_byadjacency &lt;- class_edgelist_byadjacency[what_order, ] head(class_edgelist_byadjacency) ## sender receiver ## 1.1 1 3 ## 1.2 1 5 ## 1.3 1 7 ## 1.4 1 21 ## 2.1 2 3 ## 2.2 2 6 The edgelist looks good, and, at this point, we can take the newly constructed edgelist and construct an igraph object from it, using the syntax from above. 3.3 Using the network Package We now demonstrate how to do the exact same tasks as in section 2 using the network package (Butts 2015) instead of igraph. We still want to take the input matrix (or edgelist) and attribute file and create a network. The only difference is that the format and functions will be a little different using the network package. The objects created by network can then be used with other packages, like sna, ergm, latentnet, etc., which allow us to perform network calculations, produce plots, and estimate sophisticated statistical models. Moreover, the functions and capabilities of igraph are often different than the packages associated with network. First let's detach the igraph package and load network. detach(package:igraph) library(network) 3.3.1 Using a Matrix to Construct the Network We are first going to create a network object using the matrix as input. The function is network(). The basic arguments are: x = the name of the matrix directed = T/F if directed or not vertex.attr = list of vertex (i.e., node) attributes Here we construct the network using the matrix read in above as input. class_netbymatrix_example2 &lt;- network(x = class_mat, directed = T) We can also use the network() function to put node attributes onto the network. We can do this all at once, constructing a network based on the matrix and putting the attributes on the nodes. The input is a list of attributes. Let's first create that list. It is often easier to treat the attributes as characters (rather than factors) when they are categorical variables, like gender or race. So let's go ahead and turn gender and race into character variables using an as.character() function. class_attributes$race &lt;- as.character(class_attributes$race) class_attributes$gender &lt;- as.character(class_attributes$gender) Now let’s create a list to use as input to the network() function. We will use a do.call() function to turn the data frame into a list where each element in the list is a different attribute, taken from the class_attributes data frame. attribute_list &lt;- do.call(list, class_attributes) attribute_list ## $id ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## ## $gender ## [1] &quot;Male&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; ## [9] &quot;Male&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; ## [17] &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; ## ## $grade ## [1] 12 12 12 12 12 12 11 11 11 11 10 10 10 10 10 10 10 10 10 10 10 10 10 10 ## ## $race ## [1] &quot;White&quot; &quot;White&quot; &quot;White&quot; &quot;White&quot; &quot;White&quot; &quot;White&quot; &quot;Black&quot; &quot;White&quot; &quot;White&quot; ## [10] &quot;White&quot; &quot;White&quot; &quot;White&quot; &quot;White&quot; &quot;White&quot; &quot;White&quot; &quot;White&quot; &quot;White&quot; &quot;White&quot; ## [19] &quot;White&quot; &quot;White&quot; &quot;White&quot; &quot;White&quot; &quot;White&quot; &quot;White&quot; Now, let's create the network using the network() function but this time add a vertex.attr argument, set equal to the list of vertex (or node) attributes. class_netbymatrix_example2 &lt;- network(x = class_mat, directed = T, vertex.attr = attribute_list) class_netbymatrix_example2 ## Network attributes: ## vertices = 24 ## directed = TRUE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 77 ## missing edges= 0 ## non-missing edges= 77 ## ## Vertex attribute names: ## gender grade id race vertex.names ## ## No edge attributes We see a number of vertex (i.e., node) attributes are on the constructed network. We could also have put the attributes on one at a time using the set.vertex.attribute() function, where the arguments are: x = network of interest attrname = name of attribute value = vector of attribute Here, we will we create a vertex attribute called gradenew on the network based on the grade attribute. set.vertex.attribute(x = class_netbymatrix_example2, attrname = &quot;gradenew&quot;, value = class_attributes$grade) Note for this function we do not need to assign the output to a new object. It will automatically update the input network, class_netbymatrix_example2. class_netbymatrix_example2 ## Network attributes: ## vertices = 24 ## directed = TRUE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 77 ## missing edges= 0 ## non-missing edges= 77 ## ## Vertex attribute names: ## gender grade gradenew id race vertex.names ## ## No edge attributes 3.3.2 Using an Edgelist to Construct the Network Now we will construct the same network but this time use the edgelist as input (still using the network package). To create a network from an edgelist, we still use the network() function, but this time the edgelist will be the main input. We will use the vertices argument to input the attribute information, although it is also possible to still use a vertex.attr argument. Using vertices, the attributes are inputted as a data frame, where the first column should be the ids of the nodes. The vertices approach will be particularly useful, as it makes it easier to deal with isolates (in much the same way that we saw with igraph). Using a vertices argument is also very useful as it sets the order that the vertices will be added to the network, ensuring that unexpected behavior does not occur. Now, we use the network() function to create the network. As before: class_netbyedgelist_example2 &lt;- network(x = class_edges, directed = T, vertices = class_attributes) class_netbyedgelist_example2 ## Network attributes: ## vertices = 24 ## directed = TRUE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 77 ## missing edges= 0 ## non-missing edges= 77 ## ## Vertex attribute names: ## gender grade race vertex.names ## ## Edge attribute names: ## weight As with igraph, we can extract the matrix, edgelist and vertex attributes from the network object. For example, to get the matrix back out, use: as.matrix(class_netbyedgelist_example2) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ## 2 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 3 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 ## 4 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 7 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 ## 8 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ## 9 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 10 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 ## 11 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 ## 12 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 ## 13 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 14 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 ## 15 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 ## 16 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ## 17 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 ## 18 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 ## 19 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 ## 20 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 ## 21 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 ## 22 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ## 23 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 ## 24 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 To get the vertex attributes back out, we use a get.vertex.attribute() function, with two main inputs, the network of interest (defined as x) and the attribute to extract (defined as attrname). Here we extract the grade attribute from the network. get.vertex.attribute(x = class_netbyedgelist_example2, attrname = &quot;grade&quot;) ## [1] 12 12 12 12 12 12 11 11 11 11 10 10 10 10 10 10 10 10 10 10 10 10 10 10 And we can check the order of the nodes using: get.vertex.attribute(x = class_netbyedgelist_example2, attrname = &quot;vertex.names&quot;) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 This is useful when checking if the vertex attributes have been correctly placed onto the network object (as the attributes should be attached in an order consistent with the vertex.names). 3.3.3 Putting Edge Attributes onto the Network In cases where there are edge attributes (such as the strength of a relationship) we can use a set.edge.attribute() function to add the edge attribute to the network. The arguments are: x = the network of interest attrname = name of attribute to put on network value = vector of edge values Here we add an edge attribute called weight to the network, where weight is the vector of weights taken from the edgelist. set.edge.attribute(x = class_netbyedgelist_example2, attrname = &quot;weight&quot;, value = class_edges[, &quot;weight&quot;]) class_netbyedgelist_example2 ## Network attributes: ## vertices = 24 ## directed = TRUE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 77 ## missing edges= 0 ## non-missing edges= 77 ## ## Vertex attribute names: ## gender grade race vertex.names ## ## Edge attribute names: ## weight An edge attribute, weight, has now been added to the network. We can extract edge attributes using a get.edge.attribute function(): weights &lt;- get.edge.attribute(class_netbyedgelist_example2, attrname = \"weight\") 3.3.4 Moving between igraph and network Objects We have so far seen how to construct networks using both the igraph and network packages. It is often useful to be able to move between the two packages, so that an igraph object can be turned into a network one and vice versa. For example, we may want to plot our network using one package but use the other package to do a particular analysis. Luckily, the intergraph package makes conversion a fairly easy task. Let's begin by loading the intergraph package (Bojanowski 2015). library(intergraph) The main functions are asIgraph() and asNetwork(). asIgraph() converts network objects into igraph objects while asNetwork() turns igraph objects into network objects. For example, here we turn our igraph object into a network one: network_from_igraph &lt;- asNetwork(class_netbyedgelist) network_from_igraph ## Network attributes: ## vertices = 24 ## directed = TRUE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 77 ## missing edges= 0 ## non-missing edges= 77 ## ## Vertex attribute names: ## gender grade race vertex.names ## ## Edge attribute names: ## weight 3.4 Some Key Network Measures In this section we will walk through the calculation of a few example network measures. The idea is to get a bit more familiar with R and to lay some of the groundwork for what follows. We will examine how to calculate these network measures using the matrix representation of the network, as well as with functions within the igraph package (this can also be done within the sna package). 3.4.1 Degree We start with degree, which is a simple measure capturing the number of ties for each node. We can define outdegree as the number of ties sent from node i and indegree as the number of ties received by node i. We start by calculating degree using the raw matrix, class_mat. We calculate outdegree by summing over the rows of the matrix. Each element of the matrix shows if i sends a tie to j. By summing over the rows, we calculate the total number of ties sent by i. outdeg &lt;- rowSums(class_mat) outdeg ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## 4 2 4 2 0 0 4 3 2 5 4 3 1 5 5 4 5 4 4 3 3 1 4 5 We see that the first student send 4 ties (or nominates 4 friends), the second sends 2 ties, the third sends 4, and so on. Now we sum up over the columns, calculating the number of ties received by i. indeg &lt;- colSums(class_mat) indeg ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## 1 0 4 0 5 4 1 4 3 4 2 1 7 4 3 3 2 4 5 3 5 0 3 9 We see that the first student (for example) is nominated by 1 person as a friend. Now, let's do the same thing, but use the functions within the igraph package. detach(package:network) library(igraph) The function is degree(). The main arguments are graph (the network of interest) and mode, where mode sets the type of degree calculation: in, out or total (adding up outdegree and indegree). Note that we must use the network constructed as an igraph object to use the igraph functions. outdeg_igraph &lt;- degree(graph = class_netbyedgelist, mode = &quot;out&quot;) indeg_igraph &lt;- degree(graph = class_netbyedgelist, mode = &quot;in&quot;) Let's check if the previous calculation of outdegree is the same as the calculation from igraph. table(outdeg == outdeg_igraph) ## ## TRUE ## 24 We can see that the igraph function yields the same outdegree values as calculated above (as outdeg is always equal to outdeg_igraph) 3.4.2 Density Density is another simple measure of network structure that captures the total number of edges (or ties) in the network divided by the total number of edges possible. Let's first calculate the density \"by hand\" using the size and number of edges in the network. The function gsize() get the number of edges and the function gorder() gets the number of nodes. num_edges &lt;- gsize(class_netbyedgelist) num_edges ## [1] 77 num_nodes &lt;- gorder(class_netbyedgelist) num_nodes ## [1] 24 We also need to calculate the number of dyads (excluding ii pairs) which tells us how many edges are possible in the network. number_dyads &lt;- (num_nodes * (num_nodes - 1)) And now we can go ahead and calculate density, taking the number of edges and dividing by the number possible. den &lt;- num_edges / number_dyads den ## [1] 0.1394928 And here we use the canned function, edge_density(), in igraph. edge_density(class_netbyedgelist) ## [1] 0.1394928 3.4.3 Walks We now turn to walks on the network, defined as any sequence of nodes and edges (backwards and forwards) that connect i to j. For example, a sequence of i-&gt;j-&gt;k-&gt;j-&gt;l would be a walk of length 4 from i to l. One item of interest is the number of walks of a given length between two nodes. We can use matrix multiplication to calculate this. By raising the matrix to the nth power, we get the number of walks of length n between all ij pairs. Let's calculate the number of walks of length two by multiplying the matrix by itself: walks2 &lt;- class_mat %*% class_mat walks2 ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## 1 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 2 0 0 1 1 0 0 0 1 ## 2 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 ## 3 0 0 2 0 1 1 0 0 2 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 ## 4 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 ## 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 7 0 0 2 0 3 1 1 1 1 0 0 0 0 1 0 0 0 0 2 1 1 0 0 1 ## 8 0 0 0 0 1 1 0 3 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 ## 9 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ## 10 0 0 0 0 0 1 0 0 0 2 0 0 1 2 1 1 0 0 2 1 4 0 0 2 ## 11 0 0 0 0 0 1 0 0 0 2 2 0 2 2 2 0 1 0 0 0 1 0 1 3 ## 12 0 0 0 0 0 1 0 0 0 2 1 1 1 2 2 0 0 1 0 0 1 0 0 2 ## 13 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 ## 14 0 0 0 0 1 2 0 1 0 1 0 0 0 3 1 1 0 0 2 3 2 0 0 1 ## 15 0 0 0 0 0 2 0 1 0 2 0 1 1 2 2 0 0 1 2 1 2 0 0 3 ## 16 0 0 0 0 1 1 0 2 0 0 0 0 0 1 0 2 0 0 0 1 1 0 0 1 ## 17 0 0 1 0 1 1 0 1 1 1 0 0 3 1 1 0 2 1 0 0 1 0 1 1 ## 18 0 0 0 0 1 1 0 2 0 1 0 0 2 1 1 0 1 2 0 0 1 0 1 1 ## 19 0 0 1 0 2 0 0 0 1 1 0 0 1 0 0 0 0 0 4 1 2 0 0 2 ## 20 0 0 0 0 1 1 0 0 0 1 0 0 0 2 1 1 0 0 1 2 2 0 0 0 ## 21 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 2 0 0 1 ## 22 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 ## 23 0 0 0 0 0 0 0 2 0 0 0 0 2 0 0 0 1 1 0 0 0 0 2 2 ## 24 0 0 0 0 1 1 0 0 0 2 1 0 2 2 0 0 0 0 3 2 1 0 0 3 This suggests, for example, that there is 1 walk of length 2 between node 1 and node 1 (themself). In this case, the walk is : 1-&gt;7-&gt;1. We can see this by looking at the rows for node 1 and node 7: class_mat[c(1, 7), ] ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 ## 7 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 We can see that node 1 is friends with 3, 5, 7 and 21. We can also see that node 7 is friends with 1, 9, 10 and 16, thus creating a walk of length 2: 1-&gt;7-&gt;1. We can calculate the number of walks of length 3, 4, etc. in an analogous fashion. For example, for walks of length 3: walks3 &lt;- class_mat %*% class_mat %*% class_mat walks3 ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## 1 0 0 4 0 4 2 1 1 3 1 0 0 1 3 1 1 0 0 4 2 4 0 0 2 ## 2 0 0 2 0 1 1 0 0 2 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 ## 3 0 0 0 0 3 3 0 5 0 2 1 0 2 3 0 3 0 0 3 3 2 0 0 5 ## 4 0 0 1 0 1 1 0 2 1 1 0 0 3 1 1 0 1 2 0 0 1 0 1 1 ## 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 7 1 0 1 0 2 3 0 3 2 3 0 0 2 3 1 5 0 0 3 3 5 0 0 4 ## 8 0 0 4 0 1 1 0 0 4 1 0 0 3 1 1 0 0 0 1 0 1 0 0 0 ## 9 0 0 0 0 1 1 0 3 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 ## 10 0 0 1 0 5 4 0 1 1 5 1 0 3 7 2 2 0 0 10 8 7 0 0 6 ## 11 0 0 0 0 2 5 0 3 0 7 2 2 6 7 5 0 1 4 5 3 5 0 1 9 ## 12 0 0 0 0 1 4 0 1 0 6 3 1 5 6 4 0 1 1 5 3 4 0 1 9 ## 13 0 0 0 0 1 1 0 3 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 ## 14 0 0 2 0 3 2 0 0 2 5 1 0 5 5 1 2 0 0 10 5 9 0 0 8 ## 15 0 0 1 0 2 5 0 1 1 7 3 0 6 9 4 2 1 0 7 6 8 0 1 9 ## 16 0 0 4 0 3 1 0 0 4 2 0 0 3 1 1 0 0 0 5 1 3 0 0 2 ## 17 0 0 1 0 3 3 0 7 1 3 1 0 7 3 1 1 2 3 3 2 2 0 3 7 ## 18 0 0 2 0 2 2 0 3 2 3 1 0 8 3 1 0 3 2 3 2 2 0 3 6 ## 19 0 0 0 0 3 4 0 3 0 2 0 0 0 7 2 5 0 0 4 7 7 0 0 3 ## 20 0 0 1 0 3 1 0 0 1 3 1 0 3 3 0 1 0 0 8 4 5 0 0 6 ## 21 0 0 1 0 3 1 0 0 1 2 0 0 1 2 1 1 0 0 5 3 4 0 0 2 ## 22 0 0 0 0 0 0 0 2 0 0 0 0 2 0 0 0 1 1 0 0 0 0 2 2 ## 23 0 0 2 0 2 2 0 3 2 2 0 0 6 2 2 0 3 3 0 0 2 0 2 2 ## 24 0 0 0 0 1 5 0 2 0 5 0 1 2 8 4 3 0 1 7 6 10 0 0 7 Here, we see that there are 0 walks of length 3 from 1 to 1, but 4 different walks from 1 to 3 (for example, 1-&gt;3-&gt;8-&gt;3). 3.4.4 Paths, Distance, and Closeness Paths are defined as a sequence of nodes and edges starting with one node and ending with another, where a path is not allowed to revisit the same node twice (unlike with walks). For example, i-&gt;j-&gt;l would be a path of length 2. It is often of interest to know the shortest path, or the distance, between nodes in a network. Here we will rely on the distances() function within the igraph package. The main arguments are graph and mode, setting the type of distance to calculate. The 'out' value for mode says we want distance from i to j, which is what we typically want. dist_mat &lt;- distances(graph = class_netbyedgelist, mode = &quot;out&quot;) Let's look at the first 10 rows and columns: dist_mat[1:10, 1:10] ## 1 2 3 4 5 6 7 8 9 10 ## 1 0 Inf 1 Inf 1 2 1 2 2 2 ## 2 Inf 0 1 Inf 3 1 Inf 2 3 3 ## 3 Inf Inf 0 Inf 2 1 Inf 1 2 2 ## 4 Inf Inf 3 0 3 3 Inf 2 3 3 ## 5 Inf Inf Inf Inf 0 Inf Inf Inf Inf Inf ## 6 Inf Inf Inf Inf Inf 0 Inf Inf Inf Inf ## 7 1 Inf 2 Inf 2 2 0 2 1 1 ## 8 Inf Inf 1 Inf 2 2 Inf 0 1 3 ## 9 Inf Inf 2 Inf 1 3 Inf 1 0 4 ## 10 Inf Inf 3 Inf 3 1 Inf 3 3 0 This is a matrix holding the length of the shortest path between all pairs of nodes. We can see, for example, that node 1 is 1 step from node 3 and 2 steps from node 6. Note that Inf means that i can not reach j through any path. We can get the specific paths connecting i to j using the all_shortest_paths() function. The main inputs are the graph and the starting node (from) and the ending node (to). As an example, let's go ahead and calculate all of the shortest paths between 1 and 6. all_shortest_paths(class_netbyedgelist, from = 1, to = 6) ## $res ## $res[[1]] ## + 3/24 vertices, named, from e77deaa: ## [1] 1 3 6 ## ## ## $nrgeo ## [1] 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 If we look at the first part of the output, we can see a path of length 2 between 1 and 6 (1-&gt;3-&gt;6). As another example, let's look at the shortest paths between 1 and 16. all_shortest_paths(class_netbyedgelist, from = 1, to = 16) ## $res ## $res[[1]] ## + 3/24 vertices, named, from e77deaa: ## [1] 1 3 16 ## ## $res[[2]] ## + 3/24 vertices, named, from e77deaa: ## [1] 1 7 16 ## ## ## $nrgeo ## [1] 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 2 0 0 0 0 1 0 0 1 We can see there are two paths of length 2 between 1 and 16 (1-&gt;3-&gt;16; 1-&gt;7-&gt;16). It is often of interest to summarize the distances over all ij pairs. For example, we may be interested in the mean distance between nodes. Let's go ahead and calculate the mean distance using our matrix calculated above. Before we calculate the mean, let's put an NA on the diagonal, as we are not interested in the distance to oneself (which is by definition 0). diag(dist_mat) &lt;- NA Here we calculate the mean distance across all pairs, excluding the NAs on the diagonal. mean(dist_mat, na.rm = T) ## [1] Inf The mean is Inf, as there are Inf values (so i and j are not reachable) in the matrix. This is not especially informative, however, only telling us that at least one pair is unreachable. One option would be to exclude the unreachable pairs. This is a common approach but also throws out information on all cases where i cannot reach j. Here we calculate mean distance while ignoring the unreachable pairs. mean(dist_mat[dist_mat != Inf], na.rm = T) ## [1] 2.808933 We see a mean of 2.809, so that nodes are, on average, separated by paths of length 2.809 (excluding pairs that cannot reach other at all). Alternatively, researchers will often employ closeness as the summary measure when there are unreachable pairs. Closeness is based on the inverse of the distance matrix. By inverting the distance matrix, all Inf values are turned into 0s and thus can be included in the mean calculation. The inverse of the distance matrix has the opposite interpretation as above, showing show how 'close' node i is to node j. The disadvantage of a closeness measure is that the interpretation is not as intuitive as with distance. Let's go ahead and calculate the mean closeness for this network. We first take the inverse of the distance matrix. close_mat &lt;- 1 / dist_mat close_mat[1:10, 1:10] ## 1 2 3 4 5 6 7 8 9 10 ## 1 NA 0 1.0000000 0 1.0000000 0.5000000 1 0.5000000 0.5000000 0.5000000 ## 2 0 NA 1.0000000 0 0.3333333 1.0000000 0 0.5000000 0.3333333 0.3333333 ## 3 0 0 NA 0 0.5000000 1.0000000 0 1.0000000 0.5000000 0.5000000 ## 4 0 0 0.3333333 NA 0.3333333 0.3333333 0 0.5000000 0.3333333 0.3333333 ## 5 0 0 0.0000000 0 NA 0.0000000 0 0.0000000 0.0000000 0.0000000 ## 6 0 0 0.0000000 0 0.0000000 NA 0 0.0000000 0.0000000 0.0000000 ## 7 1 0 0.5000000 0 0.5000000 0.5000000 NA 0.5000000 1.0000000 1.0000000 ## 8 0 0 1.0000000 0 0.5000000 0.5000000 0 NA 1.0000000 0.3333333 ## 9 0 0 0.5000000 0 1.0000000 0.3333333 0 1.0000000 NA 0.2500000 ## 10 0 0 0.3333333 0 0.3333333 1.0000000 0 0.3333333 0.3333333 NA We see NAs on the diagonal and the closeness values for other cells in the matrix. The values range from 0 (not reachable, so minimum closeness) to 1 (directly connected, so maximum closeness). And now we take the mean, as before. mean(close_mat, na.rm = T) ## [1] 0.3482272 We see a value of .348. Nodes are, on average, 1 / 2.872 paths close to each other. Note that this is somewhat different than what we saw above when calculating distance, where we had a mean distance of 2.809. Remember that the distance calculation excludes all unreachable nodes, while the closeness calculation does not. It may also be useful to calculate the median, as a means of comparison. median(dist_mat, na.rm = T) ## [1] 3 median(close_mat, na.rm = T) ## [1] 0.3333333 In this case we can see that the median yields the same information in both cases, that the median distance is 3 and the median closeness is 1 / 3. 3.4.5 Reachability Reachability captures whether node i can reach node j through any path. This can be calculated directly from the distance matrix. Node i can reach node j if the distance between i and j is less than Inf (i.e., there is some path between i and j). Here we use an ifelse() function to set the reachability matrix to 1 if distance is less than Inf and 0 if it is not less than Inf. reach_mat &lt;- ifelse(dist_mat &lt; Inf, yes = 1, no = 0) reach_mat ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 ## 1 NA 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 ## 2 0 NA 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 ## 3 0 0 NA 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 ## 4 0 0 1 NA 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 ## 5 0 0 0 0 NA 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 6 0 0 0 0 0 NA 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 7 1 0 1 0 1 1 NA 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 ## 8 0 0 1 0 1 1 0 NA 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 ## 9 0 0 1 0 1 1 0 1 NA 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 ## 10 0 0 1 0 1 1 0 1 1 NA 1 1 1 1 1 1 1 1 1 1 1 0 1 1 ## 11 0 0 1 0 1 1 0 1 1 1 NA 1 1 1 1 1 1 1 1 1 1 0 1 1 ## 12 0 0 1 0 1 1 0 1 1 1 1 NA 1 1 1 1 1 1 1 1 1 0 1 1 ## 13 0 0 1 0 1 1 0 1 1 1 1 1 NA 1 1 1 1 1 1 1 1 0 1 1 ## 14 0 0 1 0 1 1 0 1 1 1 1 1 1 NA 1 1 1 1 1 1 1 0 1 1 ## 15 0 0 1 0 1 1 0 1 1 1 1 1 1 1 NA 1 1 1 1 1 1 0 1 1 ## 16 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 NA 1 1 1 1 1 0 1 1 ## 17 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 NA 1 1 1 1 0 1 1 ## 18 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 NA 1 1 1 0 1 1 ## 19 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 NA 1 1 0 1 1 ## 20 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 NA 1 0 1 1 ## 21 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 NA 0 1 1 ## 22 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 NA 1 1 ## 23 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 NA 1 ## 24 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 NA We can see that node 1 cannot reach node 2 (for example). 3.4.6 Diameter We can also use the distance matrix to calculate diameter, showing the longest geodesic, or distance, between any two nodes in the network. Diameter thus takes all of the shortest paths between nodes (i.e., distance) and calculates the longest path among that set. Note that if we simply calculate the maximum over the distance matrix we will get back Inf if there are nodes that cannot reach other: max(dist_mat, na.rm = T) ## [1] Inf We may, instead, be interested in calculating the maximum distance just over those nodes that can reach other. Here, we exclude any distance that is Inf. max(dist_mat[dist_mat != Inf], na.rm = T) ## [1] 7 We can also calculate diameter using the diameter() function in igraph. By default, the function will include only those pairs that are reachable. diameter(class_netbyedgelist) ## [1] 7 This tutorial has offered background information on constructing networks in R and calculating basic network measures. The tutorials in the rest of the book will build on this material. In the second tutorial for Chapter 3, we will cover data management and network construction for the case of dynamic network data. "],["ch3-Dynamic-Network-Data-R.html", "3, Part 2. Dynamic Network Data 3.5 Discrete Longitudinal Networks 3.6 Continuous-Time Networks", " 3, Part 2. Dynamic Network Data This is the second tutorial for Chapter 3, covering the basics of network data in R. The first tutorial covered data management for cross-sectional network data, while this tutorial will walk through the analogous steps for dynamic network data. Here, we assume that a researcher has data on at least two time points and is interested in exploring changes in the network over time. We will cover the basics of handling dynamic network data and calculating summary statistics on dynamic networks. There are two basic types of dynamic network data. First, the data may be a simple extension of cross-sectional data, where a researcher collects network information, like friendships between students, at multiple time points. The researcher can then ask how friendships change between time T and T+1. The network data thus have a temporal component to it, as ties can be dropped and added, but the time periods are defined in a discrete manner. Alternatively, the data may take the form of continuous-time, streaming data. In this case, the data are time-stamped, with each interaction recorded sequentially. For example, the data may be collected via cell phones (i sends a text message to j), Bluetooth sensors (i and j are physically 'close'), or first hand observations of interactions (i talks to j in a meeting). The key conceptual shift is from relationships (i and j are friends) to specific interactions, or events. We begin the tutorial by exploring the case of discrete longitudinal network data before turning to the case of continuous-time network data. 3.5 Discrete Longitudinal Networks We will utilize a small school-based network for the first part of this tutorial, covering discrete longitudinal network data. The data were collected by Daniel McFarland and are based on friendships between students in a classroom. The network data were collected at two time points, corresponding to the first and second semester of the school year. 3.5.1 Getting the Data Ready Let’s begin by loading some useful packages. We will work with network and sna, as well as the dynamic extensions of those packages, networkDynamic (a package to manipulate dynamic network data; Butts et al. (2023)) and tsna (a package to calculate network statistics on dynamic networks; Bender-deMoll and Morris (2021)). We will not make use of igraph in this tutorial. library(network) library(sna) library(networkDynamic) library(tsna) Let's go ahead and walk through an example where we construct a dynamic network object, which can then be used as input to other functions (e.g., plotting functions). In the case of discrete-time data, we construct our dynamic network object by creating separate networks for each time period of interest and then putting the constructed networks together in a single object. We begin by reading in the data, starting with the semester 1 edgelist. In reading in the data, we set colClasses to \"character\". We want the columns in the edgelist read in as character values because the sender and receiver ids in the edgelist are labels for the nodes, and thus cannot be interpreted as numeric values. We read in the data from a URL (defined in the first line below). url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class237_sem1_edgelist.txt&quot; sem1_edgelist &lt;- read.table(file = url1, header = T, colClasses = &quot;character&quot;) head(sem1_edgelist) ## sender receiver ## 1 113214 121470 ## 2 113214 125522 ## 3 113214 149552 ## 4 113214 122728 ## 5 113214 122706 ## 6 115909 127535 We see the ids of the senders and receivers of friendship ties. An edge exists between i-&gt;j if i nominated j as a friend. Thus, 113214 nominated 121470 as a friend. Note that the values in the edgelist (113214, 121470, etc.) have no inherent meaning, and are only useful in identifying each node. Let’s also read in the attribute file, containing nodal characteristics. url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class237_sem1_attributes.txt&quot; sem1_attributes &lt;- read.table(file = url2, header = T) Let's take a look at some of the main variables. sem1_attributes[, c(&quot;ids&quot;, &quot;sem_id&quot;, &quot;expected_grade&quot;, &quot;like_subject&quot;)] ## ids sem_id expected_grade like_subject ## 1 113214 1 4 4 ## 2 115909 1 4 3 ## 3 121470 1 3 2 ## 4 122706 1 4 3 ## 5 122710 1 4 2 ## 6 122714 1 4 3 ## 7 122723 1 4 2 ## 8 122724 1 3 2 ## 9 122726 1 4 3 ## 10 122728 1 4 2 ## 11 122732 1 4 3 ## 12 122734 1 4 3 ## 13 125522 1 4 3 ## 14 125567 1 4 4 ## 15 126359 1 4 4 ## 16 127535 1 4 3 ## 17 128033 1 4 3 ## 18 129483 1 3 2 ## 19 138473 1 4 2 ## 20 139163 1 4 1 ## 21 140271 1 4 4 ## 22 141006 1 4 3 ## 23 144882 1 3 2 ## 24 149552 1 4 3 ## 25 151812 1 4 2 ids = id of node; sem_id = semester where data comes from, 1 or 2; expected_grade: D = 1 C = 2 B = 3 A = 4; like_subject: 1-4 scale, with 1 = strong dislike to 4 = like it a lot. It will be useful to have the attribute file ordered from low to high in terms of the ids. sem1_attributes &lt;- sem1_attributes[order(sem1_attributes$ids), ] Note that the first column is the ids of the nodes, and must be the same class as the columns of the edgelist, here character. sem1_attributes$ids &lt;- as.character(sem1_attributes$ids) With our attribute and edgelist objects together, we can now go ahead and construct the network for semester 1, just like in the previous tutorial. sem1_net &lt;- network(x = sem1_edgelist, directed = T, vertices = sem1_attributes) It is sometimes easier to take the labels off the edgelist before constructing the network (e.g., we did not want to have to deal with tracking the labels in the output). We will demonstrate how to do this with our semester 1 edgelist, although we will not make use of the relabeled edgelist in our analysis. The basic idea is to transform the edgelist into one without labels, where the ids of the nodes are numeric, running from 1 to N, where N is equal to the number of nodes in the network. We will write a little function to make this task easier. The arguments are edgelist (the edgelist of interest, assumed to be a two column data frame of sender and receiver) and ids (the ids of the nodes in the network). edgelist_relabel_function &lt;- function(edgelist, ids){ edgelist_simple &lt;- data.frame(sender = as.numeric(factor(edgelist[, 1], levels = ids)), receiver = as.numeric(factor(edgelist[, 2], levels = ids))) return(edgelist_simple) } And just for the sake of demonstration, let's create the simplified edgelist, without labels. We will set edgelist to the semester 1 edgelist and ids to the ids from the attribute data frame. sem1_edgelist_nolabel &lt;- edgelist_relabel_function(edgelist = sem1_edgelist, ids = sem1_attributes$ids) head(sem1_edgelist_nolabel) ## sender receiver ## 1 1 3 ## 2 1 13 ## 3 1 24 ## 4 1 10 ## 5 1 4 ## 6 2 16 We can see that the edgelist has been recoded to numbers running from 1 to 25 (the size of the network). For example, 1 (113214) is friends with 3 (121470). This is the same as above, just relabeled. Now, let's go ahead and construct the second semester network. url3 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class237_sem2_edgelist.txt&quot; sem2_edgelist &lt;- read.table(file = url3, header = T, colClasses = &quot;character&quot;) url4 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class237_sem2_attributes.txt&quot; sem2_attributes &lt;- read.table(file = url4, header = T) sem2_attributes &lt;- sem2_attributes[order(sem2_attributes$ids), ] sem2_attributes$ids &lt;- as.character(sem2_attributes$ids) sem2_net &lt;- network(x = sem2_edgelist, directed = T, vertices = sem2_attributes) As this is over time data, we need to be careful about the changing composition of the network, where nodes can come and go over time. Some students in semester 1 may not be in the class in semester 2 (and vice versa). For this example, we will keep all nodes in the constructed networks, even if they were not in the class for both semesters. This means that the semester 1 network need not have the same set of nodes as the semester 2 network. Alternatively, we could remove nodes from the networks that were not present in both semesters. It is also important to note that the attributes of the nodes are measured twice, once for each semester. Thus, there can be change over time for nodal attributes, like expected grade. 3.5.2 Constructing networkDynamic Objects We now have two network objects, one for each semester. The next task is to put the two networks together as a networkDynamic object, which is the dynamic version of a network object. networkDynamic objects are useful as they can be used to calculate various statistics, produce network movies (see Chapter 5) and serve as input to statistical models (see Chapter 13, Part 2). The main function is networkDynamic(). There are a number of ways to specify the input network data. With discretely observed longitudinal data, we can use a list of networks as the main input. The list is simply the network objects, in sequential order. net_list &lt;- list(sem1_net, sem2_net) We can now go ahead and create the networkDynamic object with the network list as input. We will also set vertex.pid to \"vertex.names\" (the name of the vertex attribute housing the ids). This ensures that all nodes present in at least one period will be included in the constructed object, with a unique id attached to them. Setting vertex.pid is particularly useful if there are nodes coming in and out of the network across time, as in this case. Finally, we set create.TEAs to T, telling the function to create time varying vertex attributes for each attribute found on the input networks. This is useful as it automatically creates vertex attributes based on the input networks, while allowing the values to change across time. net_dynamic_class &lt;- networkDynamic(network.list = net_list, vertex.pid = &quot;vertex.names&quot;, create.TEAs = T) net_dynamic_class ## NetworkDynamic properties: ## distinct change times: 3 ## maximal time range: 0 until 2 ## ## Dynamic (TEA) attributes: ## Vertex TEAs: cls_id.active ## course_challenging.active ## expected_grade.active ## had_teacher_before.active ## like_subject.active ## like_teacher.active ## sem_id.active ## ## Includes optional net.obs.period attribute: ## Network observation period info: ## Number of observation spells: 1 ## Maximal time range observed: 0 until 2 ## Temporal mode: discrete ## Time unit: step ## Suggested time increment: 1 ## ## Network attributes: ## vertices = 26 ## directed = TRUE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## vertex.pid = vertex.names ## net.obs.period: (not shown) ## total edges= 165 ## missing edges= 0 ## non-missing edges= 165 ## ## Vertex attribute names: ## active cls_id.active course_challenging.active expected_grade.active had_teacher_before.active like_subject.active like_teacher.active sem_id.active vertex.names ## ## Edge attribute names: ## active The networkDynamic object has 26 vertices (nodes present in at least one period) and 165 edges (i is friends with j in at least one period). Note that start time (or onset) is set to 0 and that the end time (or terminus) is set to 2, defined as the period where no further change is recorded. Thus, for a network with two discretely observed time periods, the first network defines the ties existing during time period 0 (here sem1_net) while the second network defines the ties existing during time period 1 (here sem2_net). We also see a number of Dynamic (TEA) time varying attributes, like expected_grade, that are measured at our two time points. Note that we can add fixed attributes to the object (that do not change over time) using the set.vertex.attribute() function. Let's take a look at the object as a data frame. net_dat_class &lt;- as.data.frame(net_dynamic_class) head(net_dat_class) ## onset terminus tail head onset.censored terminus.censored duration edge.id ## 1 0 1 1 3 FALSE FALSE 1 1 ## 2 0 2 1 13 FALSE FALSE 2 2 ## 3 0 2 1 25 FALSE FALSE 2 3 ## 4 0 1 1 10 FALSE FALSE 1 4 ## 5 0 2 1 4 FALSE FALSE 2 5 ## 6 0 2 2 16 FALSE FALSE 2 6 Each row is an edge spell, defining the duration of the i-&gt;j edge. The main columns are: onset, the point where the relationship started; terminus, when the relationship ended (or the end of the recorded time points); tail (sender) and head (receiver) of the edge; duration, showing how long the relationship lasted and edge.id, a unique identifier for that edge. For example, node 1 sends an edge to node 3 in period 0 (onset), but not in period 1 (duration is thus equal to 1). This means 1-&gt;3 was in the semester 1 network but not the semester 2 network. On the other hand, node 1 sends an edge to node 13 in period 0 (onset) and this lasts to the end of the observation period (duration = 2). The 1-&gt;13 edge was thus in the semester 1 network and the semester 2 network. An edge formed in semester 2 would have onset set to 1, terminus equal to 2, and duration equal to 1. We can extract other useful information from the networkDynamic object. For example, we can extract the ids of the nodes using a get.vertex.pid() function. Here we look at the first five ids. get.vertex.pid(net_dynamic_class)[1:5] ## [1] &quot;113214&quot; &quot;115909&quot; &quot;121470&quot; &quot;122706&quot; &quot;122710&quot; Thus, node 1 is 113214, node 2 is 115909, and so on. This is useful in interpreting output, such as the edge spells printed above (i.e., we can determine which nodes are involved in each edge). We can also extract the activity of the nodes, showing if they are present in a given period. Here we use a get.vertex.activity() function. We set as.spellList to T, to output the results as a data frame. activity_data &lt;- get.vertex.activity(net_dynamic_class, as.spellList = T) head(activity_data) ## onset terminus vertex.id onset.censored terminus.censored duration ## 1 0 2 1 FALSE FALSE 2 ## 2 0 2 2 FALSE FALSE 2 ## 3 0 2 3 FALSE FALSE 2 ## 4 0 2 4 FALSE FALSE 2 ## 5 0 2 5 FALSE FALSE 2 ## 6 0 2 6 FALSE FALSE 2 The output is similar to the edge spells data frame, but here the focus is on the nodes. We can see that node 1 (for example) is in both semesters (duration = 2). Let's see if any nodes are not in both semesters (duration less than 2): activity_data[activity_data$duration &lt; 2, ] ## onset terminus vertex.id onset.censored terminus.censored duration ## 14 0 1 14 FALSE FALSE 1 ## 17 1 2 17 FALSE FALSE 1 We can see that two nodes are not present in both semesters. Node 14 was in semester 1 but not semester 2 (onset = 0 and terminus = 1), while node 17 was in semester 2 but not semester 1 (onset = 1). Overall, there are 26 nodes, with 24 in both semesters, 1 in semester 1 only and 1 in semester 2 only (thus 25 in each semester). 3.5.3 Summary Statistics At this point, we are in a position to use our networkDynamic object. Here, we will cover the basics of calculating summary statistics. We will make use of the tSnaStats() function to calculate our summary statistics. This is a wrapper for using sna functions on networkDynamic objects. With sna functions, we can calculate things like density, distance and centrality scores. tSnaStats() has the following arguments: nd = networkDynamic object snafun = function of interest start = optional input specifying when to begin evaluation end = optional input specifying when to end evaluation time.interval = time between evaluations to be printed aggregate.dur = time period to bin (or collapse) network when calculating statistic Our networkDynamic object is simple, consisting of just two discrete networks, so we do not need to be as explicit about the time.interval or aggregate.dur values. Here we calculate density (using the gden() function in sna) for our semester 1 and semester 2 networks: tSnaStats(nd = net_dynamic_class, snafun = &quot;gden&quot;) ## Time Series: ## Start = 0 ## End = 2 ## Frequency = 1 ## Series 1 ## [1,] 0.160 ## [2,] 0.185 ## [3,] NA This is the same as calculating density on each network separately. gden(sem1_net) ## [1] 0.16 gden(sem2_net) ## [1] 0.185 Note that the tSnaStats() function does not (at the moment) work so well for node-level measures, like indegree centrality, when there are nodes coming in and out of the network across time. We can still calculate this, however, by calculating centrality on each network separately. The only trick is to match the values in semester 1 to semester 2, as the order of the nodes will not necessarily be the same (as some nodes present in semester 1 will no longer be present in semester 2). Let's first calculate indegree centrality for the first semester, creating a data frame with the first column as the ids and the second column as the centrality score. With the sna package, the function is degree() and we set cmode to \"indegree\" to get the right calculation. ids_sem1 &lt;- get.vertex.attribute(sem1_net, &quot;vertex.names&quot;) indegree_sem1 &lt;- degree(sem1_net, cmode = &quot;indegree&quot;) indegree_dat1 &lt;- data.frame(ids = ids_sem1, indegree_sem1 = indegree_sem1) head(indegree_dat1) ## ids indegree_sem1 ## 1 113214 9 ## 2 115909 2 ## 3 121470 6 ## 4 122706 6 ## 5 122710 5 ## 6 122714 3 And now for semester 2: ids_sem2 &lt;- get.vertex.attribute(sem2_net, &quot;vertex.names&quot;) indegree_sem2 &lt;- degree(sem2_net, cmode = &quot;indegree&quot;) indegree_dat2 &lt;- data.frame(ids = ids_sem2, indegree_sem2 = indegree_sem2) And now, let's use a merge() function to put together the two data frames. We set by to \"ids\" to merge based on the ids variable. We set all to T to include observations that were not in both semesters. indegree_dat &lt;- merge(indegree_dat1, indegree_dat2, by = &quot;ids&quot;, all = T) indegree_dat ## ids indegree_sem1 indegree_sem2 ## 1 113214 9 7 ## 2 115909 2 4 ## 3 121470 6 4 ## 4 122706 6 5 ## 5 122710 5 8 ## 6 122714 3 3 ## 7 122723 6 7 ## 8 122724 2 3 ## 9 122726 3 4 ## 10 122728 3 1 ## 11 122732 4 4 ## 12 122734 3 4 ## 13 125522 3 5 ## 14 125567 4 NA ## 15 126359 1 2 ## 16 127535 6 4 ## 17 127761 NA 6 ## 18 128033 2 5 ## 19 129483 2 6 ## 20 138473 3 2 ## 21 139163 3 5 ## 22 140271 4 1 ## 23 141006 1 9 ## 24 144882 5 4 ## 25 149552 5 1 ## 26 151812 5 7 We can see that 125567 received 4 friendship nominations in semester 1 but was not present in semester 2 (this corresponds to node 14 in our networkDynamic object). 127761, in contrast, was not present in semester 1 but received 6 nominations in semester 2 (node 17 in our networkDynamic object). The remaining nodes were present in both semesters and we can compare their indegree across time. 3.6 Continuous-Time Networks We now turn to the case where a researcher has time-stamped, streaming network data. Here, we shift our focus away from discrete networks observed at different time points and towards a series of events observed in real time. We will cover network construction and summary statistics for continuous-time dynamic network data. Much of this is an extension of the discrete-time case, but there are important differences and complications that arise. We are particularly concerned about measuring the continuous-time data at different levels of aggregation, and what that tells us about the network structure and dynamics. 3.6.1 Getting the Data Ready Our data are based on streaming interaction data collected by Daniel McFarland on students in classrooms. Time-stamped interactions in each classroom were recorded, with information on the 'sender' and 'receiver' of the interaction, as well as the nature of the interaction. Interactions could be social or task-based, for example. Data were collected across a large number of classrooms and days. Here we consider one classroom on a single day. We will begin by creating a dynamic network object based on our classroom interaction data. This will serve as the main inputs to the dynamic network visualization (see Chapter 5) and measurement functions. As above, we will make use of the networkDynamic() function. In this case, however, the inputs will look a little different. Here, we do not have a list of discrete networks. Instead, we construct the networkDynamic object based on two main inputs: an edge spells data frame and a vertex spells data frame. Let's begin by reading in the edge spells data frame. This is a data set reporting on the social interactions, i.e. talking, between individuals in the classroom. url5 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/example_edge_spells.csv&quot; edge_spells &lt;- read.csv(file = url5) Here we take a look at the first six rows of the data. head(edge_spells) ## start_time end_time send_col receive_col ## 1 0.143 0.143 11 2 ## 2 0.286 0.286 2 11 ## 3 0.429 0.429 2 5 ## 4 0.571 0.571 5 2 ## 5 0.714 0.714 9 8 ## 6 0.857 0.857 8 9 The edge spells data frame describes the start and end times for each observed edge. The first column is the start time of the edge (here interaction between i and j), the second column is the end time, the third column is the sender and the fourth column is the receiver. The time is recorded in standardized minutes. For example, we can see that the first social interaction in this class period involved node 11 talking to node 2, while the second interaction, happening almost immediately afterward, involved node 2 responding to node 11. Note that the events are ordered sequentially. In this case we treat the start and end time as the same (as interactions were very short) but in other cases we might have clear durations. For example, i and j may be sexual partners from period 1 to period 3, i and k may be partners from period 4 to 5 and i and j may be partners again from 6 to 7. Edges can be in the data frame multiple times, as i may talk to j at different time points during the class. For example, let's look at all of the times that node 11 talks to node 2. edge_spells[edge_spells$send_col == 11 &amp; edge_spells$receive_col == 2, ] ## start_time end_time send_col receive_col ## 1 0.143 0.143 11 2 ## 110 19.000 19.000 11 2 ## 119 22.289 22.289 11 2 ## 159 31.333 31.333 11 2 ## 180 36.080 36.080 11 2 We also need to read in a vertex spells data frame, showing how nodes move in and out of the network. url6 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/example_vertex_spells.csv&quot; vertex_spells &lt;- read.csv(file = url6) head(vertex_spells) ## start_time end_time id ## 1 0 43 1 ## 2 0 43 2 ## 3 0 43 3 ## 4 0 43 4 ## 5 0 43 5 ## 6 0 43 6 The first column is the start time for the node (when they come into the network) and the second column is the end time for the node (when they leave, or are no longer present, in the network). Here we set the start time to 0 for all nodes (as everyone is present for the whole time period) and the end time as the end of the class period. The last column is the id of the nodes. 3.6.2 Constructing networkDynamic Objects Now, we use the networkDynamic() function to create our dynamic network. The main arguments are vertex.spells and edge.spells. We use the objects read in above as inputs. net_dynamic_interactions &lt;- networkDynamic(vertex.spells = vertex_spells, edge.spells = edge_spells) Note that the size of the base network is assumed to be equal to the maximum vertex id value found in the edge spells data frame. Alternatively, we could include a base.net argument to set some of the basic properties (like size) of the network in question. Let's take a look at the networkDynamic object. net_dynamic_interactions ## NetworkDynamic properties: ## distinct change times: 229 ## maximal time range: 0 until 43 ## ## Includes optional net.obs.period attribute: ## Network observation period info: ## Number of observation spells: 1 ## Maximal time range observed: 0 until 43 ## Temporal mode: continuous ## Time unit: unknown ## Suggested time increment: NA ## ## Network attributes: ## vertices = 18 ## directed = TRUE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## net.obs.period: (not shown) ## total edges= 48 ## missing edges= 0 ## non-missing edges= 48 ## ## Vertex attribute names: ## active vertex.names ## ## Edge attribute names: ## active We can see that the time periods range from 0 to 43 minutes. We also see a network summary with 18 vertices and 48 edges. Those 48 edges correspond to the network formed from any interaction over the entire time period (so i-&gt;j exists if i talked to j at all during the class). Now that we have our networkDynamic object together, we can go ahead and summarize the features of the network. Let’s start with a simple plot. plot(net_dynamic_interactions) The default is to plot the network based on the entire period, showing if i talked to j at all over the class period. This amounts to collapsing the dynamic, time-stamped data into a single binary network, where we lose all of the dynamic information. Alternatively, we could plot the network at different time ranges (interactions happening within 1 minute intervals, 10 minute intervals, etc.), shifting the ranges to capture different network dynamics. We will explore how to do this in Chapter 5, on network visualization. 3.6.3 Summary Statistics We now turn to calculating summary statistics on our continuous-time networkDynamic object. Calculating summary statistics on continuous-time networks is more complicated than in the discrete-time case. Continuous-time networks do not have clearly defined points at which to calculate measures like indegree or density. A researcher must make a choice about the time range of interest and interpret the values with this choice in mind. We will again make use of the tSnaStats() function. As an example, we will calculate indegree for 10 minute intervals. We will collapse all interactions that occur in that 10 minute window into a 0/1 (0 = i did not talk to j; 1 = i did talk to j). We accomplish this by setting time.interval to 10 and aggregate.dur to 10. We set snafun to degree, with the added argument of cmode set to \"indegree\". tSnaStats(nd = net_dynamic_interactions, snafun = &quot;degree&quot;, time.interval = 10, aggregate.dur = 10, cmode = &quot;indegree&quot;) ## Time Series: ## Start = 0 ## End = 40 ## Frequency = 0.1 ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## 0 1 4 3 1 3 0 4 2 2 0 4 0 0 0 1 0 2 1 ## 10 2 3 4 1 3 0 1 2 2 0 4 2 0 1 1 0 4 2 ## 20 1 2 1 2 0 0 1 1 1 0 4 3 0 2 0 0 2 2 ## 30 2 3 2 1 3 0 1 2 0 0 4 0 0 0 1 0 4 1 ## 40 1 1 2 1 3 0 3 2 1 0 2 0 0 0 1 0 2 1 Looking at the output, this means that node 1 had one person talk to them during the first 10 minutes, 2 people talk to them between 10 and 20 minutes and so on. Note that we could do the same thing by calculating degree on a collapsed (or time-flattened) version of the network. Let's see how to do this on the first period of interest, running from 0 to 10 minutes. The first step is to extract the desired network from the networkDynamic object. The main function is network.collapse() (see also get.networks). network.collapse creates static versions of continuous-time networks, over the desired time range. The main arguments are: dnet = a networkDynamic object onset = start point for desired network length = length of time for desired network We will set onset to 0 (the starting point) and length to 10. This will extract a network where an edge exists between i and j if i talked to j at least once between 0 and 9.999 minutes. net_0_10 &lt;- network.collapse(dnet = net_dynamic_interactions, onset = 0, length = 10) And now we can calculate indegree. degree(net_0_10, cmode = &quot;indegree&quot;) ## [1] 1 4 3 1 3 0 4 2 2 0 4 0 0 0 1 0 2 1 This is the same as the first row in the previous output. The advantage of the tSnaStats() function is that we avoid having to do separate steps for extracting the network and then calculating the desired statistic. As another example, let's calculate density in the network, aggregated at 10 minute intervals. tSnaStats(nd = net_dynamic_interactions, snafun = &quot;gden&quot;, time.interval = 10, aggregate.dur = 10) ## Time Series: ## Start = 0 ## End = 40 ## Frequency = 0.1 ## Series 1 ## [1,] 0.09150327 ## [2,] 0.10457516 ## [3,] 0.07189542 ## [4,] 0.07843137 ## [5,] 0.06535948 As we can see, density tends to decrease somewhat over time. We can also explore more disaggregated calculations, where the statistics of interest are calculated over a smaller time frame. Here, we will redo our calculation for indegree and density but do so for 1 minute intervals. We will print out the results for every fifth minute. tSnaStats(nd = net_dynamic_interactions, snafun = &quot;degree&quot;, time.interval = 5, aggregate.dur = 1, cmode = &quot;indegree&quot;) ## Time Series: ## Start = 0 ## End = 40 ## Frequency = 0.2 ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## 0 0 2 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 ## 5 0 0 0 0 0 0 2 2 2 0 1 0 0 0 0 0 0 0 ## 10 1 1 1 0 2 0 0 0 0 0 0 0 0 1 0 0 0 1 ## 15 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 2 0 ## 20 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 25 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 2 0 ## 30 0 1 0 0 2 0 0 0 0 0 2 0 0 0 0 0 1 0 ## 35 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 1 ## 40 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 1 1 This suggests that 2 people talked to node 2 between minute 0 and 1; 0 people talked to them between 5 and 6 minutes; 1 person talked to them between 10 and 11; etc. And now for density: tSnaStats(nd = net_dynamic_interactions, snafun = &quot;gden&quot;, time.interval = 5, aggregate.dur = 1) ## Time Series: ## Start = 0 ## End = 40 ## Frequency = 0.2 ## Series 1 ## [1,] 0.01960784 ## [2,] 0.02287582 ## [3,] 0.02287582 ## [4,] 0.01960784 ## [5,] 0.00000000 ## [6,] 0.01960784 ## [7,] 0.01960784 ## [8,] 0.01960784 ## [9,] 0.02287582 We see that density rates are much lower with this very short time frame of one minute (as interactions within a given minute are fairly rare). Overall, this tutorial has covered the basics of handling dynamic network data. Network dynamics will come up again in Chapter 5 (visualization), Chapter 13 (statistical network models), Chapter 14 (network diffusion), and Chapter 15 (social influence). The next tutorial (Chapter 4) will deal with missing network data, and we will restrict our attention to the simpler case of cross-sectional data. "],["ch4-Missing-Network-Data-R.html", "4 Missing Network Data 4.1 Missing Data 4.2 Listwise Deletion 4.3 Gauging the Level of Bias 4.4 Simple Imputation Options", " 4 Missing Network Data This tutorial offers an empirical example dealing with missing network data in R. Missing data is a common problem faced by network researchers. Actors in the network may be absent the day of the survey, refuse to participate and in general offer incomplete information. Practically, a researcher must deal with missing data before doing any actual analyses. We will cover different strategies in assessing the possible level of bias due to missing data (focusing on the case of actor non-response). We will then cover simple imputation processes, with the goal of limiting the bias due to missing data. 4.1 Missing Data The data for our example are based on friendships between women in a sorority. The true network has 72 nodes. In the example that follows we will analyze the network assuming it has missing data, in the form of actor non-response. In the example, there are 14 nodes that are non-respondents, so that no information is available for those 14 nodes in terms of whom they nominated. We can assume that those 14 nodes were absent the day of the survey. Note that we still have any nominations from the non-missing nodes (those who filled out the survey) to the missing nodes (the non-respondents). Also note that the actual missing data for this example is generated for the sake of the tutorial, and does not reflect the missing data in the real network. Here we read in a data frame that indicates which nodes in our toy example are assumed to be missing. We again read in the data from a URL (defined in the first line below). url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/sorority_attributes_wmissingdata.csv&quot; sorority_attributes &lt;- read.csv(file = url1) Now we will read in the matrix describing the friendship between women in the sorority. We need to add row.names = 1 to tell R that the first column should be used to set the row names. url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/sorority_matrix_wmissingdata.csv&quot; sorority_matrix &lt;- read.csv(file = url2, row.names = 1) And let's turn that data frame into a matrix to make it easier to manipulate. sorority_matrix &lt;- as.matrix(sorority_matrix) Let's examine the data matrix and attribute file. We will first create a vector showing which nodes are missing in the network, using the missing variable in the attribute file. A 1 indicates the node is missing and 0 indicates they are not missing. The which() function returns which element is equal to 1. missing &lt;- which(sorority_attributes[, &quot;missing&quot;] == 1) missing ## [1] 2 7 12 26 29 30 44 45 47 50 54 64 65 66 Let's also create vector indicating which are not missing. notmissing &lt;- which(sorority_attributes[, &quot;missing&quot;] == 0) Now let's look at the rows and columns of our matrix, focusing on the missing cases. Here we just look at the first missing node, node 2 (just looking at the first 15 columns): sorority_matrix[2, 1:15] ## id1 id2 id3 id4 id5 id6 id7 id8 id9 id10 id11 id12 id13 id14 id15 ## NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA We can see that the row is full of NAs since node 2 is missing. The matrix thus offers no information on the nominations from node 2 to other nodes in the network. Now, let's look at the column for our missing node (just for the first 15 rows): sorority_matrix[1:15, 2] ## id1 id2 id3 id4 id5 id6 id7 id8 id9 id10 id11 id12 id13 id14 id15 ## 0 NA 0 0 0 0 NA 0 0 0 0 NA 1 0 0 There are 0s, 1s and NAs. A 1 indicates that a non-missing node reports a friendship with the missing node (here person 2); a 0 means that no tie is reported (between the non-missing node and missing node). For example, node 13 reports a tie with node 2 while node 1 does not. It is important to remember that we do not know about the nominations from the missing nodes; so we do not know if node 2 would have nominated node 13 back. The NAs correspond to cases involving another missing node. For example, node 7 is also missing and we see an NA in row 7, column 2. More substantively, there is no information at all about ties between two missing nodes. So we would also see an NA in row 2, column 7: sorority_matrix[2, 7] ## [1] NA 4.2 Listwise Deletion We will begin by constructing the network assuming that the researcher deals with the missing data by performing listwise deletion. In this case, all of the missing nodes are dropped from the network when constructing the network object. This means that all ties from non-missing nodes to missing nodes will also not be included in the network. Let's create a matrix that just includes the non-missing nodes. Note that the syntax below uses the - sign to tell R which rows and columns in the matrix should be removed, here those cases that are missing. matrix_remove_missing &lt;- sorority_matrix[-missing, -missing] dim(matrix_remove_missing) ## [1] 58 58 Now there are only 58 rows and 58 columns as the 14 missing cases have been removed. We will now take our reduced matrix (with only non-missing cases) and create an igraph object. library(igraph) net_listwise_delete &lt;- graph_from_adjacency_matrix(matrix_remove_missing, mode = &quot;directed&quot;) Let's go ahead and plot the network with the missing nodes removed, adding a few plotting options to make the graph easier to interpret. plot(net_listwise_delete, vertex.label = NA, vertex.size = 10, edge.arrow.size = .5, edge.arrow.width = .5) We can also calculate network statistics of interest on our incomplete network, with the missing nodes removed. Here we will assume that we are interested in measuring the mean closeness between nodes in the network. We first define the shortest path between each pair of nodes. We then take the inverse of those values (to avoid any problems with unreachable nodes; see Chapter 3) and then take the mean over those values. Here we calculate closeness on the network constructed above, using only the non-missing cases. The process is the same as in Chapter 3, and we again use the distances() function with mode set to out. The diagonal is not meaningful here, so we put in a NA for that. dist_listwise_delete &lt;- distances(graph = net_listwise_delete, mode = &quot;out&quot;) diag(dist_listwise_delete) &lt;- NA And now we can calculate how close everyone is, on average, by taking the mean over the inverted distance matrix. We set na.rm = T to ignore the NAs on the diagonal. mean_closeness_listwise_delete &lt;- mean(1 / dist_listwise_delete, na.rm = T) mean_closeness_listwise_delete ## [1] 0.1098182 We can also create a function that will do the same as above. Creating a function will save us from having to retype each step every time we want to calculate closeness. We will call the function closeness_function(). The only argument is network, equal to an igraph object. The rest of the steps are the same as above. closeness_function &lt;- function(network){ dist_matrix &lt;- distances(graph = network, mode = &quot;out&quot;) diag(dist_matrix) &lt;- NA mean_closeness &lt;- mean(1 / dist_matrix, na.rm = T) return(mean_closeness) } Once we create the closeness function, all we need to do is input the network of interest to calculate closeness. closeness_function(net_listwise_delete) ## [1] 0.1098182 4.3 Gauging the Level of Bias The next question is how much bias we think there is in our estimate, here for closeness. We will examine two different strategies for gauging bias. 4.3.1 Gauging Bias Using Past Results First, we will use past results on missing data and network measurement to estimate the level of bias. Past work has shown how missing data biases different network measures (and networks) at different rates. The idea is to use these results to gauge bias in the particular case of interest, given the features of the network and the measure of interest. A researcher would simply use the results that match their study most closely. In this case the network is small and directed, and the measure of interest is closeness. The percent missing is approximately 20% (14 missing nodes out of 72: 14/72). Given these conditions, we can use the tables in Smith and Moody (2013) as a guide. Looking at Table 6 in Smith and Moody (2013), we can see that for the smaller, directed networks, we can miss up to 20-25% of the network and still be under a bias of .25 for closeness (or distance). Given these results we may expect the bias to be around .25 or so, assuming there is no imputation. 4.3.2 Gauging Bias Using Predictive Models Our second strategy uses predictive modeling, where the researcher inputs various features of the study into a model that yields an estimated level of bias. Here we will rely on the model presented in Smith, Moody, and Morgan (2017). They have provided an easy to use bias calculator at: http://www.soc.duke.edu/~jmoody77/missingdata/calculator.htm. The researcher must set a number of inputs into the bias calculator and we will walk through how to practically do this in the case of the sorority network. The main inputs are: size, percent missing, indegree standard deviation, directed/undirected and the type of missing data. We start by setting the size of the true network. In this case, we set the number of nodes to 72 (as this is the size of the network) and the percent missing to 20. We can estimate the indegree standard deviation using the network above: indeg &lt;- degree(net_listwise_delete, mode = &quot;in&quot;) sd(indeg) ## [1] 1.609203 We will thus set the standard deviation to 1.609. The next input is directed/undirected and here the network is directed. The last input is the correlation between the missing nodes and their centrality. Are central nodes more/less likely to be missing? This can be difficult to determine as we do not have much information on the missing nodes. One possibility is to put in different inputs for this correlation and calculate a range of possible bias. A second option is to compare the indegree of the missing nodes to the indegree of the non-missing nodes and see which is higher. When the indegree is higher for missing nodes, the correlation is positive between being missing and centrality. Here we will set the correlation between missing nodes and centrality at .25, a slight positive correlation, although we could try alternative values. Given these inputs, the expected bias for distance is around .19 (using the bias calculator from Smith, Moody, and Morgan (2017)), similar to the estimate above. Note that the results are somewhat different if we set network type to a specific kind of network (here sorority). 4.4 Simple Imputation Options We now turn to different options in dealing with missing nodes, beyond simply removing them. Here we will consider simple imputation procedures. Simple imputation is based on the idea that a researcher can leverage the nominations from non-missing nodes to missing nodes to help fill in some of the missing data (Smith, Morgan, and Moody (2022)). For directed networks, there are three basic versions of simple imputation: asymmetric, symmetric, and probabilistic. In this section we will walk through each one. To begin, we will go back to the original matrix, sorority_matrix, and construct a network based on the non-missing nodes and any missing node that was nominated by a non-missing node. Let's identify which missing nodes we will put back in the network. To identify these nodes, we first subset the matrix, only looking at the nominations to the missing nodes. nominations_missing_nodes &lt;- sorority_matrix[, missing] Now we add up those columns, calculating the number of nominations each missing node received. indeg_missing &lt;- colSums(nominations_missing_nodes, na.rm = T) Finally, we ask which column sums are greater than 0 indicating that the missing node got at least one nomination from a non-missing node. We use that to subset the missing cases, just keeping those who got at least one nomination. impute_nodes &lt;- missing[indeg_missing &gt; 0] impute_nodes ## [1] 2 7 12 26 29 30 44 45 47 50 54 64 66 Let's also create a vector showing which nodes should be removed completely, so missing nodes who did not get nominations from non-missing nodes. still_missing &lt;- missing[indeg_missing == 0] still_missing ## [1] 65 4.4.1 Simple Imputation with Asymmetric Option So far, we have defined which missing nodes should go back into the imputed network. Next, we must decide on what ties should be imputed. Here, we will focus on the potential ties from missing nodes to the non-missing nodes who nominated them. We have no observed information on these ties so we must decide how to treat them. In the asymmetric option, we do the simplest thing and just assume that no ties exist from the missing nodes back to the non-missing nodes who nominated them. Thus, we assume that all ties from n to m (where n is a non-missing node and m is a missing node) are asymmetric, so that m to n does not exist. First, we copy the raw matrix read in above, so we don't make any changes to the original data. matrix_impute_asym &lt;- sorority_matrix Next, we set all potential outgoing ties from missing nodes to 0: matrix_impute_asym[missing, ] &lt;- 0 And finally we create the network based on the imputed matrix. net_impute_asym &lt;- graph_from_adjacency_matrix(matrix_impute_asym, mode = &quot;directed&quot;) Let's plot the network, coloring the nodes based on missing-status. We will color the nodes red if they are missing nodes and blue if they are not missing. We will use an ifelse() function on the missing column in our attribute data frame. cols &lt;- ifelse(sorority_attributes[, &quot;missing&quot;] == 1, &quot;red&quot;, &quot;blue&quot;) table(cols, sorority_attributes[, &quot;missing&quot;]) ## ## cols 0 1 ## blue 58 0 ## red 0 14 The coding looks right, so let's go ahead and put the cols vector onto the network. V(net_impute_asym)$color &lt;- cols Right now this network includes missing nodes that received no nominations and should not be in the imputed network. They are defined in still_missing. Let's go ahead and remove them. We will use a delete_vertices() function. The arguments are: graph = network; v = nodes to remove. net_impute_asym &lt;- delete_vertices(graph = net_impute_asym, v = still_missing) The network will now have 71 nodes. And now let's plot the network. plot(net_impute_asym, vertex.label = NA, vertex.size = 7, edge.arrow.size = .5, edge.arrow.width = .5) Note that all ties going to the red nodes (missing) are unreciprocated. And for a comparison let's calculate closeness using our closeness() function. We will calculate closeness under each of our imputation strategies, to see how different choices affect our estimates of a basic network property. mean_closeness_impute_asym &lt;- closeness_function(net_impute_asym) mean_closeness_impute_asym ## [1] 0.08920313 In this case, we can see that the asymmetric imputation strategy yields a lower value of closeness (or higher distances) than under listwise deletion (where closeness was .11). 4.4.2 Simple Imputation with Symmetric Option We now repeat the same process as above, still using a simple imputation strategy, but we make different assumptions about the ties going from the missing nodes back to the non-missing nodes that nominated them. Here we use a symmetric option, where we assume that all ties from n to m (where n is a non-missing node and m is a missing node) are reciprocated, so that m to n exists. As before, we start by creating a copy of the friendship matrix, so we don't make any changes to the original data. matrix_impute_sym &lt;- sorority_matrix Now, for any case where a missing node nominates a non-missing node we impute a tie from the missing node back to the non-missing node. We do this by setting the outgoing ties for the missing nodes equal to the ties sent to them. Let's first grab the columns for the missing nodes to see who nominated them. This was defined as nominations_missing_nodes above. head(nominations_missing_nodes) ## id2 id7 id12 id26 id29 id30 id44 id45 id47 id50 id54 id64 id65 id66 ## id1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## id2 NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## id3 0 1 0 1 0 0 0 0 0 0 0 1 0 1 ## id4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## id5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## id6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Remember that each 1 shows which non-missing node nominates which missing node, on the columns. We can see that node 3 nominates a missing case, node 7. There are 14 columns as this data only includes nominations to the missing nodes (on the columns). The NAs correspond to ties involving two missing nodes. We set those to 0 here as we are not imputing the ties between missing nodes, just assuming they do not exist (perhaps incorrectly). We also transpose the nomination matrix to make it the right dimensions to be outgoing ties (from missing nodes to non-missing nodes who nominated them). Thus, if node n (non-missing) nominates node m (missing) we would see a 1 in the n,m cell in the matrix; by taking the transpose, we will be able to impute a nomination back from m to n. nominations_missing_nodes[is.na(nominations_missing_nodes)] &lt;- 0 outgoing_ties_sym &lt;- t(nominations_missing_nodes) And now we set the outgoing ties for the missing nodes based on the imputation from above. matrix_impute_sym[missing, ] &lt;- outgoing_ties_sym Let's take a look at the results, focusing on the first missing node (person 2) (for the first 15 rows): matrix_impute_sym[1:15, 2] ## id1 id2 id3 id4 id5 id6 id7 id8 id9 id10 id11 id12 id13 id14 id15 ## 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 Here we see that node 13 nominates person 2, and in this version of the imputed matrix, there is also a tie from 2 to 13. matrix_impute_sym[2, 13] ## [1] 1 And note that there is no tie from 2 to 13 in the previous imputation procedure: matrix_impute_asym[2, 13] ## [1] 0 Again, we can create a network based on the imputed data. net_impute_sym &lt;- graph_from_adjacency_matrix(matrix_impute_sym, mode = &quot;directed&quot;) Now let's take out any nodes that are missing and were not put back into the network, as before. net_impute_sym &lt;- delete_vertices(graph = net_impute_sym, v = still_missing) net_impute_sym ## IGRAPH cad98d4 DN-- 71 189 -- ## + attr: name (v/c) ## + edges from cad98d4 (vertex names): ## [1] id1 -&gt;id39 id1 -&gt;id42 id2 -&gt;id13 id2 -&gt;id62 id3 -&gt;id1 id3 -&gt;id7 ## [7] id3 -&gt;id26 id3 -&gt;id64 id3 -&gt;id66 id7 -&gt;id3 id8 -&gt;id1 id8 -&gt;id42 ## [13] id8 -&gt;id55 id9 -&gt;id25 id9 -&gt;id28 id9 -&gt;id63 id9 -&gt;id64 id9 -&gt;id72 ## [19] id10-&gt;id44 id10-&gt;id48 id10-&gt;id64 id11-&gt;id50 id11-&gt;id52 id11-&gt;id71 ## [25] id12-&gt;id20 id12-&gt;id31 id12-&gt;id32 id12-&gt;id35 id12-&gt;id42 id12-&gt;id55 ## [31] id13-&gt;id2 id13-&gt;id14 id13-&gt;id22 id13-&gt;id34 id13-&gt;id52 id13-&gt;id62 ## [37] id14-&gt;id22 id14-&gt;id33 id14-&gt;id34 id14-&gt;id44 id14-&gt;id69 id15-&gt;id16 ## [43] id15-&gt;id18 id15-&gt;id22 id16-&gt;id15 id16-&gt;id41 id17-&gt;id43 id18-&gt;id15 ## + ... omitted several edges Note there are still 71 nodes but now 189 edges in the imputed network (compared to the 157 edges in net_impute_asym). mean_closeness_impute_sym &lt;- closeness_function(net_impute_sym) mean_closeness_impute_sym ## [1] 0.1574669 The closeness value is quite different, and higher, than under listwise deletion or the asymmetric option. 4.4.3 Simple Imputation with Probabilistic Option Now, we repeat the same process as above, but offer a probabilistic option on how to impute the ties going from the missing nodes back to the non-missing nodes that nominated them. Here, when a tie exists from n to m (where n is a non-missing node and m is a missing node) we assume that a tie from m to n exists with probability p, set to the reciprocity rate in the observed network (just using the set of non-missing nodes). First, we calculate the rate of reciprocity. This gives us a baseline estimate on the probability of a tie being reciprocated. We will use the network constructed above with missing nodes removed to calculate this. The function is reciprocity() (in the igraph package). Arguments are graph and mode, with mode setting the type of calculation. We will set mode to \"ratio\". Ratio ensures that we get the correct calculation, where the number of reciprocated dyads is divided by the total number of non-null dyads, symmetric / (asymmetric + symmetric). p &lt;- reciprocity(graph = net_listwise_delete, mode = &quot;ratio&quot;) p ## [1] 0.4204545 Our results suggest that about 42% of the time when i nominates j, j nominates i. Given this calculation, we can impute the ties by probabilistically assigning ties from missing nodes back to non-missing nodes. Let’s begin by copying the friendship matrix from above. matrix_impute_prob &lt;- sorority_matrix Let’s also go ahead and transpose the incoming ties matrix, showing nominations to missing nodes. This will put the matrix into the right shape to be (imputed) outgoing ties. outgoing_ties_prob &lt;- t(nominations_missing_nodes) Next, we will probabilistically determine which of the incoming ties will be imputed as reciprocated. For each nomination from n (non-missing) to m (missing), we will take a draw from a binomial distribution with probability set to p. This will probabilistically determine if the m to n tie exists, imputing if the nomination is reciprocated. Note that we only take draws for cases where outgoing_ties_prob is equal to 1, showing where a non-missing node (n) nominates a missing node (m). We use set.seed() to make it easier to reproduce our results. set.seed(200) impute_tie &lt;- outgoing_ties_prob == 1 outgoing_ties_prob[impute_tie] &lt;- rbinom(outgoing_ties_prob[impute_tie], size = 1, prob = p) And now we set the outgoing ties for the missing nodes equal to the probabilistic set of outgoing ties. matrix_impute_prob[missing, ] &lt;- outgoing_ties_prob Again, we can create a network, remove missing nodes with no incoming ties and calculate closeness. net_impute_prob &lt;- graph_from_adjacency_matrix(matrix_impute_prob, mode = &quot;directed&quot;) net_impute_prob &lt;- delete_vertices(graph = net_impute_prob, v = still_missing) mean_closeness_impute_prob &lt;- closeness_function(net_impute_prob) mean_closeness_impute_prob ## [1] 0.1351686 Note that this introduced a bit of stochastic noise into the imputation (as we took draws from a binomial distribution). It would thus make sense to repeat this process a number of times and average over the results. 4.4.4 Comparing Estimates Here we create a little table of our closeness results, to see how different imputation choices affect our estimates of closeness. closeness_table &lt;- data.frame(true = .15, listwise_delete = mean_closeness_listwise_delete, asym = mean_closeness_impute_asym, sym = mean_closeness_impute_sym, prob = mean_closeness_impute_prob) closeness_table ## true listwise_delete asym sym prob ## 1 0.15 0.1098182 0.08920313 0.1574669 0.1351686 In this example we can actually compare against the true value on the complete network. This is possible because the missing data in this case were simulated. This, of course, would not be possible in most empirical settings. The true closeness value is .15, and we can see that the symmetric option was clearly the best, with asymmetric imputation doing worse than listwise deletion. More sophisticated imputation options are also possible, relying on statistical network models to predict the presence/absence of missing edges. "],["ch5-Network-Visualization-R.html", "5 Network Visualization 5.1 Setting up the Session 5.2 Network Plots using igraph 5.3 Network Plots based on ggplot 5.4 Contour Plots 5.5 Interactive Plots 5.6 Dynamic Network Visualizations", " 5 Network Visualization In this tutorial we will cover network visualization in R. A researcher will often start an analysis by plotting the network(s) in question. In fact, one of the real benefits of a network approach is that there is a tight connection between the underlying data and the visualization of that data. We have already seen some basic plotting commands in our previous tutorials. Here, we will walk through the different approaches and options in detail. We note three things before we get started. First, it is important to remember that different plots are useful for different purposes. The kind of plot that is useful for an initial exploration of the network may be different than a more polished figure used in publication. Second, we will only cover a relatively small number of plotting options. More advanced options are available within and outside R (i.e., in programs like Pajek and Gephi). And third, we note that plotting options appropriate for one network (i.e., a small, face-to-face network) may not work for another (a large email-based network). The tutorial is split into two parts. In the first part of the tutorial, we will use cross-sectional network data and cover basic network visualization. In the second part of the tutorial, we will use continuous-time network data, and cover visualization approaches (like network movies) appropriate for dynamic network data. 5.1 Setting up the Session We will use the same network as in Chapter 3, Part 1 for the first part of this tutorial, on basic network visualization. The actors are students in a classroom and the relation of interest is friendship. There is individual information on gender, race and grade. Let's go ahead and read in the network data, saved as an edgelist (the file is read in from a URL, defined in the first line below). url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class555_edgelist.csv&quot; class_edges &lt;- read.csv(file = url1) Let's also read in the attribute file. url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class555_attributedata.csv&quot; class_attributes &lt;- read.csv(file = url2) We will begin using the igraph package. library(igraph) Now we go ahead and construct the igraph object, using the edgelist and attribute objects as inputs. class_net &lt;- graph_from_data_frame(d = class_edges, directed = T, vertices = class_attributes) class_net ## IGRAPH a5f8b51 DNW- 24 77 -- ## + attr: name (v/c), gender (v/c), grade (v/n), race (v/c), weight (e/n) ## + edges from a5f8b51 (vertex names): ## [1] 1 -&gt;3 1 -&gt;5 1 -&gt;7 1 -&gt;21 2 -&gt;3 2 -&gt;6 3 -&gt;6 3 -&gt;8 3 -&gt;16 3 -&gt;24 ## [11] 4 -&gt;13 4 -&gt;18 7 -&gt;1 7 -&gt;9 7 -&gt;10 7 -&gt;16 8 -&gt;3 8 -&gt;9 8 -&gt;13 9 -&gt;5 ## [21] 9 -&gt;8 10-&gt;6 10-&gt;14 10-&gt;19 10-&gt;20 10-&gt;24 11-&gt;12 11-&gt;15 11-&gt;18 11-&gt;24 ## [31] 12-&gt;11 12-&gt;15 12-&gt;24 13-&gt;8 14-&gt;10 14-&gt;13 14-&gt;19 14-&gt;21 14-&gt;24 15-&gt;10 ## [41] 15-&gt;11 15-&gt;13 15-&gt;14 15-&gt;24 16-&gt;3 16-&gt;5 16-&gt;9 16-&gt;19 17-&gt;8 17-&gt;13 ## [51] 17-&gt;18 17-&gt;23 17-&gt;24 18-&gt;13 18-&gt;17 18-&gt;23 18-&gt;24 19-&gt;14 19-&gt;16 19-&gt;20 ## [61] 19-&gt;21 20-&gt;19 20-&gt;21 20-&gt;24 21-&gt;5 21-&gt;19 21-&gt;20 22-&gt;23 23-&gt;5 23-&gt;13 ## [71] 23-&gt;17 23-&gt;18 24-&gt;6 24-&gt;10 24-&gt;14 24-&gt;15 24-&gt;21 5.2 Network Plots using igraph Network plots offer an intuitive way of exploring the features of the network. For example, a researcher may be interested in how demographic characteristics (like gender or race) map onto friendship. Or they may want to know if there are particularly central nodes in the network. Of course, this is not a formal test, but looking at a picture of the network(s) is a useful starting point for an analysis. Here, we will begin by exploring gender divides in the network (i.e., how strongly does gender map onto friendship groups?). Let's start with the default plotting in igraph. plot(class_net) This plot looks okay but could be much improved visually. The plot also does not tell us anything about gender. Let's go ahead and color the nodes in a more meaningful way. We will color the nodes by gender, making boys navy blue and girls light sky blue. We will first create a vector denoting the desired color of each node. cols &lt;- ifelse(class_attributes$gender == &quot;Female&quot;, &quot;lightskyblue&quot;, &quot;navy&quot;) We are using an ifelse() function to set color: light sky blue if gender equals Female, navy blue otherwise. Let's make sure that we coded this correctly using a simple table() function to look at color by gender. table(cols, class_attributes$gender) ## ## cols Female Male ## lightskyblue 16 0 ## navy 0 8 We now use a V(g)$color command to set the color of each node in the network (based on the colors defined in cols). V(class_net)$color &lt;- cols And now we plot as before. plot(class_net) It is also possible to set the colors within the function itself using a vertex.color argument: plot(class_net, vertex.color = cols) Based on this plot we can see that the network does divide along gender lines, with one small group of boys and then a larger set of girls (who are divided amongst themselves). We also see that two boys are not part of the 'boy group', and are disproportionately connected to girls. A researcher may also be interested in which nodes receive the most nominations. For example, we may want to know if particular girls/boys are really important in the network. To make this easier to see, let's size the nodes by indegree, showing how many ties people send to them. Let's first calculate indegree for each node. indeg &lt;- degree(class_net, mode = &quot;in&quot;) Now we plot the network and scale the size of the nodes by indegree, using a vertex.size argument. We set margin to -.10 to reduce some of the extra white space around the plot. plot(class_net, vertex.size = indeg, margin = -.10) Here, we make all the nodes a little bigger (adding a 3 to indegree) but the nodes are still sized by indegree. We will also change the color of the labels to make them a little easier to read. plot(class_net, vertex.size = indeg + 3, vertex.label.color = &quot;red&quot;, margin = -.10) In this plot, nodes who receive a higher number of nominations are larger. We can see that one boy (id 13) and one girl (id 24) receive a particularly high number of nominations (perhaps they are high status nodes in different groups). We can also see that the boys in the 'boy group' tend to have low indegree, as they are only friends with each other and there are few boys in the network. Let's go ahead and clean the plot up a bit to make it more visually appealing. This is important when producing figures for publications, websites, etc. We will start by changing the look of the nodes. Let's take out those labels using a vertex.label argument and take off the black edges around the nodes using a vertex.frame.color argument. plot(class_net, vertex.size = indeg + 3, vertex.label = NA, vertex.frame.color = NA, margin = -.10) Here we change the look of the arrows on the edges. Let's make those arrows a little smaller using edge.arrow.size and edge.arrow.width arguments. plot(class_net, vertex.size = indeg + 3, vertex.label = NA, vertex.frame.color = NA, edge.arrow.size = .5, edge.arrow.width = .75, margin = -.10) Now we change the look of the edges. Let's change the color of the lines to light gray (using edge.color). In general, making the edges a lighter color can help make network plots easier to interpret, especially when the network is large and/or dense. In this way, lighter edges help us avoid an unattractive 'hair ball' picture. plot(class_net, vertex.size = indeg + 3, vertex.label = NA, vertex.frame.color = NA, edge.arrow.size = .5, edge.arrow.width = .75, edge.color = &quot;light gray&quot;, margin = -.10) It is also possible to alter the layout of the plot. The default for igraph is to choose the 'best' layout function given the network, but we may want more control than that. For example, here we use an MDS-based layout. plot(class_net, vertex.size = indeg + 3, vertex.label = NA, vertex.frame.color = NA, edge.arrow.size = .5, edge.arrow.width = .75, edge.color = &quot;light gray&quot;, layout = layout_with_mds, margin = -.10) Or using an Kamada-Kawai layout: plot(class_net, vertex.size = indeg + 3, vertex.label = NA, vertex.frame.color = NA, edge.arrow.size = .5, edge.arrow.width = .75, edge.color = &quot;light gray&quot;, layout = layout_with_kk, margin = -.10) There are a number of other options a researcher could explore if they wanted to continue tweaking their plot. See the following help files for more options: ?plot.igraph ?igraph.plotting 5.3 Network Plots based on ggplot There are a number of other packages in R that we can use to plot our network. Here we will explore some of the network tools based on ggplot. ggplot offers a very general way of approaching visualizations in R, offering a flexible set of visualization options. Here, we will see how to utilize this general approach to visualization for plotting networks. We will run through this relatively quickly, as we have already worked through one network plot in detail above. ggplot uses network objects based on the network package format, so let's go ahead and detach igraph and load network and intergraph. Let's also load ggplot2 and GGally (a package for plotting networks using ggplot). detach(package:igraph) library(network) library(intergraph) library(ggplot2) library(GGally) And now let's construct a network object in the network format. Here we can rely on the intergraph package to transform the igraph object into a network object. The function is asNetwork(). class_net_sna &lt;- asNetwork(class_net) Let's add our measure of indegree to the network object as a vertex attribute. set.vertex.attribute(class_net_sna, attrname = &quot;indeg&quot;, value = indeg) 5.3.1 GGally package Here we demonstrate how to plot networks using the function ggnet2() in the GGally package (Schloerke et al. 2021). The default plot is: ggnet2(class_net_sna) Let's add some options, including size of nodes (node.size), color of nodes (node.color), size of edges (edge.size), color of edges (color.edge), and size of arrows (arrow.size). As before, we size the nodes by indegree and color the nodes by gender (defined in cols). We also set the color of the edges to a light grey and include small arrows in the plot. It is important to note that the input values that work for one set of functions (i.e., igraph) may not be exactly the same as with other functions (ggnet2). It often requires a bit of testing to find the best set of values for the network in question. ggnet2(class_net_sna, node.size = indeg, node.color = cols, edge.size = .5, arrow.size = 3, arrow.gap = 0.02, edge.color = &quot;grey80&quot;) Here we do the same plot but take off the legend on node size. This is accomplished (using the logic of ggplot) by adding a + option and then a guides() function controlling the legend, here setting it off. ggnet2(class_net_sna, node.size = indeg, node.color = cols, edge.size = .5, arrow.size = 3, arrow.gap = 0.02, edge.color = &quot;grey80&quot;) + guides(size = &quot;none&quot;) Now we will accomplish the same coloring of nodes, but this time we will use a vertex attribute on the network object (gender), setting the color in the palette argument (where male is set to navy and female to light sky blue). This will automatically create a legend for the node colors. ggnet2(class_net_sna, node.size = indeg, node.color = &quot;gender&quot;, palette = c(&quot;Male&quot; = &quot;navy&quot;, &quot;Female&quot; = &quot;lightskyblue&quot;), edge.size = .5, arrow.size = 3, arrow.gap = 0.02, edge.color = &quot;grey80&quot;) + guides(size = &quot;none&quot;) ggnet2() also makes it easy to color the edges in different ways. In this next plot, we will highlight the between and within gender ties. We will color all edges going from girls to girls light sky blue and all ties going from boys to boys navy blue. All girl-boy ties will be grey. This is accomplished by setting edge.color to c(\"color\",\"grey80\") with the first element telling ggnet2() to use the node colors to plot the edges (if they match) and the second element setting the color of cross-group ties. ggnet2(class_net_sna, node.size = indeg, node.color = &quot;gender&quot;, palette = c(&quot;Male&quot; = &quot;navy&quot;, &quot;Female&quot; = &quot;lightskyblue&quot;), edge.size = .5, arrow.size = 3, arrow.gap = 0.02, edge.color = c(&quot;color&quot;, &quot;grey80&quot;)) + guides(size = &quot;none&quot;) 5.3.2 ggnetwork package Here we offer a very short demonstration using the ggnetwork package (Briatte 2023). ggnetwork accomplishes similar tasks as ggnet2(), but requires the user to more directly rely on the ggplot functionality, which is useful for researchers seeking a large amount of control over the plot. library(ggnetwork) Let's recreate our network plot using the ggplot() function. ggplot() works by adding each desired feature (or layer) to the plot. Beyond the network itself, we add an aes() (aesthetic) function to set the location of the nodes, a geom_edges function() to set the features of the edges, a geom_nodes() function to set the features of the nodes and the theme_blank() function to set the background to blank. ggplot(class_net_sna, aes(x = x, y = y, xend = xend, yend = yend)) + geom_edges(color = &quot;lightgray&quot;) + geom_nodes(color = cols, size = indeg + 3) + theme_blank() Now, let's do the same plot but add arrows to it, capturing direction. ggplot(class_net_sna, arrow.gap = .015, aes(x = x, y = y, xend = xend, yend = yend)) + geom_edges(color = &quot;lightgray&quot;, arrow = arrow(length = unit(7.5, &quot;pt&quot;), type = &quot;closed&quot;)) + geom_nodes(color = cols, size = indeg + 3) + theme_blank() 5.4 Contour Plots As another example, we will use ggplot() to produce contour plots of our network. Countour plots offer a topographical representation of the network, especially useful for very large and very dense networks. We will use the same basic ggplot() function as above to plot the network, while layering a topography on top which reflects the density of different regions of the network. This is accomplished by adding a geom_density_2d() function to the ggplot call. ggplot(class_net_sna, arrow.gap = .01, aes(x = x, y = y, xend = xend, yend = yend)) + geom_edges(color = &quot;lightgray&quot;, arrow = arrow(length = unit(5, &quot;pt&quot;), type = &quot;closed&quot;)) + geom_nodes(color = cols) + theme_blank() + geom_density_2d() We can see that we have the same kind of network plot as before, but with a topography layered on top. 5.5 Interactive Plots We end the first part of this tutorial by looking at interactive network plots. Interactive network plots offer a more 'hands-on' experience, where the user can highlight certain nodes, rotate the graph, change the layout, etc. directly on the plot. This can be particularly useful when initially exploring the features of the network. It is also a natural way of presenting a network on a website. Here we will make use of the networkD3 package. library(networkD3) The networkD3 package indexes the nodes starting from 0 (rather than 1) so we need to create an edgelist and attribute file that starts the ids with 0. Here we grab the sender and receiver columns from the original edgelist and subtract 1 from the ids. class_edges_zeroindex &lt;- class_edges[, c(&quot;sender&quot;, &quot;receiver&quot;)] - 1 And let's also create a new attribute file. We will subtract 1 from the ids. We will also include gender and indeg, as a means of coloring and sizing the nodes. class_attributes_zeroindex &lt;- data.frame(id = class_attributes$id - 1, indeg = indeg, gender = class_attributes$gender) We are now ready to create a simple interactive plot. The main function is forceNetwork(). There are many possible arguments but we will focus on the main ones: Links = edgelist of interest Nodes = attribute file Source = name of variable on edgelist denoting sender of tie Target = name of variable on edgelist denoting receiver of tie Group = 'group' of each node, either based on an attribute or network-based group Nodesize = name of variable on attribute file to size nodes by NodeID = name of variable on attribute file showing id/name of node Here, we use the edgelist and attribute file constructed above. We use gender as the grouping variable and set the size of the nodes by indegree (indeg). forceNetwork(Links = class_edges_zeroindex, Nodes = class_attributes_zeroindex, Source = &quot;sender&quot;, Target = &quot;receiver&quot;, Group = &quot;gender&quot;, Nodesize = &quot;indeg&quot;, NodeID = &quot;id&quot;, opacity = 0.9, bounded = FALSE, opacityNoHover = .2) This function creates an html file that can be opened by a normal web browser (like Google Chrome or Firefox). Note that you can hover over desired nodes, change the layout, highlight certain edges, and so on, as a way of exploring different aspects of the network. 5.6 Dynamic Network Visualizations We now shift our attention to visualizing dynamic network data, specifically the case of continuous-time, streaming data. Here, we are working with data that is time-stamped (or at least continuously recorded), capturing the interactions between actors in the setting of interest. We will work with the same data as in Chapter 3, Part 2. The data are based on recorded task and social interactions between students in a classroom (e.g., student i talked to student j who then talked to student k, and so on). Streaming data present a visualization challenge, as the researcher must decide on the proper range of interest. For example, if the time band is defined too narrowly, we might end up plotting an empty graph, with no interactions happening in the range of interest. 5.6.1 Getting the Data Ready Let's go ahead and read in the data. We will read in three files: an edge spells data frame, a vertex spells data frame and an attribute data frame. Let's start with the edge spells data frame. url3 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/example_edge_spells.csv&quot; edge_spells &lt;- read.csv(file = url3) head(edge_spells) ## start_time end_time send_col receive_col ## 1 0.143 0.143 11 2 ## 2 0.286 0.286 2 11 ## 3 0.429 0.429 2 5 ## 4 0.571 0.571 5 2 ## 5 0.714 0.714 9 8 ## 6 0.857 0.857 8 9 The edge spells data frame captures the interactions occurring in the classroom. There is a column for start_time (when the interaction started), end_time (when the interaction ended), send_col (who initiated the interaction), and receive_col (who was the 'receiver' of the interaction). In this case the start and end time of the interaction are set to be the same, as interactions were short. And now we read in the vertex spells data frame: url4 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/example_vertex_spells.csv&quot; vertex_spells &lt;- read.csv(file = url4) head(vertex_spells) ## start_time end_time id ## 1 0 43 1 ## 2 0 43 2 ## 3 0 43 3 ## 4 0 43 4 ## 5 0 43 5 ## 6 0 43 6 The vertex spells data frame captures the movement of nodes in and out of the network, determined by start_time and end_time. In this case all nodes were in the network for the entire period. Finally, we read in the attribute file (we use read.table as the file is saved as a tab-delimited file). url5 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class_attributes.txt&quot; attributes_example2 &lt;- read.table(file = url5, header = T) head(attributes_example2) ## id gnd grd rce ## 1 1 2 10 4 ## 2 2 2 10 3 ## 3 3 2 10 3 ## 4 4 2 10 3 ## 5 5 2 10 3 ## 6 6 1 10 4 The data frame includes information for gender (gnd), grade (grd) and race (rce). We will begin by constructing a networkDynamic object, based on the vertex spells and edge spells data frames. The networkDynamic object can then be used to produce plots and network movies. library(networkDynamic) net_dynamic_interactions &lt;- networkDynamic(vertex.spells = vertex_spells, edge.spells = edge_spells) 5.6.2 Time Flattened Visualizations The simplest visualization option is to collapse the streaming data into discrete networks and then use the same basic plotting strategies discussed above. This has the advantage of creating a simple visualization, while still capturing some aspects of over time change. The clear disadvantage is that we flatten, or lose, much of the dynamic information, which was the unique part of the data in the first place. We will walk through an example of 'time flattened' visualizations before moving to more dynamic movies below. Let's create two discrete networks based on our interaction classroom data. The first network will capture interactions occurring in the first 20 minutes of class, while the second network will capture interactions in the last 20 minutes of class. Thus, an edge exists in the first network if i talked to j at least once between 0-19.999 minutes. An edge exists in the second network if i talked to j at least once between 20-39.999 minutes. We can create these networks using the get.networks() function. The main inputs are the networkDynamic object, as well as the time periods to extract the network over. We set start to 0 and time.increment to 20, telling the function to extract networks (starting with period 0) at 20 minute increments. extract_nets &lt;- get.networks(net_dynamic_interactions, start = 0, time.increment = 20) extract_nets ## [[1]] ## Network attributes: ## vertices = 18 ## directed = TRUE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 42 ## missing edges= 0 ## non-missing edges= 42 ## ## Vertex attribute names: ## vertex.names ## ## No edge attributes ## ## [[2]] ## Network attributes: ## vertices = 18 ## directed = TRUE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 34 ## missing edges= 0 ## non-missing edges= 34 ## ## Vertex attribute names: ## vertex.names ## ## No edge attributes We now have a list, with the first element corresponding to the network defined over the first 20 minutes (extract_nets[[1]]) and the second element corresponding to the network defined over the last 20 minutes (extract_nets[[2]]). Note that these networks are in the network (not igraph) format. As above, let's create a vector of colors to use in the plot. Again, we will color boys (equal to 1) navy blue and girls (equal to 2) light blue. cols_example2 &lt;- ifelse(attributes_example2$gnd == 2, &quot;lightskyblue&quot;, &quot;navy&quot;) Now we can go ahead and plot the networks. We want them side by side (accomplished using a par() function, setting mfrow to 1 row and 2 columns). Here we will use the default plot() function from the network package. We will set vertex.col (color of nodes) to the vector of colors defined above, and set vetex.cex (size of nodes) to 2. par(mfrow = c(1, 2)) plot(extract_nets[[1]], main = &quot;Talk to Network, 0 to 20 Minutes&quot;, vertex.col = cols_example2, vertex.cex = 2) plot(extract_nets[[2]], main = &quot;Talk to Network, 20 to 40 Minutes&quot;, vertex.col = cols_example2, vertex.cex = 2) The plot looks okay but over time comparisons are complicated by the fact that the nodes are not placed in the same way in the first network as the second network. We will redo our plot but this time set the layout so it is the same across the two periods. We first use the network.layout.fruchtermanreingold() function to define the locations of the nodes. These locations are then used in the subsequent plot statements (set via coord). We define the locations of the nodes based on the period 1 network. locs &lt;- network.layout.fruchtermanreingold(extract_nets[[1]], layout.par = NULL) par(mfrow = c(1, 2)) plot(extract_nets[[1]], main = &quot;Talk to Network, 0 to 20 Minutes&quot;, vertex.col = cols, vertex.cex = 2, coord = locs) plot(extract_nets[[2]], main=&quot;Talk to Network, 20 to 40 Minutes&quot;, vertex.col = cols, vertex.cex = 2, coord = locs) With this new layout, we can see that the first network splits into 3 groups, with no contact between them, while the second network has at least some (minimal) interaction between the two largest groups. 5.6.3 Dynamic Network Movies We have so far considered time flattened visualizations of our continuous-time network data. It is also possible to take the dynamic network object and construct a movie of the network evolving over time. In this way, we are able to produce a streaming version of the network, while being able to control the level of aggregation for changes in edges and nodes. Here, we will make use of the functions in the ndtv package. The ndtv package (Bender-deMoll 2022) has a very large number of features allowing the researcher to make detailed, custom-made network movies. We only cover a small portion of the features of the package. library(ndtv) We first need to set up the movie by creating a slice.par list that sets the basic features of the movie. The main arguments we need to set are: start = time in which to begin layouts for movie end = time to end layouts for movie interval = time between each layout aggregate.dur = duration which network should be aggregated Here we will run the movie over the entire class period, create a layout at each minute and do no aggregation (setting aggregate.dur to 0). We first create a list of these features and then add it to the networkDynamic object. slice.par &lt;- list(start = 0, end = 43, interval = 1, aggregate.dur = 0, rule = &quot;latest&quot;) set.network.attribute(net_dynamic_interactions, &#39;slice.par&#39;, slice.par) We now are in a position to actually create the movie. It is often useful to save the movie out as an html file. The outputted movie can then be incorporated into websites or presentations. This is accomplished using the render.d3movie() function. In this case we set output.mode to \"HTML\" and set filename to the file we want to save out. Here we will save a file called classroom_movie1.html to the working directory. For convenience, we have also embedded the movie within this html file. render.d3movie(net_dynamic_interactions, displaylabels = FALSE, vertex.cex = 1.5, output.mode = &quot;HTML&quot;, filename = &quot;classroom_movie1.html&quot;) Looking at the movie, we can see that we get a slice every minute; but since we did not aggregate at all, we only get instantaneous interactions, so only those interactions happening at that exact minute. Given that most interactions do not happen on the minute exactly, much of this is null and not very interesting. So, let’s go ahead and change the slice.par list. Here, let’s set aggregate.dur to 1, so all interactions occurring within the minute are treated as happening in a given time slice (and thus layout). We will save this out as classroom_movie2.html. slice.par &lt;- list(start = 0, end = 43, interval = 1, aggregate.dur = 1, rule = &quot;latest&quot;) set.network.attribute(net_dynamic_interactions, &#39;slice.par&#39;, slice.par) render.d3movie(net_dynamic_interactions, displaylabels = FALSE, vertex.cex = 1.5, output.mode = &quot;HTML&quot;, filename = &quot;classroom_movie2.html&quot;) We begin to see a bit more structure emerge, as reciprocity is now possible and prominent (where i talks to j and j talks to i), while this was not possible in the previous movie. Here we will produce the same movie but color the nodes by gender (set using vertex.cols). slice.par &lt;- list(start = 0, end = 43, interval = 1, aggregate.dur = 1, rule = &quot;latest&quot;) set.network.attribute(net_dynamic_interactions, &#39;slice.par&#39;, slice.par) render.d3movie(net_dynamic_interactions, displaylabels = FALSE, vertex.cex = 1.5, vertex.col = cols, output.mode = &quot;HTML&quot;, filename = &quot;classroom_movie3.html&quot;) We can aggregate even further and construct the movie based on a longer time range. Here we will aggregate over 10 minute time chucks (0-10, 10-20, 20-30, 30-40, 40-end). Thus, for the first slice, an edge exists between i and j if i talked to j in the first 10 minutes of class. This aggregation maintains some of the dynamic elements of the data while still capturing the larger structures that emerge in the network. The idea is that while we lose some of the dynamic time-stamped data (as every interaction within the threshold is treated the same, as a tie), we gain a bit more information on network features like distance and reachability, which are hard to capture when all interactions happen at unique time points. slice.par &lt;- list(start = 0, end = 43, interval = 10, aggregate.dur = 10, rule = &quot;latest&quot;) set.network.attribute(net_dynamic_interactions, &#39;slice.par&#39;, slice.par) render.d3movie(net_dynamic_interactions, displaylabels = FALSE, vertex.cex = 1.5, vertex.col = cols, output.mode = &quot;HTML&quot;, filename = &quot;classroom_movie4.html&quot;) With this longer time period of aggregation, we begin to see groups emerge which was less possible when the aggregated intervals were only a minute. Finally, we can continue to use 10 minutes to aggregate the edges but set the interval to 1. In this case, rather than just doing 0-10, 10-20, 20-30, 30-40, the movie will do 0-10, 1-11, 2-12, etc. moving the interval up by 1 in each layout. slice.par &lt;- list(start = 0, end = 43, interval = 1, aggregate.dur = 10, rule = &quot;latest&quot;) set.network.attribute(net_dynamic_interactions, &#39;slice.par&#39;, slice.par) render.d3movie(net_dynamic_interactions, displaylabels = FALSE, vertex.cex = 1.5, vertex.col = cols, output.mode = &quot;HTML&quot;, filename = &quot;classroom_movie5.html&quot;) Overall, this tutorial has offered an introduction to visualizing networks in R. We will draw on this syntax in nearly every tutorial to follow, including the next one (Chapter 6) on ego network data. "],["ch6-Ego-Network-Data-R.html", "6 Ego Network Data 6.1 Working with Ego Network Data 6.2 Plotting Networks 6.3 Homophily: Ego-Alter Attributes 6.4 Homophily: Alter Attributes 6.5 Ego Networks as Predictors 6.6 Working with One Input File", " 6 Ego Network Data In this tutorial, we will go over how to analyze ego network data in R. Unlike in Chapters 3-5, this tutorial will offer a more in-depth substantive example, where we use ego network data to answer a series of research questions. We will begin by covering the basics of ego network data, utilizing the egor package (Krenz et al. 2023) to manipulate, construct and visualize ego networks (similar to the previous tutorials, but for ego network data). We will then turn to an example analysis, focusing on substantive questions related to homophily, the tendency for similar actors to interact at higher rates than dissimilar actors. We will also consider an example where the ego network properties are used to predict other outcomes of interest, like happiness. 6.1 Working with Ego Network Data Ego network data are based on a sample of individuals and are commonly used in the social sciences. Each respondent (ego) reports on the people (alters) they are connected to, providing the local network around the focal respondent. Ego network data offer only pieces of the whole network, as there is typically no way of knowing if the alters named by one respondent are the same alters named by another. Ego network data thus require a somewhat different approach than with sociocentric network data. Most clearly, we must conceptually separate the information about the egos from information about the named alters, which is typically provided by ego. Ego network data can be structured in a number of ways but will generally have 4 kinds of information: first, demographic, behavioral, etc. information on the egos second, demographic, behavioral, etc. information on the named alters third, information about the relationship between ego and alter fourth, information about the relationships between the alters. Our example ego network data come from the 1985 General Social Survey ego network module. The GSS is a nationally representative survey that is fielded yearly, including basic demographic information, socio-economic measures and the like. In addition to the core survey given each year, the GSS will include rotating modules, including occasional modules on social networks. Here, we will work with ego network data from the GSS that has been preprocessed into three different files: a file with the ego attributes; a file with the alter attributes; and a file with the alter-alter ties. At the end of this tutorial, we will consider an example that uses a single file as the input data. Let's go ahead and read in the three files, starting with the ego attribute data (read in from a URL). url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/gss1985_ego_dat.csv&quot; ego_dat &lt;- read.csv(file = url1, stringsAsFactors = F) And here we look at the first 10 rows of the ego data frame, focusing on some of the key columns of interest. ego_dat[1:10, c(&quot;CASEID&quot;, &quot;AGE&quot;, &quot;EDUC&quot;, &quot;RACE&quot;, &quot;SEX&quot;, &quot;HAPPY&quot;, &quot;NUMGIVEN&quot;)] ## CASEID AGE EDUC RACE SEX HAPPY NUMGIVEN ## 1 19850001 33 16 white male 2 6 ## 2 19850002 49 19 white male 2 6 ## 3 19850003 23 16 white female 2 5 ## 4 19850004 26 20 white female 2 5 ## 5 19850005 24 17 white female 2 5 ## 6 19850006 45 17 white male 2 4 ## 7 19850007 44 18 white female 2 6 ## 8 19850008 56 12 white female 2 5 ## 9 19850009 85 7 white female 2 2 ## 10 19850010 65 12 white female 2 2 We see a number of variables that correspond to the attributes of ego. CASEID, for example, is the unique id for each respondent. We see demographic variables, like AGE and EDUC, as well as variables like HAPPY that correspond to important outcomes of interest. NUMGIVEN captures the number of alters named (capped at 6+). Note that respondents could say they had 6+ alters but were only allowed to report on 5. There are also cases in the data that have NAs for NUMGIVEN, which means they did not answer the network portion of the survey. Here we remove them from the data frame. ego_dat &lt;- ego_dat[!is.na(ego_dat$NUMGIVEN), ] Now, let's read in the alter attribute data. url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/gss1985_alter_dat.csv&quot; alter_dat &lt;- read.csv(file = url2, stringsAsFactors = F) Here we look at the first ten rows and select columns of the alter attribute data. alter_dat[1:10, c(&quot;CASEID&quot;, &quot;ALTERID&quot;, &quot;AGE&quot;, &quot;EDUC&quot;, &quot;RACE&quot;, &quot;SEX&quot;, &quot;KIN&quot;)] ## CASEID ALTERID AGE EDUC RACE SEX KIN ## 1 19850001 1 32 18 white male 0 ## 2 19850001 2 29 16 white female 1 ## 3 19850001 3 32 18 white male 0 ## 4 19850001 4 35 16 white male 1 ## 5 19850001 5 29 13 white female 0 ## 6 19850002 1 42 12 white female 1 ## 7 19850002 2 44 18 white male 0 ## 8 19850002 3 45 16 white male 0 ## 9 19850002 4 40 12 white female 0 ## 10 19850002 5 50 18 white male 0 The data look similar to the ego data frame but there are important differences. In this case, each row corresponds to a different named alter. Each alter is denoted by an alter id (ALTERID), unique to that respondent (based on CASEID). We see similar attributes as with the ego data. There is also information on the relationship between ego and each alter. For example, the KIN variable shows if ego is kin with that alter. As an example, we can see that respondent 1 (CASEID = 19850001) names 5 alters. The first alter (ALTERID = 1) is 32, has 18 years of education, and is not kin to ego. Note that the number of rows in the two data frames is not the same. nrow(ego_dat) ## [1] 1531 nrow(alter_dat) ## [1] 4483 We can see that there are 1531 respondents in the ego data frame and 4483 alters in the alter data frame. Finally, we read in the alter-alter tie data. url3 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/gss1985_alteralter_dat.csv&quot; alteralter_dat &lt;- read.csv(file = url3) alteralter_dat[1:10, ] ## CASEID ALTER1 ALTER2 WEIGHT ## 1 19850001 1 2 2 ## 2 19850001 1 3 1 ## 3 19850001 1 4 1 ## 4 19850001 1 5 1 ## 5 19850001 2 3 2 ## 6 19850001 2 4 2 ## 7 19850001 2 5 2 ## 8 19850001 3 4 1 ## 9 19850001 3 5 1 ## 10 19850001 4 5 1 This data frame captures the ties between the named alters (as reported on by the respondent). We see four columns. The first column defines the relevant ego using CASEID. ALTER1 defines the first alter in the dyad and ALTER2 defines the second. These ids must correspond to the ALTERID on the alter data. Weight defines the nature of the relationship. A value of 1 means that the alters know each other while a value of 2 means they are especially close. If there is no relationship between i and j then that dyad will not show up in this data frame. Our main substantive question is about the strength of homophily on different demographic dimensions. Homophily is a fundamental feature of social networks, as individuals tend to interact with similar others. Homophily in networks emerges because of opportunities (e.g., individuals with higher incomes may live in similar neighborhoods, belong to the same clubs, etc.), differences in resources across groups, and preferences (people select friends who are similar to themselves). By examining ego network data, we can uncover quite a bit about the social boundaries that exist in society. We can ask, for example, if race is a more important social dimension than say, gender. This would be reflected in the fact that cross-race ties are rare while cross-gender ties are comparatively common. We may also want to know how the strength of homophily varies across kinds of relationships; for example kin compared to non-kin ties. This is important when trying to understand how resources, like emotional support and advice, are shared (or not) across groups. Before we dive into examining patterns of homophily, it will be useful to do a bit of data management and network construction using the functions in the egor package. We will also do some basic visualizations. The first challenge in analyzing ego network data is that we must transform traditional survey data into something that has the structure of a network, so that we can then utilize packages like igraph and sna. Our survey data will not look like traditional network inputs (matrices, edgelists, etc.) and each survey is likely to be different, complicating the task of putting together the ego networks. Luckily, the egor package has made the task of constructing ego networks from survey data much easier. We will utilize the basic functionality of the egor package throughout the tutorial. Let's go ahead and load the package. library(egor) The basic idea is to first construct an egor object from the survey data. We can then use that object to form igraph and/or network objects, to plot the networks and to calculate statistics of interest. The function we will use to construct the egor object is egor(). The egor() function assumes that you are inputting the data using three separate files. The main arguments are: alters = alter attributes data frame egos = ego attributes data frame aaties = alter-alter tie data frame alter_design = list of arguments to specify nomination information from survey ego_design = list of arguments to specify survey design of study ID.vars = list of variable names corresponding to key columns: ego = variable name for id of ego alter = variable name for id of alter (in alter data) source = variable name for 'sender' of tie in alter-alter data target = variable name for 'receiver' of tie in alter-alter data Here, we will use the three data frames read in above as the main inputs. We will also tell R that CASEID is the ego id variable and ALTERID is the id variable for alters, while ALTER1 and ALTER2 are the source/target variables in the alter-alter data frame. We also note that the maximum number of alters was set to 5. It is also possible to incorporate features of the survey, like sampling weights and strata, into the constructed egor object. For the sake of simplicity, we will not include these features here, but a researcher may want to in more formal analyses. egonetlist &lt;- egor(alters = alter_dat, egos = ego_dat, aaties = alteralter_dat, alter_design = list(max = 5), ID.vars = list(ego = &quot;CASEID&quot;, alter =&quot;ALTERID&quot;, source = &quot;ALTER1&quot;, target = &quot;ALTER2&quot;)) egonetlist ## # EGO data (active): 1,531 × 13 ## .egoID AGE EDUC RACE SEX RELIG AGE_CATEGORICAL EDUC_CATEGORICAL NUMGIVEN HAPPY HEALTH PARTYID WTSSALL ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 19850001 33 16 white male jewish 30s College 6 2 2 1 1.04 ## 2 19850002 49 19 white male catholic 40s Post Graduate 6 2 1 4 1.04 ## 3 19850003 23 16 white female jewish 20s College 5 2 1 1 1.04 ## 4 19850004 26 20 white female jewish 20s Post Graduate 5 2 2 0 0.518 ## 5 19850005 24 17 white female catholic 20s Post Graduate 5 2 2 2 0.518 ## # ℹ 1,526 more rows ## # ALTER data: 4,483 × 12 ## .altID .egoID AGE EDUC RACE SEX RELIG AGE_CATEGORICAL EDUC_CATEGORICAL TALKTO SPOUSE KIN ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 19850001 32 18 white male jewish 30s Post Graduate 2 2 0 ## 2 2 19850001 29 16 white female protestant 20s College 1 1 1 ## 3 3 19850001 32 18 white male jewish 30s Post Graduate 3 2 0 ## # ℹ 4,480 more rows ## # AATIE data: 4,880 × 4 ## .egoID .srcID .tgtID WEIGHT ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 19850001 1 2 2 ## 2 19850001 1 3 1 ## 3 19850001 1 4 1 ## # ℹ 4,877 more rows egor objects are constructed as tibbles, which are data frames built using the tidyverse logic. For those versed in the tidyverse, one can take advantage of all the functions, calls, etc. that go along with those kinds of objects. It is, however, not strictly necessary to know the syntax of the tidyverse to work with egor objects. Let's take a look at the egor object. names(egonetlist) ## [1] &quot;ego&quot; &quot;alter&quot; &quot;aatie&quot; We can see that the elements are made up of our three data frames. For example, here we look at the ego data, extracted from the egor object (just looking at the first five columns). egonetlist[[&quot;ego&quot;]][, 1:5] ## # A tibble: 1,531 × 5 ## .egoID AGE EDUC RACE SEX ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 19850001 33 16 white male ## 2 19850002 49 19 white male ## 3 19850003 23 16 white female ## 4 19850004 26 20 white female ## 5 19850005 24 17 white female ## 6 19850006 45 17 white male ## 7 19850007 44 18 white female ## 8 19850008 56 12 white female ## 9 19850009 85 7 white female ## 10 19850010 65 12 white female ## # ℹ 1,521 more rows Note that the id variable for ego has been renamed to .egoID (it was CASEID on the original data). Here, we look at the alter attributes (just for the first five columns). egonetlist[[&quot;alter&quot;]][, 1:5] ## # A tibble: 4,483 × 5 ## .altID .egoID AGE EDUC RACE ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 19850001 32 18 white ## 2 2 19850001 29 16 white ## 3 3 19850001 32 18 white ## 4 4 19850001 35 16 white ## 5 5 19850001 29 13 white ## 6 1 19850002 42 12 white ## 7 2 19850002 44 18 white ## 8 3 19850002 45 16 white ## 9 4 19850002 40 12 white ## 10 5 19850002 50 18 white ## # ℹ 4,473 more rows We can see that the alter id has been renamed .altID (from ALTERID on the original data). And now we look at the alter-alter ties. egonetlist[[&quot;aatie&quot;]] ## # A tibble: 4,880 × 4 ## .egoID .srcID .tgtID WEIGHT ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 19850001 1 2 2 ## 2 19850001 1 3 1 ## 3 19850001 1 4 1 ## 4 19850001 1 5 1 ## 5 19850001 2 3 2 ## 6 19850001 2 4 2 ## 7 19850001 2 5 2 ## 8 19850001 3 4 1 ## 9 19850001 3 5 1 ## 10 19850001 4 5 1 ## # ℹ 4,870 more rows We can see that the column names for the alter-alter ties have also been renamed from the input data. The variables are now .srcID and .tgtID (rather than ALTER1 and ALTER2, as on the original data). Let's begin by calculating a simple summary statistic on our egor object. We will begin with density, showing the proportion of alter-alter ties that exist in each ego network, relative to the number of possible ties (note that all ego networks of size 0 or 1 will have NAs for density). The function is ego_density(). dens &lt;- ego_density(egonetlist) head(dens) ## # A tibble: 6 × 2 ## .egoID density ## &lt;int&gt; &lt;dbl&gt; ## 1 19850001 1 ## 2 19850002 0.8 ## 3 19850003 0.6 ## 4 19850004 0.6 ## 5 19850005 1 ## 6 19850006 0.667 The density scores are stored in the object as density. For example, respondent 1 (19850001) has 5 alters and all 10 possible ties exist (density = 1), while respondent 2 (1950002) has 5 alters but only 8 ties exist (density = .8). To check: alteralter_dat[alteralter_dat$CASEID == 19850001, ] ## CASEID ALTER1 ALTER2 WEIGHT ## 1 19850001 1 2 2 ## 2 19850001 1 3 1 ## 3 19850001 1 4 1 ## 4 19850001 1 5 1 ## 5 19850001 2 3 2 ## 6 19850001 2 4 2 ## 7 19850001 2 5 2 ## 8 19850001 3 4 1 ## 9 19850001 3 5 1 ## 10 19850001 4 5 1 alteralter_dat[alteralter_dat$CASEID == 19850002, ] ## CASEID ALTER1 ALTER2 WEIGHT ## 11 19850002 1 2 1 ## 12 19850002 1 3 1 ## 13 19850002 1 4 2 ## 14 19850002 1 5 2 ## 15 19850002 2 4 2 ## 16 19850002 3 4 2 ## 17 19850002 3 5 1 ## 18 19850002 4 5 2 6.2 Plotting Networks Here will go over how to plot the ego networks using packages like igraph. Plotting the ego networks will give us an initial view of the data and will help inform the analysis to come. library(igraph) The first step in making use of the functionality of igraph is to convert the information in the egor object to igraph objects. We do this using the as_igraph() function. We would use an as_network() function if we wanted to construct networks in the network format. igraph_nets &lt;- as_igraph(egonetlist) Now, let’s take a look at the first three ego networks. igraph_nets[1:3] ## $`19850001` ## IGRAPH e6da677 UN-- 5 10 -- ## + attr: .egoID (g/n), name (v/c), AGE (v/n), EDUC (v/n), RACE (v/c), SEX (v/c), RELIG (v/c), AGE_CATEGORICAL (v/c), EDUC_CATEGORICAL (v/c), TALKTO (v/n), SPOUSE (v/n), KIN (v/n), WEIGHT (e/n) ## + edges from e6da677 (vertex names): ## [1] 1--2 1--3 1--4 1--5 2--3 2--4 2--5 3--4 3--5 4--5 ## ## $`19850002` ## IGRAPH 0897905 UN-- 5 8 -- ## + attr: .egoID (g/n), name (v/c), AGE (v/n), EDUC (v/n), RACE (v/c), SEX (v/c), RELIG (v/c), AGE_CATEGORICAL (v/c), EDUC_CATEGORICAL (v/c), TALKTO (v/n), SPOUSE (v/n), KIN (v/n), WEIGHT (e/n) ## + edges from 0897905 (vertex names): ## [1] 1--2 1--3 1--4 1--5 2--4 3--4 3--5 4--5 ## ## $`19850003` ## IGRAPH 983be9d UN-- 5 6 -- ## + attr: .egoID (g/n), name (v/c), AGE (v/n), EDUC (v/n), RACE (v/c), SEX (v/c), RELIG (v/c), AGE_CATEGORICAL (v/c), EDUC_CATEGORICAL (v/c), TALKTO (v/n), SPOUSE (v/n), KIN (v/n), WEIGHT (e/n) ## + edges from 983be9d (vertex names): ## [1] 1--2 1--3 1--4 2--3 2--4 3--4 We have a list of ego networks (in the igraph format), with each ego network in a different element in the list. We can see that the information on the alters was automatically passed to the igraph objects, as was the information on the weights for the alter-alter ties. Note that by default the igraph objects will not include ego. Ego is often (but not always) excluded from visualizations and calculations because ego is, by definition, tied to all alters. Including ego thus offers little additional structural information. We will consider measures below that incorporate both ego and alter information (see Section 6.3). As with all igraph objects, we can extract useful information, like the attributes of the nodes. As an example, let's extract information on sex of alters from the first ego network. vertex_attr(igraph_nets[[1]], &quot;SEX&quot;) ## [1] &quot;male&quot; &quot;female&quot; &quot;male&quot; &quot;male&quot; &quot;female&quot; This is the same information as: alter_dat[alter_dat$CASEID == 19850001, &quot;SEX&quot;] ## [1] &quot;male&quot; &quot;female&quot; &quot;male&quot; &quot;male&quot; &quot;female&quot; Now, let's plot a couple of example networks, focusing on the first 3 ego networks. par(mfrow = c(1, 3)) purrr::walk(igraph_nets[1:3], plot) It is also possible to generate the code using lapply: lapply(igraph_nets[1:3], plot). lapply() will perform a given function, here plot(), over every element of an input list, here the first three elements of igraph_nets. Now, let's see if we can color the nodes to make the plot more informative. Let's color the nodes based on gender, so we can get a sense of the level of gender homogeneity in these ego networks. This is a somewhat more difficult task than with a single network (see visualization lab), as we need to extract and use the gender attributes over multiple networks. Here we will write a little function to perform this task and then apply it over the first three ego networks. plotfunc_colorgender &lt;- function(nets){ # Arguments: # nets: ego network of interest # extracting the attribute from the ego network: cols &lt;- vertex_attr(nets, &quot;SEX&quot;) # now we use an ifelse statement to set color, # light sky blue if gender equals female, blue otherwise: cols &lt;- ifelse(cols == &quot;female&quot;, &quot;lightskyblue&quot;, &quot;blue&quot;) # plotting ego network with nodes colored based on gender: plot(nets, vertex.color = cols) } Now let’s run plotfunc_colorgender() over the first three ego networks. par(mfrow = c(1, 3)) purrr::walk(igraph_nets[1:3], plotfunc_colorgender) We can see that the third ego network is homogenous in terms of gender, but the first two are quite heterogeneous. We also see different patterns of ties between the named alters. The first ego network is complete, with a tie between all alter-alter pairs. In contrast, the third ego network is less dense, with one alter (id = 5) completely disconnected from the rest of the alters. 6.3 Homophily: Ego-Alter Attributes We now move to our substantive questions about the strength and patterning of homophily. We will focus on gender and racial homophily, comparing across kin and non-kin ties. The first part of the analysis (Section 6.3.1 and 6.3.2) will utilize the ego-alter information, looking at the level of similarity between ego and alter for each ego network. 6.3.1 Proportion of Alters Matching Ego We will begin by calculating ego-level summary measures. Our first measure of interest, calculated for each ego, is the proportion of alters that ego matches on for the attribute of interest. For example, if ego is male, what proportion of the named alters are also male? The egor package has a number of built-in functions, like ego_density(), that make it easy to calculate summary statistics. In many cases, however, we will want to calculate a measure that is not currently included in egor. In this case, we can write our own function and then apply it to the egor object using a comp_ply() function (part of the egor package). That is what we will do here. Let's first write a little function that will compare ego's attribute to the attributes of the alters and calculate the proportion of times they match. prop_same_function &lt;- function(alt.attr, ego.attr){ # Arguments: # alt.attr: alter attributes for a given ego # ego.attr: ego attributes # taking ego attribute and comparing to alter # attributes, summing up number of times they match # ignoring missing data: same &lt;- sum(ego.attr == alt.attr, na.rm = T) # calculating proportion of ego-alter pairs that match: # just for alters with no missing data prop_same &lt;- same / sum(!is.na(alt.attr)) # making sure if ego is missing, then prop_same is also missing prop_same[is.na(ego.attr)] &lt;- NA return(prop_same) } Note that any ego with no alters or with missing data for the alter attributes will return an NA. We start by looking at the gender variable for all ties, including both kin and non-kin ties. We will calculate the proportion matching on gender by using our function (prop_same_function()) within a comp_ply() function. The main arguments to comp_ply() are: the ego object alt.attr = the alter attribute of interest .f = the function of interest ego.attr = the ego attribute of interest, if required Here the alter and ego attribute are SEX and the function of interest is prop_same_function(). pmatch_sex &lt;- comp_ply(egonetlist, alt.attr = &quot;SEX&quot;, .f = prop_same_function, ego.attr = &quot;SEX&quot;) head(pmatch_sex) ## # A tibble: 6 × 2 ## .egoID result ## &lt;int&gt; &lt;dbl&gt; ## 1 19850001 0.6 ## 2 19850002 0.6 ## 3 19850003 1 ## 4 19850004 0.4 ## 5 19850005 0.4 ## 6 19850006 0.75 With the comp_ply() function, the calculated values are stored as result. We can see that the first two respondents match with .6 of the alters and the third matches with all of the named alters. We can go back to the raw data to spot check that this is correct. For example, respondent 1 is male (ego_dat[ego_dat$CASEID == 19850001, \"SEX\"]) and has 3 male and 2 female alters (alter_dat[alter_dat$CASEID == 19850001, \"SEX\"]), thus matching with .6 of the alters. Now, let's do the same thing, but only consider ties that are based on kin relations. We still want to calculate homophily for gender, but we will only consider ego-alter pairs that have a kin tie. The first step is to create a new egor object that subsets the alter data to just those cases where a kin relation exists. Here, we will use a subset() function. The main inputs are the original egor object, the condition of interest and the unit to subset on. In this case, the condition of interest is where the KIN variable (on the alter attribute data frame) is equal to 1. The unit is the alter, as we only want to keep some of the alters (those with a kin tie to ego). egonetlist_kin &lt;- subset(egonetlist, egonetlist[[&quot;alter&quot;]]$KIN == 1, unit = &quot;alter&quot;) Now we rerun our function as before, but use the kin-based egor object. pmatch_sex_kin &lt;- comp_ply(egonetlist_kin, alt.attr = &quot;SEX&quot;, .f = prop_same_function, ego.attr = &quot;SEX&quot;) Here we do the same thing again, but for non-kin relations (so only keeping those alters with a non-kin tie to ego). egonetlist_nonkin &lt;- subset(egonetlist, egonetlist[[&quot;alter&quot;]]$KIN == 0, unit = &quot;alter&quot;) pmatch_sex_nonkin &lt;- comp_ply(egonetlist_nonkin, alt.attr = &quot;SEX&quot;, .f = prop_same_function, ego.attr = &quot;SEX&quot;) And now let's put those vectors together into a single data frame, just extracting the result part of the output. sexdat &lt;- data.frame(pmatch_sex$result, pmatch_sex_kin$result, pmatch_sex_nonkin$result) head(sexdat) ## pmatch_sex.result pmatch_sex_kin.result pmatch_sex_nonkin.result ## 19850001 0.60 0.5 0.6666667 ## 19850002 0.60 0.0 0.7500000 ## 19850003 1.00 1.0 1.0000000 ## 19850004 0.40 NaN 0.4000000 ## 19850005 0.40 0.4 NaN ## 19850006 0.75 1.0 0.6666667 Each row shows the proportion of alters who have the same gender as ego (one row for each ego). The second and third columns show the results for kin ties and non-kin ties only. Now, we will quickly do the same calculations for race and then compare the results with those for gender. The only inputs that change are the alter and ego attribute name. pmatch_race &lt;- comp_ply(egonetlist, alt.attr = &quot;RACE&quot;, .f = prop_same_function, ego.attr = &quot;RACE&quot;) #all alters pmatch_race_kin &lt;- comp_ply(egonetlist_kin, alt.attr = &quot;RACE&quot;, .f = prop_same_function, ego.attr = &quot;RACE&quot;) #kin pmatch_race_nonkin &lt;- comp_ply(egonetlist_nonkin, alt.attr = &quot;RACE&quot;, .f = prop_same_function, ego.attr = &quot;RACE&quot;) #nonkin racedat &lt;- data.frame(pmatch_race$result, pmatch_race_kin$result, pmatch_race_nonkin$result) head(racedat) ## pmatch_race.result pmatch_race_kin.result pmatch_race_nonkin.result ## 19850001 1 1 1 ## 19850002 1 1 1 ## 19850003 1 1 1 ## 19850004 1 NaN 1 ## 19850005 1 1 NaN ## 19850006 1 1 1 Now, let's do a simple summary over the columns in each data frame using the apply() function. apply() allows us to do a specified function for each column or row of a matrix (it will automatically be coerced into a matrix if it is a data frame). This is the equivalent of lapply() for matrices. Here we will set MARGIN to 2 to take a summary over the columns and set FUN (the function of interest) to summary. apply(sexdat, MARGIN = 2, FUN = summary) ## pmatch_sex.result pmatch_sex_kin.result pmatch_sex_nonkin.result ## Min. 0.0000000 0.0000000 0.000000 ## 1st Qu. 0.4000000 0.0000000 0.500000 ## Median 0.6000000 0.5000000 1.000000 ## Mean 0.5917264 0.4647869 0.738193 ## 3rd Qu. 0.8000000 0.6666667 1.000000 ## Max. 1.0000000 1.0000000 1.000000 ## NA&#39;s 137.0000000 405.0000000 557.000000 apply(racedat, MARGIN = 2, FUN = summary) ## pmatch_race.result pmatch_race_kin.result pmatch_race_nonkin.result ## Min. 0.0000000 0.0000000 0.0000000 ## 1st Qu. 1.0000000 1.0000000 1.0000000 ## Median 1.0000000 1.0000000 1.0000000 ## Mean 0.9477834 0.9690074 0.9294502 ## 3rd Qu. 1.0000000 1.0000000 1.0000000 ## Max. 1.0000000 1.0000000 1.0000000 ## NA&#39;s 140.0000000 406.0000000 561.0000000 We can see that there are clear differences by relation and demographic characteristic. Starting with gender, the mean value for proportion matching is .592. This value decreases to .465 for kin ties and increases to .738 for non-kin ties. Thus, individuals tend to have much more homogenous ego networks for gender in terms of non-kin ties. This follows as kin ties (such as marriage, parent-child, etc.) tend to cut across gender lines. Race offers a very different story. The overall level of matching is quite high (over .9) and is actually a little higher in the kin case than in the non-kin case. This would suggest that racial/ethnic boundaries, while quite strong in this data, are a bit weaker in cases of friendship, etc. which extend beyond marital and family relations. More generally, we see that race is a much more salient dimension than gender, with many respondents matching perfectly with all members of their network along racial lines, but much less so with gender, where differences between ego and alter are more common. 6.3.2 Analyzing Homophily at the Dyadic Level: Ego-Alter Pairs So far, we have summarized the level of homophily within each ego network. It is often of interest, however, to look at the overall rates of interaction between groups. We may be interested in knowing how likely a tie is to exist between two people who match on gender and/or race (e.g., Smith, McPherson, and Smith-Lovin (2014)). This necessitates thinking about the data in dyadic terms, where we want to summarize the rates of interaction between two different groups (male/female) looking at all ego-alter pairs in the data. For example, what proportion of ties are homogenous along gender lines? As a first step, we will the take the egor object and create a data frame that captures all ego-alter pairs. Here we can use the as_alters_df() function. We use our egor object as input and tell the function to also include ego attributes, which we need in this case. ego_alter_dat &lt;- data.frame(as_alters_df(egonetlist, include.ego.vars = TRUE)) We now have a data frame where each row corresponds to a different ego-alter pair. Let's take a look at the first six rows: head(ego_alter_dat) ## .altID .egoID AGE EDUC RACE SEX RELIG AGE_CATEGORICAL EDUC_CATEGORICAL TALKTO SPOUSE KIN AGE_ego EDUC_ego RACE_ego SEX_ego RELIG_ego AGE_CATEGORICAL_ego EDUC_CATEGORICAL_ego NUMGIVEN_ego HAPPY_ego HEALTH_ego PARTYID_ego WTSSALL_ego ## 1 1 19850001 32 18 white male jewish 30s Post Graduate 2 2 0 33 16 white male jewish 30s College 6 2 2 1 1.0363 ## 2 2 19850001 29 16 white female protestant 20s College 1 1 1 33 16 white male jewish 30s College 6 2 2 1 1.0363 ## 3 3 19850001 32 18 white male jewish 30s Post Graduate 3 2 0 33 16 white male jewish 30s College 6 2 2 1 1.0363 ## 4 4 19850001 35 16 white male jewish 30s College 3 2 1 33 16 white male jewish 30s College 6 2 2 1 1.0363 ## 5 5 19850001 29 13 white female catholic 20s Some College 2 2 0 33 16 white male jewish 30s College 6 2 2 1 1.0363 ## 6 1 19850002 42 12 white female catholic 40s HS 1 1 1 49 19 white male catholic 40s Post Graduate 6 2 1 4 1.0363 Note that the columns associated with ego have a _ego after it (RACE_ego, SEX_ego, and so on) while the variables corresponding to the alters do not (RACE, SEX). We can see that the ego values are repeated N times, for each N alter that they name. It will be useful to rename the alter variables to have a \"_alter\" addition, to be consistent with the ego variables. Let's go ahead and change the alter column names. First we grab the column names on the ego-alter data frame. cnames &lt;- colnames(ego_alter_dat) Now we identify which columns are the alter columns, running from AGE to EDUC_CATEGORICAL: which_age_column &lt;- which(colnames(ego_alter_dat) == &quot;AGE&quot;) which_educ_cat_column &lt;- which(colnames(ego_alter_dat) == &quot;EDUC_CATEGORICAL&quot;) alter_columns &lt;- which_age_column:which_educ_cat_column Here we add _alter to the alter column names using a paste command: cnames[alter_columns] &lt;- paste(cnames[alter_columns], &quot;_alter&quot;, sep = &quot;&quot;) And finally we put the new variable names on the ego-alter data frame. colnames(ego_alter_dat) &lt;- cnames head(ego_alter_dat) ## .altID .egoID AGE_alter EDUC_alter RACE_alter SEX_alter RELIG_alter AGE_CATEGORICAL_alter EDUC_CATEGORICAL_alter TALKTO SPOUSE KIN AGE_ego EDUC_ego RACE_ego SEX_ego RELIG_ego AGE_CATEGORICAL_ego EDUC_CATEGORICAL_ego NUMGIVEN_ego HAPPY_ego HEALTH_ego PARTYID_ego WTSSALL_ego ## 1 1 19850001 32 18 white male jewish 30s Post Graduate 2 2 0 33 16 white male jewish 30s College 6 2 2 1 1.0363 ## 2 2 19850001 29 16 white female protestant 20s College 1 1 1 33 16 white male jewish 30s College 6 2 2 1 1.0363 ## 3 3 19850001 32 18 white male jewish 30s Post Graduate 3 2 0 33 16 white male jewish 30s College 6 2 2 1 1.0363 ## 4 4 19850001 35 16 white male jewish 30s College 3 2 1 33 16 white male jewish 30s College 6 2 2 1 1.0363 ## 5 5 19850001 29 13 white female catholic 20s Some College 2 2 0 33 16 white male jewish 30s College 6 2 2 1 1.0363 ## 6 1 19850002 42 12 white female catholic 40s HS 1 1 1 49 19 white male catholic 40s Post Graduate 6 2 1 4 1.0363 With the ego-alter data in hand, we can begin to explore the pattern of homophily in the data. We start by creating a simple table of ego gender by alter gender. Each ego will be included in the table multiple times (once for each named alter). sextab &lt;- table(ego_alter_dat[, &quot;SEX_ego&quot;], ego_alter_dat[, &quot;SEX_alter&quot;]) sextab ## ## female male ## female 1515 970 ## male 748 1245 We can see from the table that out of all ego-alter pairs, 1515 are female-female, 1245 are male-male and so on. We can use this information to calculate a number of useful items. For example, we can calculate the proportion of ties that match on gender. We take the number of ego-alter pairs that match (1515 + 1245) and divide that by the total number of ego-alter pairs (1515 + 970 + 748 + 1245). sum(diag(sextab)) / sum(sextab) ## [1] 0.6163466 We can see that over all ego-alter pairs, .616 are of the same gender. And now for race: racetab &lt;- table(ego_alter_dat[, &quot;RACE_ego&quot;], ego_alter_dat[, &quot;RACE_alter&quot;]) sum(diag(racetab)) / sum(racetab) ## [1] 0.948781 We can see that about .949 of ego-alter pairs match on race, much higher than for gender. Tables like these offer a useful starting point for an analysis of homophily. The tables are limited, however, as they only tell us about the existing ties in the data. There is no sense of what the rate of homophily looks like compared to baseline, or chance, expectations. For example, we may want to know if gender homophily is stronger than what we would observe if people randomly formed ties (i.e., with no consideration of gender). By comparing to random, baseline expectations, we can get a better picture of the salience of the demographic characteristic in question. There are a number of ways to compare a raw mixing table, like that in sextab, to a specified baseline expectation. For example, we can use log-linear models, case-control models, or exponential random graph models. Here we will do a very simple analysis, calculating the odds of an observed tie matching on gender (or race, etc.) compared to the odds of matching by chance. To form the chance expectations, we first pair each case in the data with each other. We then take these random pairings and calculate the odds of two people matching on gender (or race). The odds of matching in the actual ego-alter pairs is then compared to the odds of matching based on these random pairings. Here, we will write a little function to calculate the odds ratio of interest. oddsratio_function &lt;- function(egoalter_tab, attribute) { # Arguments: # egoalter_tab: table of interest # attribute: vector representing attribute of interest in the sample # We first calculate the number of dyads that match and mismatch on the # attribute based on the observed data. # We calculate the number matching by summing up over the diagonal of the # the table (as these are cases where ego and alter have the same value). match &lt;- sum(diag(egoalter_tab)) # We now calculate the number mismatching by taking the total number of # dyads and subtracting the number that match (calculated above). notmatch &lt;- sum(egoalter_tab) - match # Now we calculate our chance expectations, defined as what would happen # if we randomly paired all respondents from the data (defined in the # input attribute vector), and calculated how many of those # pairs were the same (and different) on the attribute of interest. # We first do a table on the attribute, to see how many people fall # into each category. freq &lt;- table(attribute) # We next calculate the total number of random pairings, # assuming we pair all respondents with all respondents. # This is simply the number of respondents times the number of # respondents minus 1, as we assume we are not pairing # people with themself. total_dyads &lt;- sum(freq) * (sum(freq) - 1) # We now calculate the number of random pairings expected to # match by chance. # Formally, we take the number in each category (e.g., number of men) and # multiply that number by itself (minus 1 as we again # assume people are not paired with themself), showing # how many pairs would be the same if we paired all respondents with all # respondents. Remember that R will multiply things element-wise, # so the following bit of code will take the first value in freq # and multiply it by the first element in freq-1. We sum up the values # to get the total number of dyads that are expected to match. match_chance &lt;- sum(freq * (freq - 1)) # We now calculate the number of dyads not matching by chance as the # difference between the total number of dyads and # those matching by chance. notmatch_chance &lt;- total_dyads - match_chance # And finally we can calculate the odds ratio of # observed odds of matching to odds of matching by chance or &lt;- (match * notmatch_chance) / (notmatch * match_chance) return(or) } Now, let's use our little function to calculate the odds of a tie matching on gender and race, relative to chance. The two inputs are the table of ego-alter characteristics (sextab or racetab) and the input vector of attributes, showing the distribution of gender or race in the sample. Here, our baseline expectations will be based on randomly pairing all cases in the original sample. We thus set the attribute argument to the sex or race values from the ego data frame. oddsratio_function(egoalter_tab = sextab, attribute = ego_dat[, &quot;SEX&quot;]) ## [1] 1.575119 oddsratio_function(egoalter_tab = racetab, attribute = ego_dat[, &quot;RACE&quot;]) ## [1] 7.379298 The results show that the odds of a tie matching on gender is 1.575 times higher than what we expect by chance. The odds ratio is much higher for race (7.379), suggesting that even net of chance expectations, race is the more salient dimension. More generally, we see that individuals are not forming ties at random in regard to race or gender (as the observed rate of matching is well above what we expect by chance). Let's also calculate the odds ratios for kin and non-kin ties, seeing how the odds of matching differ by relation. The main difference from above is that we need to subset the ego-alter data to just include kin (or non-kin) ego-alter pairs. The rest is analogous to the code from above. For these analyses we will use all cases to form the chance expectations, but we could do analogous calculations just looking at those with kin (or non-kin) ties. #odds of matching for kin ties, gender kin_sextab &lt;- table(ego_alter_dat[ego_alter_dat$KIN == 1, &quot;SEX_ego&quot;], ego_alter_dat[ego_alter_dat$KIN == 1, &quot;SEX_alter&quot;]) oddsratio_function(egoalter_tab = kin_sextab, attribute = ego_dat[, &quot;SEX&quot;]) ## [1] 0.9580393 #odds of matching for kin ties, race kin_racetab &lt;- table(ego_alter_dat[ego_alter_dat$KIN == 1, &quot;RACE_ego&quot;], ego_alter_dat[ego_alter_dat$KIN == 1, &quot;RACE_alter&quot;]) oddsratio_function(egoalter_tab = kin_racetab, attribute = ego_dat[, &quot;RACE&quot;]) ## [1] 14.12947 #odds of matching for non-kin ties, gender nonkin_sextab &lt;- table(ego_alter_dat[ego_alter_dat$KIN == 0, &quot;SEX_ego&quot;], ego_alter_dat[ego_alter_dat$KIN == 0, &quot;SEX_alter&quot;]) oddsratio_function(egoalter_tab = nonkin_sextab, attribute = ego_dat[, &quot;SEX&quot;]) ## [1] 2.937705 #odds of matching for non-kin ties, race nonkin_racetab &lt;- table(ego_alter_dat[ego_alter_dat$KIN == 0, &quot;RACE_ego&quot;], ego_alter_dat[ ego_alter_dat$KIN == 0, &quot;RACE_alter&quot;]) oddsratio_function(egoalter_tab = nonkin_racetab, attribute = ego_dat[, &quot;RACE&quot;]) ## [1] 4.758637 As before, we see that the odds of matching on race are much higher than matching on gender for kin ties, while the differences are much less pronounced for non-kin ties. We have thus far employed very simple models to construct the baseline expectations. More complex models are possible, however. For example, we could construct our baseline expectations to take into account degree differences between groups; i.e., if men have larger ego networks, they would be represented more heavily when forming the random pairs of respondents (and perhaps that would increase the rate of matching expected by chance). Or, we may want to control for isolation status when constructing our baseline expectations. We would still ask what happens if people randomly formed ties, but now we adjust for the fact that some people do not form any ties at all. Here, we recalculate the odds ratio for gender matching, but only consider non-isolates when forming the baseline expectations. oddsratio_function(egoalter_tab = sextab, attribute = ego_dat$SEX[ego_dat$NUMGIVEN &gt; 0]) ## [1] 1.571536 6.4 Homophily: Alter Attributes So far we have looked exclusively at the ego-alter data, seeing if ego is similar to the named alters on gender and race. It is also possible to focus exclusively on the alters. Here, we ignore the ego attribute and simply summarize the alter attributes within each ego network. Such measures are often used as predictors of other outcomes, like political attitudes and mental health (see below for an example). For example, we may want to know how diverse the ego network is, or how similar/different the alters are to each other. Is ego surrounded by people who are all the same? One summary measure is Shannon entropy, equal to: -1 * (sum(pi * log(pi)) where pi is the proportion of alters in category i. This is a measure of diversity, with larger values meaning the ego network is more diverse. The function to calculate diversity is alts_diversity_entropy(). The main inputs are the egor object, the alter attribute of interest (set using the alt.attr argument) and then a base value when calculating the log (here set to exp(1) using the base argument). We start with diversity for gender. sex_diversity &lt;- alts_diversity_entropy(egonetlist, alt.attr = &quot;SEX&quot;, base = exp(1)) head(sex_diversity) ## # A tibble: 6 × 2 ## .egoID entropy ## &lt;int&gt; &lt;dbl&gt; ## 1 19850001 0.673 ## 2 19850002 0.673 ## 3 19850003 0 ## 4 19850004 0.673 ## 5 19850005 0.673 ## 6 19850006 0.562 The calculated statistic is stored as entropy. We can see that the first two respondents have high diversity (as the ego networks are nearly half male and half female) while the third respondent has a value of 0 (as all alters are female). And now we do the same thing for race. race_diversity &lt;- alts_diversity_entropy(egonetlist, alt.attr = &quot;RACE&quot;, base = exp(1)) It is important to note that the theoretical maximum is higher in the race case as the number of categories is higher. Also note that the function will yield a 0 for cases where there are no alters or 1 alter, which may not be desirable. To put in NAs for our isolates and those with 1 alter: sex_diversity[ego_dat$NUMGIVEN &lt;= 1, &quot;entropy&quot;] &lt;- NA race_diversity[ego_dat$NUMGIVEN &lt;= 1, &quot;entropy&quot;] &lt;- NA And now we can go ahead and look at the summary statistics for diversity. summary(sex_diversity[, &quot;entropy&quot;]) ## entropy ## Min. :0.0000 ## 1st Qu.:0.5004 ## Median :0.6365 ## Mean :0.4898 ## 3rd Qu.:0.6730 ## Max. :0.6931 ## NA&#39;s :364 summary(race_diversity[, &quot;entropy&quot;]) ## entropy ## Min. :0.0000 ## 1st Qu.:0.0000 ## Median :0.0000 ## Mean :0.0507 ## 3rd Qu.:0.0000 ## Max. :1.3322 ## NA&#39;s :364 We can see that the diversity measures are much lower for race than sex, suggesting that the alters tend to be more homogenous in terms of race than sex (in keeping with our findings above). We can do similar summary measures for continuous variables, like education. For example, we may want to know what the mean (or standard deviation) of education is for the alters in each ego network. We will use a comp_ply() function again. The key inputs are the egor object, the alter attribute (set using the alt.attr argument) and then the function of interest, in this case the mean function (set using the .f argument). mean_altereduc &lt;- comp_ply(egonetlist, alt.attr = &quot;EDUC&quot;, .f = mean, na.rm = TRUE) summary(mean_altereduc[, &quot;result&quot;]) ## result ## Min. : 3.50 ## 1st Qu.:12.00 ## Median :12.50 ## Mean :12.89 ## 3rd Qu.:14.33 ## Max. :18.00 ## NA&#39;s :153 6.5 Ego Networks as Predictors In the section above we examined the properties of the ego networks, focusing mostly on racial and gender homophily. There are a number of other properties we could explore in more detail, like density or network size. For example, we might want to predict network size as a function of race, gender or other demographic characteristics. We can also use properties of the ego networks as predictors of other outcomes of interest. For example, let's try and predict the variable HAPPY using the features of the ego networks. Are individuals with larger ego networks happier? Does the composition of the ego network matter? Happy is coded as a 1 (very happy), 2 (pretty happy), 3 (not too happy). Let's add a label to the variable and reorder it to run from not happy to happy. ego_dat$HAPPY_FACTOR &lt;- factor(ego_dat$HAPPY, levels = c(3, 2, 1), labels = c(&quot;not too happy&quot;, &quot;pretty happy&quot;, &quot;very happy&quot;)) Let's also turn our race and sex variables into factors. We set white as the first category in our race variable. ego_dat$RACE_FACTOR &lt;- factor(ego_dat$RACE, levels = c(&quot;white&quot;, &quot;asian&quot;, &quot;black&quot;, &quot;hispanic&quot;, &quot;other&quot;)) ego_dat$SEX_FACTOR &lt;- factor(ego_dat$SEX) Let’s also put three of our calculated network measures, density, racial diversity and mean alter education, onto the ego data frame. ego_dat$DENSITY &lt;- dens[[&quot;density&quot;]] # getting values out of tibble format ego_dat$RACE_DIVERSITY &lt;- race_diversity[[&quot;entropy&quot;]] ego_dat$MEAN_ALTEREDUC &lt;- mean_altereduc[[&quot;result&quot;]] HAPPY is an ordinal variable. With ordinal outcome variables, it is best to utilize ordered logistic regression (or a similar model). We will need the polr() function in the MASS package to run these models. library(MASS) To simplify things, let's create a data frame that has no missing data on any of the variables we want to include in the full model. The outcome of interest is HAPPY_FACTOR and the main predictors are ego network size (NUMGIVEN), density (DENSITY) the mean education of the alter (MEAN_ALTEREDUC) and racial diversity of the alters (RACE_DIVERSITY). We also include a number of demographic controls. ego_dat_nomiss &lt;- na.omit(ego_dat[, c(&quot;HAPPY_FACTOR&quot;, &quot;NUMGIVEN&quot;, &quot;DENSITY&quot;, &quot;MEAN_ALTEREDUC&quot;, &quot;RACE_DIVERSITY&quot;, &quot;EDUC&quot;, &quot;AGE&quot;, &quot;RACE_FACTOR&quot;, &quot;SEX_FACTOR&quot;)]) Now, we are in a position to run our ordered logistic regression predicting happiness. For our first model we will predict happiness as a function of our two structural network features, ego network size and density. summary(happy_mod1 &lt;- polr(HAPPY_FACTOR ~ NUMGIVEN + DENSITY, data = ego_dat_nomiss)) ## ## Re-fitting to get Hessian ## Call: ## polr(formula = HAPPY_FACTOR ~ NUMGIVEN + DENSITY, data = ego_dat_nomiss) ## ## Coefficients: ## Value Std. Error t value ## NUMGIVEN 0.08905 0.04789 1.859 ## DENSITY 0.49492 0.20547 2.409 ## ## Intercepts: ## Value Std. Error t value ## not too happy|pretty happy -1.4143 0.2722 -5.1965 ## pretty happy|very happy 1.6190 0.2724 5.9435 ## ## Residual Deviance: 2080.162 ## AIC: 2088.162 The results suggest that respondents with dense networks report higher levels of happiness, while ego network size (NUMGIVEN) is not a significant predictor of happiness, controlling for density. The initial results would suggest that it is less about the number of people in your ego network that matters for happiness, and more about whether they know each other. And now let's add some controls for demographics. summary(happy_mod2 &lt;- polr(HAPPY_FACTOR ~ NUMGIVEN + DENSITY + EDUC + AGE + RACE_FACTOR + SEX_FACTOR, data = ego_dat_nomiss)) ## ## Re-fitting to get Hessian ## Call: ## polr(formula = HAPPY_FACTOR ~ NUMGIVEN + DENSITY + EDUC + AGE + ## RACE_FACTOR + SEX_FACTOR, data = ego_dat_nomiss) ## ## Coefficients: ## Value Std. Error t value ## NUMGIVEN 0.03867 0.04982 0.7761 ## DENSITY 0.53994 0.20849 2.5897 ## EDUC 0.06446 0.02180 2.9563 ## AGE 0.00244 0.00363 0.6722 ## RACE_FACTORasian 1.07909 0.75375 1.4316 ## RACE_FACTORblack -0.92220 0.24011 -3.8407 ## RACE_FACTORhispanic -0.40628 0.28247 -1.4383 ## RACE_FACTORother -0.33871 0.67691 -0.5004 ## SEX_FACTORmale 0.09657 0.12030 0.8027 ## ## Intercepts: ## Value Std. Error t value ## not too happy|pretty happy -0.7120 0.4383 -1.6242 ## pretty happy|very happy 2.3946 0.4438 5.3962 ## ## Residual Deviance: 2047.593 ## AIC: 2069.593 Controlling for education, race, age and sex, we see that density is still a significant predictor of happiness. Individuals with alters who are interconnected consistently report higher levels of happiness, showing the potential benefits of being part of an integrated social group. We also see that individuals with higher education tend to report higher levels of happiness, while those individuals identifying as black report lower levels of happiness. Finally, we will add our two compositional measures: mean alter education and racial diversity. summary(happy_mod3 &lt;- polr(HAPPY_FACTOR ~ NUMGIVEN + DENSITY + EDUC + AGE + RACE_FACTOR + SEX_FACTOR + MEAN_ALTEREDUC + RACE_DIVERSITY, data = ego_dat_nomiss)) ## ## Re-fitting to get Hessian ## Call: ## polr(formula = HAPPY_FACTOR ~ NUMGIVEN + DENSITY + EDUC + AGE + ## RACE_FACTOR + SEX_FACTOR + MEAN_ALTEREDUC + RACE_DIVERSITY, ## data = ego_dat_nomiss) ## ## Coefficients: ## Value Std. Error t value ## NUMGIVEN 3.654e-02 0.050117 0.72909 ## DENSITY 6.040e-01 0.211527 2.85561 ## EDUC 1.198e-02 0.026014 0.46034 ## AGE -9.306e-05 0.003691 -0.02521 ## RACE_FACTORasian 1.014e+00 0.765069 1.32518 ## RACE_FACTORblack -8.308e-01 0.242392 -3.42758 ## RACE_FACTORhispanic -1.823e-01 0.287255 -0.63450 ## RACE_FACTORother -1.382e-02 0.680679 -0.02030 ## SEX_FACTORmale 7.795e-02 0.120653 0.64609 ## MEAN_ALTEREDUC 1.294e-01 0.035478 3.64683 ## RACE_DIVERSITY -6.904e-01 0.361167 -1.91148 ## ## Intercepts: ## Value Std. Error t value ## not too happy|pretty happy 0.1813 0.5104 0.3552 ## pretty happy|very happy 3.3248 0.5211 6.3808 ## ## Residual Deviance: 2030.842 ## AIC: 2056.842 Our full model offers interesting insights when compared to model 2. We see that individuals surrounded by highly educated alters tend to report higher levels of happiness. This may be the case as individuals with higher education (especially in 1985) are less stressed, have better financial resources, etc. Being surrounded by such people may make it easier to feel happy oneself. We also see that the EDUC variable, capturing the respondent's education, is no longer a significant predictor. This suggests that happiness is less about having higher levels of education, and more about the social context in which one lives (see, for example: McPherson and Smith (2019)). Being surrounded by folks who are less stressed (for example) may lead to happiness, even if the respondent does not themself have high levels of education. We also see that the coefficient on racial diversity is negative, but it is not significant (at traditional levels). Thus, while there may be some tendency for respondents with more racially diverse networks to be less happy, our uncertainty in the estimate is too great to be sure the effect is real. 6.6 Working with One Input File We end the tutorial by demonstrating how to construct egor objects using a single file as the main input. This is directly analogous to part 1 above, but here we use data stored as a single file, rather than 3 separate files. A single file is a convenient and common way of storing ego network data. A single file does, however, make the task of constructing the egor object somewhat more difficult than with 3 separate files. Here, we will demonstrate how to take the ego network data, stored as one file, and turn into the three data frames we read in above, one for the ego information, one for the alter information, and one for the alter-alter tie information (see the onefile_to_egor() function for an alternative approach). Once we have the three data frames, we can use them to construct the egor object, as above. Let's go ahead and read in the data. The data are the same as above, just stored as one file. url4 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/gss1985_egonetworks.csv&quot; egonets &lt;- read.csv(file = url4) head(egonets) ## CASEID AGE EDUC RACE SEX RELIG AGE_CATEGORICAL EDUC_CATEGORICAL NUMGIVEN HAPPY HEALTH PARTYID WTSSALL AGE1 AGE2 AGE3 AGE4 AGE5 EDUC1 EDUC2 EDUC3 EDUC4 EDUC5 RACE1 RACE2 RACE3 RACE4 RACE5 SEX1 SEX2 SEX3 SEX4 SEX5 RELIG1 RELIG2 RELIG3 RELIG4 RELIG5 AGE_CATEGORICAL1 AGE_CATEGORICAL2 AGE_CATEGORICAL3 AGE_CATEGORICAL4 AGE_CATEGORICAL5 EDUC_CATEGORICAL1 EDUC_CATEGORICAL2 EDUC_CATEGORICAL3 EDUC_CATEGORICAL4 EDUC_CATEGORICAL5 TALKTO1 TALKTO2 TALKTO3 TALKTO4 TALKTO5 SPOUSE1 SPOUSE2 SPOUSE3 SPOUSE4 SPOUSE5 KIN1 KIN2 KIN3 KIN4 KIN5 CLOSE12 CLOSE13 CLOSE14 CLOSE15 CLOSE23 CLOSE24 CLOSE25 CLOSE34 CLOSE35 CLOSE45 ## 1 19850001 33 16 white male jewish 30s College 6 2 2 1 1.0363 32 29 32 35 29 18 16 18 16 13 white white white white white male female male male female jewish protestant jewish jewish catholic 30s 20s 30s 30s 20s Post Graduate College Post Graduate College Some College 2 1 3 3 2 2 1 2 2 2 0 1 0 1 0 2 1 1 1 2 2 2 1 1 1 ## 2 19850002 49 19 white male catholic 40s Post Graduate 6 2 1 4 1.0363 42 44 45 40 50 12 18 16 12 18 white white white white white female male male female male catholic jewish jewish catholic jewish 40s 40s 40s 40s 50s HS Post Graduate College HS Post Graduate 1 2 1 1 1 1 2 2 2 2 1 0 0 0 0 1 1 2 2 0 2 0 2 1 2 ## 3 19850003 23 16 white female jewish 20s College 5 2 1 1 1.0363 25 24 46 21 35 16 12 18 16 18 white white white white white female female female female female jewish jewish jewish jewish jewish 20s 20s 40s 20s 30s College HS Post Graduate College Post Graduate 2 2 2 1 2 2 2 2 2 2 0 0 1 1 0 1 1 1 NA 1 1 NA 2 NA 0 ## 4 19850004 26 20 white female jewish 20s Post Graduate 5 2 2 0 0.5181 26 27 28 27 25 18 18 18 18 16 white white white white white female male male male female none protestant none none jewish 20s 20s 20s 20s 20s Post Graduate Post Graduate Post Graduate Post Graduate College 1 1 3 2 1 2 2 2 2 2 0 0 0 0 0 1 0 0 1 0 1 1 1 0 1 ## 5 19850005 24 17 white female catholic 20s Post Graduate 5 2 2 2 0.5181 44 25 20 19 12 12 13 13 13 8 white white white white white female female male male male catholic catholic catholic catholic catholic 40s 20s 20s 20s 20s HS Some College Some College Some College No HS 2 2 3 3 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 ## 6 19850006 45 17 white male none 40s Post Graduate 4 2 2 1 0.5181 40 35 44 35 NA 16 16 18 16 NA white white white white &lt;NA&gt; male male male female &lt;NA&gt; protestant none none protestant &lt;NA&gt; 40s 30s 40s 30s &lt;NA&gt; College College Post Graduate College &lt;NA&gt; 4 2 2 1 NA 2 2 2 2 NA 1 0 0 0 NA 0 1 0 NA 1 1 NA 1 NA NA Each row is a respondent, reporting on their network alters. We see a number of columns that correspond to the attributes of the respondent, from CASEID all the way through PARTYID. These columns are the same as in ego_dat. For example, NUMGIVEN reflects the number of alters named (capped at 6+). There are also alter characteristics for age, education, race, religion and sex. For example, for age, we see AGE1, AGE2, AGE3, AGE4, AGE5, corresponding to the age of alter 1, 2, 3, etc. There is also information on the relationship between ego and each alter. For example, the KIN variables (KIN1, KIN2...) show if ego is family with alter 1, 2, and so on. This is analogous to the information in alter_dat, although organized column-wise rather than row-wise. Finally, CLOSE12, CLOSE13... captures the kind of relationship between alter 1-2, 1-3, etc. This corresponds to the information in alteralter_dat. Note that a value of 0 in the CLOSE variables means no relationship exists (1 = know each other and 2 = especially close). Before we construct our three data frames from the ego network data, it will be useful to define a column that corresponds to the size of the ego networks. Right now, the NUMGIVEN column shows a 6 if they have 6+ ties. But as they only report on 5 alters, we will define their actual network size as 5 if they report having 6+ alters. We will use the ifelse() function : egonets$netsize &lt;- ifelse(egonets$NUMGIVEN == 6, 5, egonets$NUMGIVEN) As before, we will take out any cases where they have NA for NUMGIVEN. egonets &lt;- egonets[!is.na(egonets$NUMGIVEN), ] We are now in a position to create our three data frames. We will begin with the ego data. Let's go ahead and grab the ego attribute information from egonets, our one file ego network data frame. This is a straightforward task and just requires subsetting the data to keep the columns of interest, corresponding to ego; in this case the first 13 columns in the data. ego_dat_fromonefile &lt;- egonets[, 1:13] This data is the same as ego_dat. Now we need to get the alter attribute information. The task is harder here, as we need to take data defined by ego, where each ego is a different row, and turn it into a data frame where the rows correspond to each named alter. This amounts to stacking the columns, moving from a 'wide' format to a 'long' format. We will write a little function to accomplish this task. We could also imagine employing functions like reshape() (in the reshape package) to do the same thing (see Chapter 3, Part 1). create_alter_data &lt;- function(egonets, netsize, var_names, egoID){ # Arguments: # egonets: ego network data frame # netsize: vector of ego network size # var_names: names of variables to put on alter data frame (assumed to be # var.name1, var.name2, for each alter) # egoID: name of id of ego on ego network data frame # creating empty list to hold output for each variable: overall_list &lt;- list() # taking out the isolates as they have no alters: egonets_noisolates &lt;- egonets[netsize &gt; 0, ] # redefining network size after taking out isolates: netsize_updated &lt;- netsize[netsize &gt; 0] # running over each variable name: for (p in 1:length(var_names)){ var_list &lt;- list() alter_list &lt;- list() # running over each ego: for (x in 1:nrow(egonets_noisolates)){ # getting alter id number: alter_nums &lt;- rep(1:netsize_updated[x], times = 1) # Now we grab the alter columns for that ego and put those # values in a larger list so we can stack them later on # into one big vector (one for each variable of interest) alt_cols &lt;- paste(rep(var_names[p], each = netsize_updated[x]), alter_nums, sep = &quot;&quot;) var_list[[x]] &lt;- egonets_noisolates[x, alt_cols] alter_list[[x]] &lt;- alter_nums } # stacking all alter values into one long column var &lt;- unlist(var_list) overall_list[[p]] &lt;- var } # putting all new variables together: dat &lt;- data.frame(do.call(data.frame, overall_list)) # putting useful column names on data frame colnames(dat) &lt;- var_names # adding egoID and alterID to data frame: dat &lt;- cbind(rep(egonets_noisolates[, egoID], netsize_updated), unlist(alter_list), dat) colnames(dat)[1:2] &lt;- c(egoID, &quot;alterID&quot;) return(dat) } Now we will use our function to get the alter data frame from our 'one file format' ego network data. The main arguments are egonets (the input data frame), netsize (a vector showing the ego network size) and var_names, the column names of interest. Here we restrict the column names to AGE, EDUC, RACE, SEX and RELIG. We also set the name for the egoID as CASEID. alter_dat_fromonefile &lt;- create_alter_data(egonets = egonets, netsize = egonets$netsize, var_names = c(&quot;AGE&quot;, &quot;EDUC&quot;, &quot;RACE&quot;, &quot;SEX&quot;, &quot;RELIG&quot;), egoID = &quot;CASEID&quot;) head(alter_dat_fromonefile) ## CASEID alterID AGE EDUC RACE SEX RELIG ## 1 19850001 1 32 18 white male jewish ## 2 19850001 2 29 16 white female protestant ## 3 19850001 3 32 18 white male jewish ## 4 19850001 4 35 16 white male jewish ## 5 19850001 5 29 13 white female catholic ## 6 19850002 1 42 12 white female catholic We can see that the result of this function is a data frame with alters on the rows and the attributes of interest on the columns. We also have the id of the ego for each alter. This data frame is the same as alter_dat. As a final step, we need to create our alter-alter tie information data frame. Again, we will write a little function to accomplish this. create_alteralter_data &lt;- function(egonets, netsize, aa_tie_data, egoID, max_alter){ # Arguments: # egonets: ego network data frame # netsize: vector of ego network size # aa_tie_data : data for each ego showing ties between alters; # assumed to be ordered by 1-2; 1-3; 1-4; 1-5, etc. on the columns # egoID: name of id of ego on ego network data frame # max_alter: maximum of number of alter of which alter-alter tie # data was reported on overall_list &lt;- list() # taking out the isolates and those # with only one alter as they have no alter-alter ties egonets_noisolates &lt;- egonets[netsize &gt; 1, ] # also taking out the # isolates and those with only one alter # for the alter-alter tie input data alteralter_tie_data &lt;- aa_tie_data[netsize &gt; 1, ] # redefining network size after taking out isolates: netsize_updated &lt;- netsize[netsize &gt; 1] # defining possible alter-alter ties alter_ids &lt;- t(combn(max_alter, 2)) # running over each ego: for (x in 1:nrow(egonets_noisolates)){ # First we create a data frame based on the ego ids, the possible # alter-alter ties and the weights for each alter-alter tie, # based on the input data for that ego alter_dat_row &lt;- data.frame(egoID = egonets_noisolates[x, egoID], alter_ids, weight = unlist(alteralter_tie_data[x, ])) # Here we reduce some of the rows (corresponding to alter-alter ties) # if ego had less than the max number of alters or if some # of the alter-alter ties are not present (assumed if value # is equal to 0 or NA) alter_dat_row &lt;- alter_dat_row[alter_dat_row[, 2] &lt;= netsize_updated[x] &amp; alter_dat_row[, 3] &lt;= netsize_updated[x] &amp; !is.na(alter_dat_row$weight) &amp; alter_dat_row$weight != 0, ] overall_list[[x]] &lt;- alter_dat_row } #putting all alter-alter ties, by ego, in one data frame: alter_alter_dat &lt;- do.call(rbind, overall_list) #putting useful column names on the data frame: colnames(alter_alter_dat) &lt;- c(egoID, &quot;source&quot;, &quot;target&quot;, &quot;weight&quot;) rownames(alter_alter_dat) &lt;- 1:nrow(alter_alter_dat) return(alter_alter_dat) } We now use our function to create the alter-alter tie data frame. We first need to define the aa_tie_data input. This amounts to grabbing the columns from egonets corresponding to the ties between each alter. In this case, this corresponds to columns 64 to 73. We also set max_alter at 5. The rest of the arguments (egonets, netsize and egoID) are the same as in the previous function. aa_tie_data &lt;- egonets[, 64:73] alteralter_dat_fromonefile &lt;- create_alteralter_data(egonets = egonets, netsize = egonets$netsize, aa_tie_data = aa_tie_data, egoID = &quot;CASEID&quot;, max_alter = 5) head(alteralter_dat_fromonefile) ## CASEID source target weight ## 1 19850001 1 2 2 ## 2 19850001 1 3 1 ## 3 19850001 1 4 1 ## 4 19850001 1 5 1 ## 5 19850001 2 3 2 ## 6 19850001 2 4 2 The function yields our alter-alter tie data frame, with each row corresponding to an alter-alter pair where a tie exists (nested within egos). This is the same as alteralter_dat. Now that we have our three files we can go ahead and create the egor object as before (with different column names): egonetlist2 &lt;- egor(alters = alter_dat_fromonefile, egos = ego_dat_fromonefile, aaties = alteralter_dat_fromonefile, alter_design = list(max = 5), ID.vars = list(ego = &quot;CASEID&quot;, alter = &quot;alterID&quot;, source = &quot;source&quot;, target = &quot;target&quot;)) egonetlist2 ## # EGO data (active): 1,531 × 13 ## .egoID AGE EDUC RACE SEX RELIG AGE_CATEGORICAL EDUC_CATEGORICAL NUMGIVEN HAPPY HEALTH PARTYID WTSSALL ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 19850001 33 16 white male jewish 30s College 6 2 2 1 1.04 ## 2 19850002 49 19 white male catholic 40s Post Graduate 6 2 1 4 1.04 ## 3 19850003 23 16 white female jewish 20s College 5 2 1 1 1.04 ## 4 19850004 26 20 white female jewish 20s Post Graduate 5 2 2 0 0.518 ## 5 19850005 24 17 white female catholic 20s Post Graduate 5 2 2 2 0.518 ## # ℹ 1,526 more rows ## # ALTER data: 4,483 × 7 ## .altID .egoID AGE EDUC RACE SEX RELIG ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 19850001 32 18 white male jewish ## 2 2 19850001 29 16 white female protestant ## 3 3 19850001 32 18 white male jewish ## # ℹ 4,480 more rows ## # AATIE data: 4,880 × 4 ## .egoID .srcID .tgtID weight ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 19850001 1 2 2 ## 2 19850001 1 3 1 ## 3 19850001 1 4 1 ## # ℹ 4,877 more rows This tutorial has walked through one analysis of ego network data in R. The remaining tutorials will generally focus on sociocentric network data (although see Chapter 14 for an example using ego network data in the context of epidemiological simulations), but many of the questions and problems have a direct analogy to the ego network case. With this in mind, our next tutorial deals with dyadic and triadic processes. "],["ch7-Dyads-Triads-R.html", "7 Dyads and Triads 7.1 Setting up the Session 7.2 Dyadic Processes 7.3 Triadic Processes", " 7 Dyads and Triads This tutorial walks through an analysis of dyads and triads in R. We will examine the micro processes that govern tie formation. We will begin with dyadic processes before adding a third node and considering triads. The tutorial will draw on many of the ideas introduced in the previous chapters (including basic network construction, measurement and more substantive questions about homophily). We will consider a sociocentric network based on friendship and advice ties in an organization. Our main substantive concerns are about the basic ‘rules’ that govern advice and friendship formation. For example, do we see the same behavioral rules being followed when examining advice compared to friendship? Our second main question is how these behavioral rules map onto the structure of the organization itself. In what way does the organizational structure (different departments, levels, etc.) shape the way that individual nodes form friendships and seek advice? 7.1 Setting up the Session For this session we will work primarily with the sna package (Butts 2023b), so let's start by loading that package. library(sna) We will make use of a data set collected by David Krackhardt. The data are based on three relations collected on workers in a single organization. Krackhardt asked employees at a high-tech firm that was undergoing a union certification campaign, to whom they reported, with whom they were friends, and to whom they go to for advice. These relationships provide insight into the firm's embedded social structure, which may play a strong role in shaping opinion and opinion change. We begin by constructing the networks of interest, starting with the advice network. We first read in the data on advice relations, saved as a dyad-level dataset (read in from a URL). url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/krackhardt_advice.csv&quot; advice_data_frame &lt;- read.csv(file = url1) head(advice_data_frame) ## ego alter advice_tie ## 1 1 1 0 ## 2 1 2 1 ## 3 1 3 0 ## 4 1 4 1 ## 5 1 5 0 ## 6 1 6 0 Each row in the data frame is a distinct dyad, with the first column corresponding to person 1 and the second column person 2. The third column indicates whether person 1 (or ego) seeks advice from person 2 (the alter). There is a 1 if the ij tie exists and a 0 otherwise. The first thing we will do is subset that full data frame to only keep those cases where a tie exists, thus giving us the edgelist for this network. advice_edgelist &lt;- advice_data_frame[advice_data_frame$advice_tie == 1, ] Now we will read in the data file for the attributes of the nodes: url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/krackhardt_attributes.csv&quot; attributes &lt;- read.csv(file = url2) attributes ## ids AGE TENURE LEVEL DEPT ## 1 1 33 9.333 3 4 ## 2 2 42 19.583 2 4 ## 3 3 40 12.750 3 2 ## 4 4 33 7.500 3 4 ## 5 5 32 3.333 3 2 ## 6 6 59 28.000 3 1 ## 7 7 55 30.000 1 0 ## 8 8 34 11.333 3 1 ## 9 9 62 5.417 3 2 ## 10 10 37 9.250 3 3 ## 11 11 46 27.000 3 3 ## 12 12 34 8.917 3 1 ## 13 13 48 0.250 3 2 ## 14 14 43 10.417 2 2 ## 15 15 40 8.417 3 2 ## 16 16 27 4.667 3 4 ## 17 17 30 12.417 3 1 ## 18 18 33 9.083 2 3 ## 19 19 32 4.833 3 2 ## 20 20 38 11.667 3 2 ## 21 21 36 12.500 2 1 We now have a data frame, called attributes, which holds the characteristics of our nodes. This captures the characteristics of employees at the company. The attributes include age, tenure, level of authority and department. With the edgelist and attribute data, we are now ready to construct our network using the network() function. We will treat the network as directed. As we are working with an edgelist, we can use a vertices argument for the attribute information. krack_advice &lt;- network(x = advice_edgelist, directed = T, vertices = attributes) krack_advice ## Network attributes: ## vertices = 21 ## directed = TRUE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 190 ## missing edges= 0 ## non-missing edges= 190 ## ## Vertex attribute names: ## AGE DEPT LEVEL TENURE vertex.names ## ## Edge attribute names: ## advice_tie We will now do the same thing for the friendship network. The tie information can be read in as: url3 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/krackhardt_friendship.csv&quot; friends_data_frame &lt;- read.csv(file = url3) head(friends_data_frame) ## ego alter friendship_tie ## 1 1 1 0 ## 2 1 2 1 ## 3 1 3 0 ## 4 1 4 1 ## 5 1 5 0 ## 6 1 6 0 Now we will subset the data to only include those dyads where a tie exists from i to j. friends_edgelist &lt;- friends_data_frame[friends_data_frame$friendship_tie == 1, ] We now construct the network, as before, using the network() function. krack_friendship &lt;- network(x = friends_edgelist, directed = T, vertices = attributes) Let’s do a quick plot of the advice and friendship networks to get sense of the two networks. We will use the ggnet2() function discussed in Chapter 5. library(GGally) We start with the advice network. We will size the nodes by indegree. indeg_advice &lt;- degree(krack_advice, cmode = &quot;indegree&quot;) We will also add color, coloring the nodes by level of authority in the organization, where 1 is the highest level (CEO), 2 is middle level (manager) and 3 is the lowest level. We accomplish this by adding node.color and palette arguments. ggnet2(krack_advice, node.color = &quot;LEVEL&quot;, node.size = indeg_advice, palette = c(&quot;1&quot; = &quot;navy&quot;, &quot;2&quot; = &quot;blue&quot;, &quot;3&quot; = &quot;lightskyblue&quot;), edge.size = .5, arrow.size = 3, arrow.gap = 0.02, edge.color = &quot;grey80&quot;) + guides(size = &quot;none&quot;) Now for the friendship network: indeg_friendship &lt;- degree(dat = krack_friendship, cmode = &quot;indegree&quot;) ggnet2(krack_friendship, node.color = &quot;LEVEL&quot;, node.size = indeg_friendship, palette = c(&quot;1&quot; = &quot;navy&quot;, &quot;2&quot; = &quot;blue&quot;, &quot;3&quot; = &quot;lightskyblue&quot;), edge.size = .5, arrow.size = 3, arrow.gap = 0.02, edge.color = &quot;grey80&quot;) + guides(size = &quot;none&quot;) We can already see that the advice network is much denser than the friendship network in this organization. Otherwise, it is difficult to immediately see the key differences in the two networks, meaning we will need to dig a little deeper to answer our questions about advice, friendship and organizational structure. 7.2 Dyadic Processes Now that we have our networks constructed, we can begin to formally identify the rules of interaction that operate in this organization. We begin with the simplest case, the dyad, involving each pair of nodes in the network. 7.2.1 Dyad Census and Reciprocity We start by calculating the dyad census, which characterizes each dyad as null, asymmetric or mutual. Here we can utilize the dyad.census() function in sna. We focus on the friendship network for now. dyadcensus_friendship &lt;- dyad.census(krack_friendship) dyadcensus_friendship ## Mut Asym Null ## [1,] 23 56 131 There are 23 mutual dyads (i&lt;-&gt;j), 56 asymmetric dyads (i-&gt;j) and 131 null dyads (no tie between i and j) in the friendship network. We can use the dyad census to calculate summary measures of interest, like reciprocity, showing how often i is tied to j when j is tied to i. We can calculate the rate of reciprocity as the proportion of non-null dyads where ij exists and ji exists. In the friendship network, we see that .709 of the non-null dyads are asymmetric (56 / (23 + 56)) and .291 are mutual (23 / (23 + 56)). The reciprocity rate is thus .291. Substantively, this suggests that there are some expectations for reciprocity but that many friendship nominations are, in fact, not reciprocated. We can also use the grecip() function to calculate the reciprocity rate directly. The main arguments are: dat = the network of interest measure = option indicating what kind of calculation to perform Here we set measure to \"dyadic.nonnull\" to get the calculation where only non-null dyads are considered. recip_friendship &lt;- grecip(dat = krack_friendship, measure = &quot;dyadic.nonnull&quot;) recip_friendship ## Mut ## 0.2911392 The value is the same as calculated above using the dyad census. The reciprocity rate is also useful at it facilitates comparisons across networks. Here, let’s compare the rate of reciprocity between the advice and friendship networks. recip_advice &lt;- grecip(dat = krack_advice, measure = &quot;dyadic.nonnull&quot;) recip_advice ## Mut ## 0.3103448 In this case, the reciprocity rate is very similar in the two networks and the expectations surrounding reciprocation do not appear to vary dramatically across friendship and advice. 7.2.2 Conditional Uniform Graph Tests Our analysis so far has focused on summary measures. We know that the reciprocity rate in the two networks is similar, around .30. But how should we interpret this value? Is .30 ‘large’ or ‘small’? It is difficult to tell based on the value alone. What we are implicitly missing is some baseline by which to judge the magnitude of the observed value. Thus, what we want to do is compare the values we saw in the empirical network to some baseline value; for example, the value we would expect under a random network of the same density. This will give us some sense if the observed values could have been generated by chance, just based on the size and volume of the network. Substantively, we will be able to see if the tendencies we saw in the observed network deviate from what the network would have looked like if people randomly formed ties, a useful null hypothesis. Here we can use the cug.test() function to compare the observed values to that observed under random tie formation. The arguments are: dat = network FUN = function of interest mode = setting if network is directed or not (digraph if directed) cmode = type of null hypothesis, or what type of random network to generate; options are size, edges (generate network with same density as network) and dyad.census (condition on dyad census itself) reps = number of iterations to use in simulation FUN.args = list of arguments to be passed to function Here, we will run a conditional uniform graph test on the friendship network, focusing on reciprocity. The inputs are the friendship network, the function is grecip, mode is set to \"digraph\" (as the network is directed), cmode is set to \"edges\" as we want to condition on the density of the network. We also include an argument to FUN.args, to ensure that we have the same version of reciprocity as calculated above. cug_recip &lt;- cug.test(dat = krack_friendship, FUN = grecip, mode = c(&quot;digraph&quot;), cmode = c(&quot;edges&quot;), reps = 1000, FUN.args = list(measure = &quot;dyadic.nonnull&quot;)) cug_recip ## ## Univariate Conditional Uniform Graph Test ## ## Conditioning Method: edges ## Graph Type: digraph ## Diagonal Used: FALSE ## Replications: 1000 ## ## Observed Value: 0.2911392 ## Pr(X&gt;=Obs): 0 ## Pr(X&lt;=Obs): 1 Let's look at the observed and generated null values. The observed value is stored as obs.stat in the outputted object, while the generated null values are stored as rep.stat. cug_recip$obs.stat ## Mut ## 0.2911392 summary(cug_recip$rep.stat) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.05155 0.12088 0.13333 0.14046 0.15909 0.25926 We can see that under a random network with the same density as the true network we would expect, on average, to see a reciprocity rate of 0.14. In every simulated network, the observed value (.291) is larger than that expected if people were randomly forming ties (as P(X &gt;= Obs) is equal to 0). This suggests that the actual friendship network has reciprocity rates above chance expectations. Norms of reciprocity are, at least in part, governing tie formation in this network. 7.2.3 Dyad Census by Attributes of Nodes So far we have seen that there are more mutual dyads than we expect by chance, but that asymmetries are far from uncommon in this network. One useful way of exploring these tendencies more closely is to ask if there are certain conditions where ties are particularly likely to be asymmetric (or mutual). For example, we can ask if cross-gender ties are more/less likely to be reciprocated. Here, let’s explore the dyad census in the context of organizational structure. In this organization there are 3 basic levels of authority, a boss (1), middle manager (2) and worker (3). table(attributes$LEVEL) ## ## 1 2 3 ## 1 4 16 We see that there are very few people at the top, a handful of people acting as middle managers and most at the very bottom, the actual workers. The question is how does the social structure, based on friendship or advice, map onto the organizational structure, where some people are in positions of authority and most are not. Here we will calculate the dyad census again for friendship, but do it separately for dyads that are at the same level and dyads that are not at the same level. We want to know if the rate of mutual dyads is higher/lower if the two people are at the same level of authority. First, let’s determine if each ij pair is in the same level of the organization. Here we will utilize an outer command, asking if i and j are at the same level, doing this over all ij pairs. same_level &lt;- outer(attributes$LEVEL, attributes$LEVEL, &quot;==&quot;) Now, we need to construct a matrix that shows if each i-j pair is null, asymmetric or mutual. This will make it possible to calculate the dyad census conditioned on the attributes of the nodes in each dyad (i.e., are they at the same level of the organization or not). Let’s write a little function that will take the input network and output the matrix of dyad types. dyad_mat_function &lt;- function(dat){ # Arguments: # dat: network object # getting matrix form of network mat &lt;- as.matrix(dat) # putting NA diagonal as we don’t want to consider self-ties diag(mat) &lt;- NA # Next, we do a little trick where we take the matrix and # add it to its transpose, yielding a matrix of 0s, 1s # and 2s. If it is null, the resulting value will be 0 # (neither ij nor ji exists); if it is asymmetric there # will be a 1 (as ij or ji exists but not both); and # if it is mutual there will be a 2 (as there are ties # from ij and ji). dyad_mat &lt;- mat + t(mat) # Now we label for ease of interpretation: dyad_mat[dyad_mat == 0] &lt;- &quot;null&quot; dyad_mat[dyad_mat == 1] &lt;- &quot;asym&quot; dyad_mat[dyad_mat == 2] &lt;- &quot;mut&quot; return(dyad_mat) } Let’s use our function to output the matrix of dyad types for the friendship network. friends_dyads &lt;- dyad_mat_function(krack_friendship) Let's look at the first five rows and columns. friends_dyads[1:5, 1:5] ## 1 2 3 4 5 ## 1 NA &quot;mut&quot; &quot;null&quot; &quot;mut&quot; &quot;null&quot; ## 2 &quot;mut&quot; NA &quot;null&quot; &quot;asym&quot; &quot;asym&quot; ## 3 &quot;null&quot; &quot;null&quot; NA &quot;null&quot; &quot;null&quot; ## 4 &quot;mut&quot; &quot;asym&quot; &quot;null&quot; NA &quot;null&quot; ## 5 &quot;null&quot; &quot;asym&quot; &quot;null&quot; &quot;null&quot; NA We see that 1-2 is a mutual dyad, 1-3 is null and so on. Note that if we do a table over this and divide by 2 (as we don’t want to count each dyad twice) we get the dyad census, as above: table(friends_dyads) / 2 ## friends_dyads ## asym mut null ## 56 23 131 Now, we calculate the dyad census but only consider dyads where same_level is equal to TRUE, so that i and j are both at the same level of authority. table(friends_dyads[same_level == TRUE]) / 2 ## ## asym mut null ## 38 19 69 Now, we calculate the dyad census but only consider dyads where same_level is equal to FALSE, so that i and j are not at the same level of authority. table(friends_dyads[same_level == FALSE]) / 2 ## ## asym mut null ## 18 4 62 The results show a clear difference between dyads that cross levels of authority and those that do not. The rate of reciprocity is .333 when the dyads are at the same level (19 / (19 + 38)) but only .182 when they are at different levels (4 / (4 + 18)). This means that there is a much higher proportion of mutual dyads when both are at the same level than when one person is in authority position and the other is not. Friendships do tend to map onto the authority structure in the organization, where mutual friendship across levels, while possible, are unlikely. And more generally, it would appear that the clarity around friendship expectations is clearer when the two people are in the same level of the organization. 7.2.4 Advice and Friendship Together As a second way of examining our results more closely, we can begin to ask questions about the joint nature of advice and friendship. Rather than simply looking at reciprocity across the two networks, we can see if advice maps onto friendship in particular ways. For example, if i nominates j as a friend and j does not reciprocate, does that make it more likely that ij will also have an asymmetric advice relation? More generally, what are the joint rules governing friendship and advice in this organization? To answer this question we begin by outputting the dyad type matrix for the advice network, using our function written above: advice_dyads &lt;- dyad_mat_function(krack_advice) advice_dyads[1:5, 1:5] ## 1 2 3 4 5 ## 1 NA &quot;asym&quot; &quot;asym&quot; &quot;mut&quot; &quot;asym&quot; ## 2 &quot;asym&quot; NA &quot;asym&quot; &quot;asym&quot; &quot;asym&quot; ## 3 &quot;asym&quot; &quot;asym&quot; NA &quot;asym&quot; &quot;null&quot; ## 4 &quot;mut&quot; &quot;asym&quot; &quot;asym&quot; NA &quot;null&quot; ## 5 &quot;asym&quot; &quot;asym&quot; &quot;null&quot; &quot;null&quot; NA We can see that 1-2 is asymmetric, 1-3 is asymmetric, 1-4 is mutual, and so on. Now, let’s take the dyad matrices for friendship and advice and do a simple table. table(friends_dyads, advice_dyads) / 2 ## advice_dyads ## friends_dyads asym mut null ## asym 37 10 9 ## mut 9 9 5 ## null 54 26 51 Looking at the first row, we can see that 37 dyads are asymmetric on both relations, friendship and advice, 10 are asymmetric on friendship but mutual on advice, and 9 are asymmetric on friendship and null on advice. This suggests that dyads that tend to be asymmetric on friendship also tend to be asymmetric on advice (.661 of asymmetric friendship dyads are also asymmetric on advice). Compare this to the second row, the mutual friendship row. Mutual friendship dyads are evenly split between asymmetric and mutual advice dyads, and there is even a fairly high percentage in the null advice dyad (.391 in asymmetric, .391 in mutual and .217 in null). This means that a mutual friendship can be coupled with advice in almost anyway, while asymmetries in friendship are almost always coupled with asymmetries in advice. This may be the case as the advisee imputes a friendship onto the advisor that the advisor does not reciprocate. The very nature of advisee-advisor relationships invites the possibility of asymmetries in less formal, more friendship-based ties. More generally, we can see that advice and friendship combine in complex ways, even when we focus on just two nodes at a time. 7.3 Triadic Processes We now move to more complex micro processes involving three nodes. Adding a third node adds a number of possibilities that did not exist with only two nodes. For example, there is now the possibility that one node can play the other two off each other for some gain or benefit. There is also the possibility that the relationship between two nodes (A-B) is strained because of the relationship to the third node (A likes C but B does not). The question is what kind of rules of interaction do we see in our friendship and advice networks. We also want to know if the triadic rules for friendship are different (or similar) to the triadic rules for advice. We first discuss the triad census before moving to transitivity. 7.3.1 Triad Census The triad census captures the distribution of triads across 16 basic types, representing different patterns of interaction between the three nodes. There are 16 different triad types: 003 A, B, C, empty triad 012 A-&gt;B, C 102 A&lt;-&gt;B, C 021D A&lt;-B-&gt;C 021U A-&gt;B&lt;-C 021C A-&gt;B-&gt;C 111D A&lt;-&gt;B&lt;-C 111U A&lt;-&gt;B-&gt;C 030T A-&gt;B&lt;-C, A-&gt;C 030C A&lt;-B&lt;-C, A-&gt;C 201 A&lt;-&gt;B&lt;-&gt;C 120D A&lt;-B-&gt;C, A&lt;-&gt;C 120U A-&gt;B&lt;-C, A&lt;-&gt;C 120C A-&gt;B-&gt;C, A&lt;-&gt;C 210 A-&gt;B&lt;-&gt;C, A&lt;-&gt;C 300 A&lt;-&gt;B&lt;-&gt;C, A&lt;-&gt;C, completely connected We can summarize the full distribution of triads using the triad.census() function. We want to compare the triad counts across the two network relations, to see if the patterns differ for friendship compared to advice. The triad.census() function outputs the counts in each triad type. First for friendship: triads_friendship &lt;- triad.census(krack_friendship) triads_friendship ## 003 012 102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U 120C 210 300 ## [1,] 376 366 143 114 34 35 39 101 23 0 20 16 25 9 23 6 And now for advice: triads_advice &lt;- triad.census(krack_advice) triads_advice ## 003 012 102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U 120C 210 300 ## [1,] 74 153 90 160 86 49 59 101 190 2 72 62 78 17 107 30 Let's make a table to make the comparisons easier. triad_data &lt;- data.frame(advice = t(triads_advice), friendship = t(triads_friendship)) triad_data ## advice friendship ## 003 74 376 ## 012 153 366 ## 102 90 143 ## 021D 160 114 ## 021U 86 34 ## 021C 49 35 ## 111D 59 39 ## 111U 101 101 ## 030T 190 23 ## 030C 2 0 ## 201 72 20 ## 120D 62 16 ## 120U 78 25 ## 120C 17 9 ## 210 107 23 ## 300 30 6 Note that the total number of triads in each network is the same, making comparisons easier. There are a number of things one could highlight between the two networks, but one striking difference is in the 030T triad, A-&gt;B&lt;-C, A-&gt;C. The advice network has a much higher proportion of 030T triads than the friendship network, suggestive of an underlying hierarchy of advice less present in the friendship network; where A gives advice to B and C and C gives advice to B and B simply receives advice from others. The underlying hierarchy in this advice relation thus runs: A to C to B, as A influences B and C, C influences only B and B influences no one. Such a pattern of hierarchy does not emerge as clearly in the friendship network. In general, the triads associated with transitive dominance relations (021D, 120D, 021U, 030T, 120U) are more likely to be present in the advice network than the friendship network. Note that we need to be a little careful in pushing that conclusion too far, as the networks have different levels of density (and possibly reciprocity) that may be driving the differences we observe in the triad count. We need to take these baseline differences into account before formally interpreting the triad counts. Here, we will utilize a CUG test to compare the triad census to what we might expect by chance, if people randomly formed ties. We discuss other, more complex, options in future chapters. We use the same cug.test() function as we used above when looking at reciprocity. The main difference is that here we condition the random networks in a different way. In this case we will condition on the dyad census, to see how the counts of the triad census differ above what we expect from a network of the same size, density and dyadic tendencies (i.e., same rate of reciprocity). Our particular question is if different specific triad types show up more (or less) than we expect in a random network, conditioned on those features. To facilitate this, we will first write a function to make it easier to specify the test statistic of interest, here the counts of specific triad types. Our function will take the network of interest, calculate the triad census and output the count of the particular triad of interest. We could also write a similar function to take the weighted sum over a specified set of triads. Here we focus on one triad type at a time. count_triads_function &lt;- function(dat, triad_names){ # Arguments: # dat: the network object # triad_names: the name of the triad type triads &lt;- triad.census(dat) # calculating the triad census triads[1, triad_names] # grabbing counts of triad of interest } Let’s see how this works before we use it in a CUG test. Let’s focus again on 030T, where A gives advice to B and C and C gives advice to B. count_triads_function(krack_advice, triad_names = c(&quot;030T&quot;)) ## 030T ## 190 Now, let’s use our function to count the triad type of interest but this time compare the counts to what we expect by chance, in a random network conditioned on the dyad census. We now use the cug.test() function. The main inputs are the network (here for advice), the desired conditioning of the random network (cmode set to \"dyad.census\") and the function of interest, in this case count_triads_function. The input to that function is triad_names, here set to \"030T\". cug_triads_advice &lt;- cug.test(krack_advice, FUN = count_triads_function, mode = c(&quot;digraph&quot;), cmode = c(&quot;dyad.census&quot;), reps = 1000, FUN.args = list(triad_names = c(&quot;030T&quot;))) cug_triads_advice ## ## Univariate Conditional Uniform Graph Test ## ## Conditioning Method: dyad.census ## Graph Type: digraph ## Diagonal Used: FALSE ## Replications: 1000 ## ## Observed Value: 190 ## Pr(X&gt;=Obs): 0 ## Pr(X&lt;=Obs): 1 We can see that the total number of 030T triads in the observed network is above what we expect in the random network. In all 1000 random networks, the observed count is above that seen in the randomly generated network (P(X &gt;= Obs) is equal to 0). This means that the tendency for A to give advice to B and C and then C to give advice to B cannot be accounted for based simply on the density of advice relations and expectations of reciprocity. We now do the same analysis for the friendship network. cug_triads_friendship &lt;- cug.test(krack_friendship, FUN = count_triads_function, mode = c(&quot;digraph&quot;), cmode = c(&quot;dyad.census&quot;), reps = 1000, FUN.args = list(triad_names = c(&quot;030T&quot;))) cug_triads_friendship ## ## Univariate Conditional Uniform Graph Test ## ## Conditioning Method: dyad.census ## Graph Type: digraph ## Diagonal Used: FALSE ## Replications: 1000 ## ## Observed Value: 23 ## Pr(X&gt;=Obs): 0.127 ## Pr(X&lt;=Obs): 0.917 The story is different for friendship. Here we see that in many iterations the observed value (the count of 030T) is not above that expected in a random network of that size, density and dyad census. Using traditional statistical cutoffs, we cannot be certain that there is a difference between the observed and random expectations. This suggests that the tendency for A to be friends with B and C and C to be friends with B is not particularly strong in the friendship network, at least not above what could have arisen if people randomly formed friendships (conditioned on the density of the network and the reciprocity rate). Let’s take a closer look at the observed values and the values expected under the randomly generated networks. cug_triads_friendship$obs.stat ## 030T ## 23 summary(cug_triads_friendship$rep.stat) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 7.00 15.00 18.00 18.04 20.00 31.00 On average, the expected count in the random network is 18.04, close to that observed in the true network (23). What if we had considered a different triad type in our friendship network? For example, let’s consider 120U (A-&gt;B&lt;-C, A&lt;-&gt;C). In this case A and C have a reciprocated friendship. They also both nominate B who does not nominate them back. There are asymmetries in such a triad, but from a balance perspective this may not be problematic. A and C are both positively linked and they have positive sentiments towards B. There is thus agreement about a third party (B) from the point of view of A and C, even if B does not consider them friends in return. Let’s do a CUG test on the count of 120U on the friendship network: cug_triads_friendship2 &lt;- cug.test(krack_friendship, FUN = count_triads_function, mode = c(&quot;digraph&quot;), cmode = c(&quot;dyad.census&quot;), reps = 1000, FUN.args = list(triad_names = c(&quot;120U&quot;))) cug_triads_friendship2 ## ## Univariate Conditional Uniform Graph Test ## ## Conditioning Method: dyad.census ## Graph Type: digraph ## Diagonal Used: FALSE ## Replications: 1000 ## ## Observed Value: 25 ## Pr(X&gt;=Obs): 0 ## Pr(X&lt;=Obs): 1 In this case, we can see that the observed count (25) is above that expected by chance, and this is the case in all 1000 randomly generated networks. This suggests that a triad based on balanced relations, even if not completely mutual, is observed at comparatively high rates in the friendship network. Overall, it would appear that the advice network is built around dominance type relations in a way that the friendship network is not. Individuals give advice in a way that follows patterns of status; where, for example, A gives advice ‘down the line’ to B and C. In contrast, the friendship network, at least in our short analysis, is consistent with balancing relations, where if two nodes are strong friends, they will tend to agree (in terms of liking or not liking) third party nodes, even if that third party does not return the friendship. This was not captured very well in the dyadic analysis, where the reciprocity rates were pretty similar in the two networks. To explore these ideas more carefully, a researcher may want to look at all triad types for each network. As part of this book, we have written an additional function that will automatically do a CUG test for all triad types at once (rather than a single example, as above) and then output a summary table of the results. Let's read in the function: source(&quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/R/CUGtest_alltriads.R&quot;) The function is called CUGtest_alltriads(). The arguments are dat, the network of interest (assuming it is in the network format), mode, cmode and reps, all directly analogous to cug.test. And now we will use the function on our friendship network. CUG_test_table &lt;- CUGtest_alltriads(dat = krack_friendship, mode = &quot;digraph&quot;, cmode = &quot;dyad.census&quot;, reps = 1000) CUG_test_table ## triads obs mean_random prop_random_lte_obs ## 1 003 376 319.674 1.000 ## 2 012 366 417.532 0.000 ## 3 102 143 171.489 0.002 ## 4 021D 114 44.029 1.000 ## 5 021U 34 44.260 0.052 ## 6 021C 35 87.575 0.000 ## 7 111D 39 73.748 0.000 ## 8 111U 101 73.418 0.999 ## 9 030T 23 18.299 0.907 ## 10 030C 0 6.209 0.000 ## 11 201 20 28.906 0.026 ## 12 120D 16 7.740 0.998 ## 13 120U 25 7.754 1.000 ## 14 120C 9 15.272 0.063 ## 15 210 23 12.518 0.998 ## 16 300 6 1.577 0.999 The outputted table has 4 columns. The first column is the triad type. The second column is the observed count in the network of interest. The third column is the mean count seen in the random networks (under the desired conditioning). The fourth column is the proportion of times where the value from the random network is less than or equal to the value in the observed network. We can use these results to explore which triads are found in higher or lower levels than what we expect by chance. 7.3.2 Transitivity In analogous fashion to the reciprocity rate, we can use transitivity as a summary measure of the triad census. Transitivity is defined as the proportion of (potentially intransitive) triads that are transitive, defined as all triads with i, j, and k, such that whenever i -&gt; j and j -&gt; k then i -&gt; k. Or, more colloquially, a friend of a friend is a friend. The function is gtrans(). The main arguments are: dat = network of interest measure = type of calculation In this case, the measure option we want is \"weak\" (this calculates the proportion of transitive triads divided by the number of potentially intransitive triads, defined as triads where i -&gt; j and j -&gt; k; the strong version divides by the total number of triads). gtrans(dat = krack_advice, measure = &quot;weak&quot;) ## [1] 0.6639785 As we saw with the triad census, transitive closure would appear to be strong in the advice network. If i advises j and j advises k, then i will also tend to advise k, maintaining the hierarchy of relations between the nodes. Overall, a picture of the dyadic and triadic processes in this network begins to emerge. Friendships are often reciprocated, but asymmetries are common, especially when the pair cross levels of authority in the organization. And while the friendship and advice networks have similar levels of reciprocity, there are clear differences in the triadic processes. Advise appears to be driven more heavily by hierarchical relations, while friendship tends to be driven by agreement, or balance, between close friends (in relation to a third node). More generally, the analysis shows the importance of contextualizing our basic measures, showing how they map onto the particular context (here organizational structure) of interest. The analysis also suggests the importance of considering multiple relations at once. We will return to many of these ideas in future tutorials, most directly in Chapter 9 (where we cover centrality and hierarchy) and Chapter 13 (where we cover statistical network models). "],["ch8-Network-Cohesion-Communities-R.html", "8 Network Cohesion and Communities 8.1 First Example Network 8.2 Cohesion 8.3 Community (or Group) Detection 8.4 Demographic Characteristics 8.5 Community Overlap 8.6 Big Data Example", " 8 Network Cohesion and Communities In this tutorial we will offer an extended example in R on cohesion and communities (or groups). We will first examine cohesion, capturing how robust the network is to being fragmented. We will then cover community structure, capturing which sets of actors form a group (in the sense that they have high rates of interaction, are close in the network, etc.). The tutorial will build on material from previous chapters, most directly from the tutorial on basic network measurement. We will utilize two example networks in our analysis. The first example comes from adolescent data collected by Daniel McFarland on the different kinds of interactions occurring in one classroom. This network is small, with only 16 actors. The second example will take up the problem of analyzing a much larger network, here an email network of over 200,000 nodes. This second example is included as a means of demonstrating some of the added complications of dealing with 'big' data, as well as to offer some suggestion on best practices when analyzing such networks. The classroom example is motivated by two main substantive questions. First, is the classroom network cohesive, or the does the network strongly split into local communities (or groups), with few ties between communities? This a crucial question if, for example, we are interested in the maintenance of norms in the classroom. Do we find two divided communities that are unlikely to agree on things? Or is there the possibility of general consensus, given the connections between communities? Our second question focuses on the composition of the communities themselves, focusing specifically on racial identity. Do we find distinct communities that correspond to racial identity (a ‘white’ group and a ‘black’ group) or are things more complicated? Putting these questions together, we want to know something about the potential for consensus and solidarity in the classroom and we want to know if the threats to consensus are tied to racial divides. 8.1 First Example Network For this session we will work primarily with the igraph package. We will also load ggplot2. library(igraph) library(ggplot2) Let's first read in the classroom network data (from a URL) and take a look at the first six rows: url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class182_networkdata.csv&quot; class182_networkdata &lt;- read.csv(file = url1) head(class182_networkdata) ## ego alter friend_tie social_tie task_tie ## 1 1 1 0 0.0 0.0 ## 2 1 2 0 0.0 0.0 ## 3 1 3 0 0.0 0.0 ## 4 1 4 0 0.0 0.0 ## 5 1 5 0 1.2 0.3 ## 6 1 6 0 0.0 0.0 The data frame holds information for each dyad in the classroom showing information for three relations: friendship, social interactions and task interactions. The first column is the ego, the second column is the alter, and the third, fourth and fifth columns are values, greater than or equal to zero, showing the strength of the association for the two nodes. Friendship is measured as: 2 = best friend, 1 = friend, 0 = not friend. Social interaction is measured as social interactions per hour. Task interaction is measured as task interactions per hour. Here we will utilize the friendship data. Let's also read in the attribute file. url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class182_attributedata.csv&quot; class182_attributes &lt;- read.csv(file = url2) class182_attributes ## ids race grade gender ## 1 1 white 10 male ## 2 2 black 10 female ## 3 3 white 10 female ## 4 4 white 11 female ## 5 5 white 10 male ## 6 6 white 10 female ## 7 7 black 10 female ## 8 8 black 10 female ## 9 9 white 10 female ## 10 10 black 11 male ## 11 11 white 10 male ## 12 12 black 11 female ## 13 13 white 10 female ## 14 14 black 10 female ## 15 15 black 10 female ## 16 16 black 13 male We have information on race, grade and gender for each student in the classroom. Let's create an edgelist based on the friendship relation. Here we will define an edge as any case where the friendship value is greater than 0. edge_value &lt;- class182_networkdata$friend_tie edgelist_friendship &lt;- class182_networkdata[edge_value &gt; 0, c(&quot;ego&quot;, &quot;alter&quot;, &quot;friend_tie&quot;)] head(edgelist_friendship) ## ego alter friend_tie ## 17 2 1 1 ## 23 2 7 1 ## 24 2 8 1 ## 29 2 13 2 ## 30 2 14 1 ## 37 3 5 1 And now we can go ahead and create our igraph object using the edgelist and attribute data frames. net182_friend &lt;- graph_from_data_frame(d = edgelist_friendship, directed = T, vertices = class182_attributes) net182_friend ## IGRAPH 45bbf9a DN-- 16 62 -- ## + attr: name (v/c), race (v/c), grade (v/n), gender (v/c), friend_tie (e/n) ## + edges from 45bbf9a (vertex names): ## [1] 2 -&gt;1 2 -&gt;7 2 -&gt;8 2 -&gt;13 2 -&gt;14 3 -&gt;5 3 -&gt;6 3 -&gt;11 3 -&gt;14 3 -&gt;15 5 -&gt;1 5 -&gt;3 5 -&gt;6 5 -&gt;8 5 -&gt;10 5 -&gt;11 6 -&gt;1 6 -&gt;3 6 -&gt;5 6 -&gt;7 6 -&gt;10 6 -&gt;11 6 -&gt;12 7 -&gt;2 7 -&gt;8 7 -&gt;13 7 -&gt;14 8 -&gt;2 8 -&gt;5 8 -&gt;7 8 -&gt;13 8 -&gt;14 8 -&gt;15 9 -&gt;1 9 -&gt;3 9 -&gt;10 9 -&gt;12 9 -&gt;15 10-&gt;1 10-&gt;9 10-&gt;12 10-&gt;15 11-&gt;1 11-&gt;3 11-&gt;5 11-&gt;6 11-&gt;10 12-&gt;1 12-&gt;9 12-&gt;15 13-&gt;2 13-&gt;7 13-&gt;8 13-&gt;14 14-&gt;2 14-&gt;3 14-&gt;8 14-&gt;12 15-&gt;1 15-&gt;7 15-&gt;9 15-&gt;12 Note that the network is directed and has an edge attribute, friend_tie, showing the strength of the relationship. 8.2 Cohesion We start by looking at the overall cohesion of the network. We will begin by doing a quick plot of the friendship network. This will give us a useful, intuitive picture of the network structure. plot(net182_friend, vertex.label = NA, vertex.size = 10, edge.arrow.size = .25, edge.arrow.width = 1, edge.color = &quot;light gray&quot;, vertex.frame.color = NA) We can see that the classroom network is fairly cohesive (ignoring the two isolates), although it does appear to roughly split into two groups. Let’s more formally explore some of the properties of the network, starting with density. edge_density(net182_friend) ## [1] 0.2583333 We can see here that .258 of all possible ties exist in the network. We might take that as an indication of high levels of cohesion, as many observed networks have much lower values for density. It is important to remember, however, that density only captures something about the volume of ties, not the pattern of ties (it is also worth remembering that density is typically higher in smaller networks, and we are working with a small network). A network with many ties may still be fragile, in the sense that the ties are arranged in a way that some parts of the network are disconnected (or only very loosely connected) to other parts. It is thus useful to consider other measures that more directly capture cohesion. For example, let's examine component size. The main component is the largest set of nodes such that all ij pairs can reach one another. This in itself is a relatively low bar for cohesion, as it only says that i can reach j, not how interconnected i and j actually are (e.g., are i and j connected through multiple short paths or one long path?). The function here is called components(). The main arguments are graph (network of interest, as an igraph object) and mode. The mode = \"strong\" option defines a component as a set where i can reach j and j can reach i. The mode = \"weak\" option would give us the other version, where i and j are in the same component as long as i can reach j or j can reach i. Here we use the weak option. Note that the mode argument is ignored in the case of undirected networks. components_friendship &lt;- components(graph = net182_friend, mode = &quot;weak&quot;) components_friendship ## $membership ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 3 ## ## $csize ## [1] 14 1 1 ## ## $no ## [1] 3 The output is a list with three elements: membership (the component each node belongs to); csize (the size of each component); and no (the number of components). Let’s calculate the proportion of nodes in each component. components_friendship$csize / sum(components_friendship$csize) ## [1] 0.8750 0.0625 0.0625 We can see that 87.5% of the nodes fall in the largest component. Our two isolates are each in their own component, while all other nodes (with ties) are in one large component. This substantively suggests that most students, while perhaps also falling into more local peer groups, do appear to be part of a larger social grouping (which is at least minimally connected), defined at the level of the classroom itself. Past work has also often used bicomponent size to measure social cohesion (Moody and White 2003). Bicomponent size is an extension of component size, showing sets of nodes that are connected by at least two independent paths, so that removing a single nodes leaves the entire set connected. Bicomponent size is an ideal measure of cohesion as it captures if the network can withstand the loss (or removal) of key nodes. A cohesive network is thus robust to disconnection, existing as a larger unit beyond the presence of particular nodes. The function to calculate bicomponent size is biconnected_components(). Note that the function will automatically treat the network as undirected, equivalent to the ‘weak’ option above for component size. bicomponents_friendship &lt;- biconnected_components(graph = net182_friend) The output is a list. The main item of interest is the nodes that are in each bicomponent. This can be found in the components part of the list. bicomponents_friendship$components ## [[1]] ## + 14/16 vertices, named, from 45bbf9a: ## [1] 10 15 11 9 12 14 13 8 5 3 6 7 2 1 In this case 14 out of 16 nodes are in a single bicomponent, connected by at least two independent paths. Again, the only nodes who are not in this main bicomponent are our two isolates. This would suggest that the network has high cohesion, as all nodes (save for two isolates) are connected to each other in multiple ways. Building on the ideas of bicomponent size and component size, we can also explore the connectivity of specific pairs of nodes in the network, or vertex connectivity. Here we utilize the vertex_disjoint_paths() function. The arguments are graph, source (starting node) and target (end node). The output is the number of nodes that would need to be removed so that i can no longer reach j (excluding any direct tie from i to j). To be consistent with the component and bicomponent calculations above, let’s create an undirected network to look at vertex connectivity. We set mode to the \"collapse\" option, which uses a 'weak' rule, so if i is tied to j or j is tied to i, there is a tie between i and j. net182_friend_und &lt;- as.undirected(net182_friend, mode = &quot;collapse&quot;) net182_friend_und ## IGRAPH 0b7b0f7 UN-- 16 42 -- ## + attr: name (v/c), race (v/c), grade (v/n), gender (v/c) ## + edges from 0b7b0f7 (vertex names): ## [1] 1 --2 1 --5 3 --5 1 --6 3 --6 5 --6 2 --7 6 --7 2 --8 5 --8 7 --8 1 --9 3 --9 1 --10 5 --10 6 --10 9 --10 1 --11 3 --11 5 --11 6 --11 10--11 1 --12 6 --12 9 --12 10--12 2 --13 7 --13 8 --13 2 --14 3 --14 7 --14 8 --14 12--14 13--14 1 --15 3 --15 7 --15 8 --15 9 --15 10--15 12--15 Here we will calculate vertex connectivity for two example nodes, 1 and 9: vertex_disjoint_paths(graph = net182_friend_und, source = 1, target = 9) ## [1] 5 We can see that to disconnect node 1 and node 9 in the friendship network, one would need to remove 5 other nodes, a very high number. Node 1 and node 9 are thus highly interconnected, and likely part of the same social group. We can extend this kind of calculation to all dyads, summarizing over the distribution of vertex connectivity to create a summary measure of cohesion that complements, but extends, bicomponent size. Bicomponent size only captures being connected by a minimal threshold of two independent paths; it does not tell us the actual number of paths that connect our nodes. Note, however, that calculating the vertex connectivity over all dyads can be expensive (time-wise) and it may be useful/necessary to sample dyads to use in the calculation. We can also use the vertex_connectivity() function to get the connectivity for the entire network, the minimum number of nodes that would need to be removed to disconnect the network (so that all i could not reach all j). vertex_connectivity(graph = net182_friend_und) ## [1] 0 We see here that vertex connectivity is 0. This is the case because of the two isolates in the network. One would not need to remove any nodes to disconnect the network. Let’s recalculate vertex connectivity, this time removing the two isolates from the network. We first identify the isolates. We then create a new network without the isolates and recalculate vertex connectivity. isolates &lt;- which(degree(net182_friend_und) == 0) net182_noisolates &lt;- delete_vertices(net182_friend_und, isolates) vertex_connectivity(graph = net182_noisolates) ## [1] 4 Ignoring the isolates, one would have to remove 4 nodes in order to disconnect the network. Overall, we see that this classroom setting has a high level of cohesion, outside of the two isolates. All non-isolates are part of one large bicomponent, connected by at least two independent paths. Going a little further, all non-isolates are actually connected by a minimum of 4 independent paths, making it very robust to disconnection. 8.3 Community (or Group) Detection So far we have established that the friendship network in this classroom is cohesive. We now want to zoom in, asking about the communities (or groups) within the larger cohesive set. The goal is to identify sets of nodes that have high internal density and few ties to outside members. Thus, even in a cohesive network, there may be important communities (or dense regions) within the larger network. We have three main questions. The first question is about the communities that are present in the network. Does the network divide into a number of small communities? Or does the network basically split in half, with one strong dividing line? The second question is about the composition of these communities. Does race map strongly onto the found communities? The third question is about the level of contact between communities. How do the communities (and the ties between) contribute or detract from the overall cohesion of the network? We utilize community detection to address these questions. There are many different ways to detect communities. In this tutorial, we will use four: walktrap, edge-betweenness, hierarchical clustering, and cohesive blocking. As we walk through them, it is important to consider how they portray communities and consider which one(s) afford a sensible view of the social world as cohesively organized. Note that igraph uses the language of communities, rather than groups, but they are interchangeable. For consistency and simplicity, we'll use the undirected network in our analysis. 8.3.1 Walktrap This algorithm detects communities through a series of short random walks, with the idea that the nodes encountered on any given random walk are more likely to be within a community than not. The algorithm initially treats all nodes as communities of their own, then merges them into larger communities, and these into still larger communities, and so on. The walktrap algorithm calls upon the user to specify the length of the random walks. Past work suggests using walks of length 4 or 5 but there is no guarantee that this will yield the 'best' partition; for example, the one that maximizes modularity (Pons and Latapy 2005). Modularity (discussed in greater detail in the main text, Chapter 8) measures the quality of the partition, comparing the number of edges going within communities (based on the given partition) compared to that expected under a null model. The function is cluster_walktrap(). The main arguments are graph (network of interest, as an igraph object), steps (number of steps in random walk) and membership (T/F, should membership be calculated based on highest modularity score? T by default). Let's begin by going 4 steps out. friend_comm_wt4 &lt;- cluster_walktrap(graph = net182_friend_und, steps = 4, membership = T) friend_comm_wt4 ## IGRAPH clustering walktrap, groups: 4, mod: 0.27 ## + groups: ## $`1` ## [1] &quot;2&quot; &quot;7&quot; &quot;8&quot; &quot;13&quot; &quot;14&quot; ## ## $`2` ## [1] &quot;1&quot; &quot;3&quot; &quot;5&quot; &quot;6&quot; &quot;9&quot; &quot;10&quot; &quot;11&quot; &quot;12&quot; &quot;15&quot; ## ## $`3` ## [1] &quot;4&quot; ## ## $`4` ## + ... omitted several groups/vertices Now let's get the membership of each node as well the modularity score based on 4 our step solution. mems_wt_4step &lt;- membership(friend_comm_wt4) mems_wt_4step ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## 2 1 2 3 2 2 1 1 2 2 2 2 1 1 2 4 mod_wt_4step &lt;- modularity(friend_comm_wt4) mod_wt_4step ## [1] 0.2695578 What would happen if we used 3 steps? friend_comm_wt3 &lt;- cluster_walktrap(graph = net182_friend_und, steps = 3, membership = T) mems_wt_3step &lt;- membership(friend_comm_wt3) mems_wt_3step ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## 2 1 3 4 3 3 1 1 2 2 3 2 1 1 2 5 mod_wt_3step &lt;- modularity(friend_comm_wt3) mod_wt_3step ## [1] 0.2797619 We can see that the modularity is slightly higher when we use 3 steps. We can compare the two partitions using the table function: table(mems_wt_4step, mems_wt_3step) ## mems_wt_3step ## mems_wt_4step 1 2 3 4 5 ## 1 5 0 0 0 0 ## 2 0 5 4 0 0 ## 3 0 0 0 1 0 ## 4 0 0 0 0 1 We can see that the big difference is that using 3 steps seems to split one of the communities (based on 4 steps) into 2. And let's also plot the network based on the found communities, comparing the two partitions. We will plot the network as before, but color the nodes based on the communities found using the walktrap algorithm. We first define the layout to use in both plots. This ensures that the network is placed and rotated in the same way, making it easier to compare partitions. par(mfrow = c(1, 2)) layout &lt;- layout.fruchterman.reingold(net182_friend) plot(net182_friend_und, layout = layout, #note the use of layout vertex.color = mems_wt_4step, edge.color = &quot;light gray&quot;, vertex.size = 20, main = &quot;Walktrap: 4 Steps&quot;) plot(net182_friend_und, layout = layout, #note the use of layout vertex.color = mems_wt_3step, edge.color = &quot;light gray&quot;, vertex.size = 20, main = &quot;Walktrap: 3 Steps&quot;) The results suggest, so far, that there are 2 basic communities (2, 7, 8, 13, 14 and 1, 3, 5, 6, 9, 10, 11, 12, 15) (ignoring the isolates) with one of those communities having their own internal division. We see with the 3 step solution that the larger community is split in two: (3, 5, 6, 11) and (1, 9, 10, 12, 15). Note that is it possible to include a weights argument on the edges so that larger values correspond to stronger ties (the default is to use the edges weights on the igraph object). 8.3.2 Edge Betweenness The idea of the edge-betweenness algorithm is that it is likely that edges connecting separate communities have high edge-betweenness, as all the shortest paths from one community to another must traverse through them. So, if we iteratively remove the edge with the highest edge-betweenness score we will get a hierarchical map of the communities in the graph. See Newman and Girvan (2004) for more details. The function is cluster_edge_betweenness(). friend_comm_eb &lt;- cluster_edge_betweenness(graph = net182_friend_und) friend_comm_eb ## IGRAPH clustering edge betweenness, groups: 5, mod: 0.28 ## + groups: ## $`1` ## [1] &quot;1&quot; &quot;9&quot; &quot;10&quot; &quot;12&quot; &quot;15&quot; ## ## $`2` ## [1] &quot;2&quot; &quot;7&quot; &quot;8&quot; &quot;13&quot; &quot;14&quot; ## ## $`3` ## [1] &quot;3&quot; &quot;5&quot; &quot;6&quot; &quot;11&quot; ## ## $`4` ## + ... omitted several groups/vertices The default is to select the communities that will maximize modularity. Let's grab the membership and compare the partition to the 3 step walktrap solution above. mems_eb &lt;- membership(friend_comm_eb) table(mems_wt_3step, mems_eb) ## mems_eb ## mems_wt_3step 1 2 3 4 5 ## 1 0 5 0 0 0 ## 2 5 0 0 0 0 ## 3 0 0 4 0 0 ## 4 0 0 0 1 0 ## 5 0 0 0 0 1 In this case the found communities are the same across the algorithms (even though the labeling of the communities may be different). Note that while the default is to select the membership based on the highest modularity value, it is also possible to examine the full hierarchical splitting of nodes into communities. plot(as.dendrogram(friend_comm_eb)) This is a dendrogram plot for the edge betweenness results. The y-axis captures the order that the communities are split, with higher values indicating that nodes (in separate branches) are split out early in the process of edge removal (and are thus less likely to be placed in a community). The dendrogram is hierarchical, showing the composition of the communities at different levels of disaggregation. For example, 4 and 16 are split out first, followed by splitting (11, 6, 5, 3, 15, 12, 10, 9, 1) and (14, 13, 8, 7, 2). The next level splits (11, 6, 5, 3) and (15, 12, 10, 9, 1). We can use the dendrogram to visually examine the communities at different levels of disaggregation. As we move down the dendrogram, we look at more disaggregated solutions, with a higher number of communities. 8.3.3 Scree Plots We will often want to go beyond visual inspection and more formally examine the clustering solutions at different levels of dissaggregation. This amounts to asking how the fit changes as the number of communities increases. The goal is to produce a figure where the modularity scores are plotted against the number of communities. We can use these scree plots as a means of deciding what range of solutions are worth looking at in more detail. In this way, we can do a little better than automatically using the solution with the highest modularity score. This is especially important for larger networks, as there are many possible partitions, and many solutions will offer similar modularity scores. We will begin by writing a function to extract the number of communities and modularity scores over all levels of disaggregation. The arguments are communities (the communities object) and graph (the igraph network of interest). The function assumes that the community detection algorithm has a hierarchical structure (as with walktrap or edge betweenness). extract_modularity_data &lt;- function(communities, graph){ # Arguments: # communities: igraph communities object # graph: igraph object mems_list &lt;- list() # list where membership information will be saved num_communities &lt;- NA # vector where number of communities will be saved modularities &lt;- NA # avector to store modularity scores # Extracting levels of aggregation, or steps, # in the hierarchical merge data. num_merges &lt;- 0:nrow(communities$merges) # Note that we start from 0 as the first level # corresponds to all nodes in their own community, # and that does not have a row in the merge data frame. # Looping through each level of aggregation for (x in 1:length(num_merges)){ # We first extract the membership # information at the given merge level using a # cut_at function. The inputs are the community # object and the merge step of interest. mems_list[[x]] &lt;- cut_at(communities, steps = num_merges[x]) # Now we calculate the number of communities associated # with the given clustering solution: num_communities[x] &lt;- length(unique(mems_list[[x]])) # Let&#39;s also calculate the modularity score, just to make sure # we get the right value for that set of community memberships: modularities[x] &lt;- modularity(graph, mems_list[[x]]) } # We will now put together our extracted # information in a data frame. plot_data &lt;- data.frame(modularity = modularities, num_communities = num_communities) # Let&#39;s reorder to go from low number of communities to high: mems_list &lt;- mems_list[order(plot_data$num_communities)] plot_data &lt;- plot_data[order(plot_data$num_communities), ] rownames(plot_data) &lt;- 1:nrow(plot_data) # outputting results in a list: return(list(summary_data = plot_data, membership_list = mems_list)) } Let's go ahead and run our function, extract_modularity_data(), using our edge betweenness results as input. modularity_data &lt;- extract_modularity_data(communities = friend_comm_eb, graph = net182_friend_und) The output is a list with two elements. The first is a summary data frame, showing the modularity scores as the number of communities increases. The second is a list, showing the community memberships under each clustering solution (one for each row in the summary data frame). Let's extract the summary data frame and take a look: summary_data &lt;- modularity_data[[1]] summary_data ## modularity num_communities ## 1 0.000000000 3 ## 2 0.269557823 4 ## 3 0.279761905 5 ## 4 0.238945578 6 ## 5 0.211734694 7 ## 6 0.147675737 8 ## 7 0.103458050 9 ## 8 0.064909297 10 ## 9 0.021825397 11 ## 10 0.007936508 12 ## 11 -0.013888889 13 ## 12 -0.044501134 14 ## 13 -0.061507937 15 ## 14 -0.073412698 16 The first column reports the modularity score for each clustering solution. The second column is the number of communities associated with that modularity score. We are now in a position to plot the results. We will use ggplot() to create a scree plot with number of communities on the x-axis and modularity score on the y-axis. We will include a line and bar plot. Note that we could subset our input data frame (summary_data) if we only wanted to plot some of the modularity scores; this can be useful in large networks where the number of values to plot can be quite large. ggplot(summary_data, aes(num_communities, modularity)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;grey&quot;) + geom_line(color = &quot;black&quot;, linetype = &quot;solid&quot;) + geom_point(shape = 19, size = 1, color = &quot;black&quot;) + labs(title = &quot;Modularity by Number of Communities&quot;, x = &quot;Number of Communities&quot;, y = &quot;Modularity&quot;) Such plots are useful in trying to decide what is the optimal number of communities. In general, we want solutions with high modularity, but we may not necessarily choose the one with the highest modularity, especially if adding more communities only marginally improves the fit and the simpler solution offers a more intuitive, substantively clearer story. Additionally, many solutions may offer similar modularity scores, and scree plots can help us determine which solutions to look at in more detail. Looking at our plot, it is clear that the two best solutions are with 4 or 5 communities, after which the modularity scores start to decrease. We would want to examine these two solutions more carefully. In fact, we have already examined the 5 community solution, as this is the solution with the highest modularity, automatically included in the original edge betweenness results. The 4 community results are not as easily accessed. We first need to extract the membership list from modularity_data (the outputted object from our function), showing the community memberships for each solution represented in the summary plot above. The membership list is the second element in modularity_data. mems_list &lt;- modularity_data[[2]] We will now determine what part of the membership list to grab. We want the membership ids associated with 4 communities. mems_ids &lt;- which(summary_data$num_communities == 4) We now extract the memberships for the 4 community solution. mems_eb4 &lt;- mems_list[[mems_ids]] mems_eb4 ## [1] 1 2 1 3 1 1 2 2 1 1 1 1 2 2 1 4 As it turns out, this is the same set of communities using the walktrap algorithm, using 4 steps. 8.3.4 Multi-level Clustering Another commonly used approach is multi-level clustering, or multi-level modularity optimization. The basic idea is to begin with each node in its own community. Each vertex is then moved to a community to increase modularity the most (or, for each node i, the algorithm selects the move that will increase modularity the most). If no gain is positive it stays in its own community. This phase continues until a local maxima of modularity is reached. The process then begins again based on the newly merged communities, where the new nodes are the communities found in the previous phase. See Blondel et al. (2008). The function in igraph is cluster_louvain(). We can set a resolution argument, which determines the resolution that the algorithm will use when looking for communities; higher values will yield a larger number of smaller communities, while lower values will yield a smaller number of larger communities. Here we will set it at 1, the default, but we might also want to run this over multiple values and explore the results under fewer/greater numbers of communities (similar to the analysis above). We will also set a seed to help replicate the results. set.seed(100) friend_comm_multi &lt;- cluster_louvain(graph = net182_friend_und, resolution = 1) Let's look at the multi-level community structure. friend_comm_multi$memberships ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] ## [1,] 1 2 3 4 1 1 2 2 3 1 1 3 2 2 3 5 ## [2,] 1 2 1 3 1 1 2 2 1 1 1 1 2 2 1 4 We can see that there are two rows, or levels. We can think of this as looking at the community/group structure at two different levels of granularity. The output is organized from least aggregated (row 1) to most aggregated (row 2). Let's do a quick table to see how individuals at one level are nested at the second level. mems_mult_level1 &lt;- friend_comm_multi$memberships[1, ] mems_mult_level2 &lt;- friend_comm_multi$memberships[2, ] table(mems_mult_level1, mems_mult_level2) ## mems_mult_level2 ## mems_mult_level1 1 2 3 4 ## 1 5 0 0 0 ## 2 0 5 0 0 ## 3 4 0 0 0 ## 4 0 0 1 0 ## 5 0 0 0 1 We can see that everyone in community 1 at the second level is split into two communities at the first level. We can also look at the modularity associated with each solution: friend_comm_multi$modularity ## [1] 0.2559524 0.2695578 And now we plot the two solutions. par(mfrow = c(1, 2)) #4 communities plot(net182_friend_und, layout = layout, vertex.color = mems_mult_level2, edge.color = &quot;light gray&quot;, vertex.size = 20, main = &quot;Level 2 Community Membership&quot;) #5 communities plot(net182_friend_und, layout = layout, vertex.color = mems_mult_level1, edge.color = &quot;light gray&quot;, vertex.size = 20, main = &quot;Level 1 Community Membership&quot;) This is similar to what we saw with the edge betweenness at different levels of aggregation, although the 5 community solution here is a little different (and offers a somewhat lower modularity score). More generally, the edge betweenness and multilevel algorithms agree on the basic divides in the network (when there are 4 communities), but not on how to divide the network further (looking within the larger communities). Note that other algorithms, like fast and greedy and label propagation offer similar results, although that will not always be the case. See other options: ?cluster_fast_greedy ?cluster_label_prop ?cluster_spinglass 8.3.5 Cohesive Blocking As a fourth example, we consider the cohesive blocking scheme of Moody and White to look at cohesive groups in the friendship network (Moody and White 2003). The idea is to hierarchically subset the network into groups based on vertex connectivity. A set of nodes are k-connected if it would take the removal of k nodes before all members could no longer reach other. The algorithm works by successively finding sets of nodes with higher levels of connectivity. This builds on the kinds of ideas we explored above in the cohesion section. For example, in the earlier calculation of vertex connectivity, we saw that all non-isolates were 4-connected. The function to run the cohesive blocking scheme of Moody and White is cohesive_blocks(). friend_comm_cohesive &lt;- cohesive_blocks(graph = net182_friend_und) friend_comm_cohesive ## Cohesive block structure: ## B-1 c 0, n 16 ## &#39;- B-2 c 4, n 14 ooo.oooooo ooooo. The output consists of block membership and cohesion (for each block). It looks like there are two blocks, one with vertex connectivity 0 and the second with vertex connectivity 4. We can also plot the results. plot(friend_comm_cohesive, net182_friend_und) Clearly the blocking scheme first separates the isolates from the main component. Everyone in the main component is connected at cohesion level 4 so that it takes the removal of 4 people to disconnect the set. This mirrors the results we saw above. So far, we have walked through different algorithms to determine the communities in the friendship network. The results suggest that that there are likely 5 well-defined communities (including the two isolates). The cohesive blocking scheme would suggest that the main component, while potentially splitting into 3 subgroups, is itself still fairly cohesive (as all members are connected by 4 independent paths). We can take the results of the community analysis and do further analysis to tease out the substantive implications of the found communities. We could ask about behavioral norms across communities (e.g., deviant behaviors), the identities mapped onto each community, the relationships between communities (i.e., do members of one group dislike another?), and so on. Here we will look at how demographic characteristics do (or do not) map onto the communities. We will then look at the relationships between communities. 8.4 Demographic Characteristics For the sake of simplicity, we will use the communities based on the edge betweenness algorithm and focus on a single characteristic, racial identification. The question is how salient is race in this classroom. Does race map strongly onto the communities? Or are the communities organized around something else? We will begin by plotting the network again, but this time we want to color the nodes by race. We first set the color for the nodes based on race: blue if black and white if white. library(car) cols &lt;- recode(class182_attributes$race, as.factor = F, &quot;&#39;black&#39; = &#39;blue&#39;; NA = NA; else = &#39;white&#39;&quot;) Let's check to make sure it worked. table(class182_attributes$race, cols) ## cols ## blue white ## black 8 0 ## white 0 8 Looks good. Now, let's produce a plot where we color the nodes by race and put polygons around the found communities (based on the edge_betweenness partitioning above). plot(friend_comm_eb, net182_friend_und, col = cols, layout = layout, main = &quot;Race Mapped onto Communities&quot;) We can see that this classroom divides into three communities based on racial composition: a predominately black community, a predominately white community and one community that is split evenly. This suggests that social communities can, but need not, correspond to a priori social categories, like white/black. As a network analyst, the task is to ask what social communities emerge rather than assume they map onto pre-determined labels. Similarly, we can see that the large community that is subdivided is divided within itself along racial lines (white (3, 6, 5, 11) versus more integrated (1, 9, 10, 12 15)). We can also calculate the proportion white/black in each community and compare that to what we would expect by chance. Let's write a little function to calculate the racial distribution for each community. proportion_function &lt;- function(communities, attribute){ # Arguments: # communities: vector of community membership # attribute: attribute vector # Here we calculate the proportion that fall into each category # found in attribute. We first do a table and then find the proportion # in each category. This is done for each community (using tapply # over the communities). dat &lt;- tapply(factor(attribute), communities, function(x) {y &lt;- table(x); y / sum(y)}) # We then output it as a matrix using do.call return(do.call(rbind, dat)) } Let's use our function with the edge_betweenness membership to define the communities and the race vector to define the attributes. proportion_function(communities = mems_eb, attribute = class182_attributes$race) ## black white ## 1 0.6 0.4 ## 2 0.8 0.2 ## 3 0.0 1.0 ## 4 0.0 1.0 ## 5 1.0 0.0 We see that in community 1, .6 are black and .4 are white. In community 2, .8 are black and .2 and white, and so on. Note that the above bit of code, which utilized a function, could be accomplished in a number of ways. For example: table_list &lt;- list() race_factor &lt;- factor(class182_attributes$race) for (i in 1:max(mems_eb)){ tab &lt;- table(race_factor[mems_eb == i]) table_list[[i]] &lt;- tab / sum(tab) } do.call(rbind, table_list) ## black white ## [1,] 0.6 0.4 ## [2,] 0.8 0.2 ## [3,] 0.0 1.0 ## [4,] 0.0 1.0 ## [5,] 1.0 0.0 We can also compare the proportions in each community to what we would expect under random expectations (a similar problem to what we saw in the ego network tutorial). For example, we could hold the network and the community membership fixed but randomly assign characteristics to the members, showing what the distribution of race would like if students were randomly distributed across the communities. In this case, we will do this a bit informally and start with a simple table of race in the classroom. table(class182_attributes$race) ## ## black white ## 8 8 In this case, with 8 people identifying as black and 8 as white we would expect .50 of each community to be black and .50 to be white. Clearly, the actual results deviate from this random baseline, particularly in community 2 and 3, which are predominately black and white respectively. Community 1, the 'mixed' community, is right at what we would expect if individuals were randomly forming communities, with no sorting based on race. Communities 4 and 5 are both isolated nodes and thus no variation is possible. We could, of course, also use simulation to generate the null hypothesis; where through a series of draws, we randomly assign individuals to communities, calculate the proportion white/black in each community and compare those values to the observed values (on the actual network). 8.5 Community Overlap Here we examine the level of contact between our communities. How many friendships are within (or across) communities? This will tell us about the interconnections (or lack thereof) between the found communities. This combines aspects of community detection with more general questions about cohesion; as the connections between communities largely structure how cohesive the network is as a whole. We will begin by creating a table of within and between community friendship ties (and non-ties). This necessitates having two pieces of information for each dyad: first, if i-j are friends; and second, if i-j are in the same community. Let's first grab the dyad data frame from above. class182_dyads &lt;- class182_networkdata[, c(&quot;ego&quot;, &quot;alter&quot;)] head(class182_dyads) ## ego alter ## 1 1 1 ## 2 1 2 ## 3 1 3 ## 4 1 4 ## 5 1 5 ## 6 1 6 Now, let's get a vector showing if ego and alter are tied together. First, we grab the matrix. We then put a NA on the diagonal as we don't want to consider ties to oneself. We then take the take the matrix and string it out into a vector, using a transpose to make sure the order is correct, running from rows to columns: 1-2; 1-3... mat182_friend &lt;- as_adjacency_matrix(net182_friend_und, sparse = F) diag(mat182_friend) &lt;- NA friend &lt;- c(t(mat182_friend)) head(friend) ## [1] NA 1 0 0 1 1 A 1 means that i and j are friends and 0 means that i and j are not friends. Now, we make a vector showing if each dyad is in the same community or not. For this example, we will use the membership based on the multilevel solution with 4 communities, mems_mult_level2. The following bit of code takes the membership based on the multilevel solution, subsets it by ego (class_182_dyads$ego), subsets it by alter (class_182_dyads$alter), and then asks if ego and alter are in the same community or not. ego_community &lt;- mems_mult_level2[class182_dyads$ego] alter_community &lt;- mems_mult_level2[class182_dyads$alter] same_comm &lt;- ego_community == alter_community Putting this together, we are now in a position to create a table of community status (are i and j in the same community?) by friendship (are i and j friends?). We divide by 2 as the ij information is the same as the ji information and there is no reason to double count. table(same_comm, friend) / 2 ## friend ## same_comm 0 1 ## FALSE 67 7 ## TRUE 11 35 The table suggests that 67 dyads are not friends and are not in the same community; 11 are not friends but are in the same community; 7 are friends but are in different communities; and 35 are friends and in the same community. Given this kind of table, we can calculate various statistics of interest. Here we will calculate an odds ratio, showing the odds of two people in the same community choosing each other as friends (compared to the odds of two people in different communities choosing each other as friends). (35 * 67) / (7 * 11) ## [1] 30.45455 Clearly there are many more ties within groups than between groups. But, as we have seen throughout the analysis, there are enough cross-group ties (coupled with the small size of the network) to make the overall network quite cohesive. Similarly, we see that race does, by and large, map onto the groups. Such mapping of demographics onto social groups does not really pose a strong threat to the overall solidarity of the classroom, however, as the groups themselves are interconnected. 8.6 Big Data Example We now turn to a second, completely different example using the email patterns of individuals working at a large European research institute. The goal is to work through a community analysis on a much larger network than the classroom data used above. The email network has 265,214 nodes and over 400,000 edges. The data are freely available at: https://snap.stanford.edu/data/email-EuAll.html Let's first read in the data and construct a network. url3 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/email-EuAll.txt&quot; email_edges &lt;- read.table(file = url3) dim(email_edges) ## [1] 420045 2 head(email_edges) ## V1 V2 ## 1 0 1 ## 2 0 4 ## 3 0 5 ## 4 0 8 ## 5 0 11 ## 6 0 20 The edges capture if person i sent at least one email to person j. We need to clean this up a bit before we can create our network. First, let's put some useful column names on the data. colnames(email_edges) &lt;- c(&quot;ego&quot;, &quot;alter&quot;) Second, we need to add 1 to all of the ids, as the raw file was indexed by 0, but we want to start with 1. email_edges &lt;- email_edges + 1 There are also a couple of cases where i sent an email to i, and let's take those out. email_edges &lt;- email_edges[email_edges[, 1] != email_edges[, 2], ] Now, let's create our igraph object. We make sure to capture any isolates by setting the vertices argument, denoting the ids of all nodes in the network. email_network &lt;- graph_from_data_frame(d = email_edges, directed = T, vertices = (id = as.numeric(1:265214))) Let's treat the network as undirected. email_network &lt;- as.undirected(email_network, mode = &quot;collapse&quot;) Given the number of nodes in the network, strategies that were effective with the classroom network have to be tweaked or even abandoned with a network of this size. Our interpretation of the output will also have to shift somewhat. In particular, some of the algorithms are likely to yield solutions with a small number of large communities (as such solutions may optimize modularity when looking at a very coarse resolution). This may tell us that a few large divides exist, but will not be so useful in giving us more detailed communities that make up these larger divisions. It may then be important to look at the communities that exist at different levels of the network, or different granularity. Let's take a look at one strategy for dealing with larger networks. 8.6.1 Multi-level Clustering Past work has shown that some algorithms scale much better than others as network size increases. In particular, some algorithms will have quite long run times and utilize large amounts of memory on large networks. This doesn't mean one couldn't use these algorithms, but it is important to understand the potential costs of such a strategy. For example, Yang, Algesheimer, and Tessone (2016) show that the multilevel clustering approach works well on large networks, compared to other approaches, like edge betweenness or spin glass. Here we will utilize a multi-level approach on a network with over 200000 nodes. set.seed(101) email_comm_multi &lt;- cluster_louvain(graph = email_network, resolution = 1) This can take much longer with other algorithms. For example: email_comm_wt &lt;- cluster_walktrap(graph = email_network, steps = 4, membership = T) We now take a look at the community/group structure at different levels of aggregation. email_mems &lt;- email_comm_multi$memberships nrow(email_mems) ## [1] 3 It looks like there are 3 different levels. Let's see how many communities exist at each level of aggregation. Here, we write a little function to calculate the number of people in each community. length_func &lt;- function(x) {length(unique(x))} And now we apply it over the rows of the membership matrix. num_comms &lt;- apply(email_mems, 1, length_func) num_comms ## [1] 18351 16022 15920 We can see that the least aggregated solution has 18351 communities and the most aggregated has 15920 communities. Let's also look at the modularity of each solution. email_comm_multi$modularity ## [1] 0.6897286 0.7852888 0.7906157 We will focus on the most aggregated solution (level 3) and see what the results look like. As a first step to assessing the found communities, let's see how big the communities are based on this solution. We will calculate a table on the membership vector and a summary on that table, to see what the size distribution of communities looks like. mems_email &lt;- email_comm_multi$memberships[3, ] summary(as.numeric(table(mems_email))) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 2.00 2.00 16.66 2.00 16233.00 As we can see, the median (and 75 percentile) is size 2, while the max is 16233, suggesting that most of the groups are small but that a few large groups exist. Some of these larger communities are over 1000, even 10000 nodes. It is likely that a community of 10000 nodes is too large to be directly interpretable, even if does constitute a set of people with relatively high in-group density. Thus, one strategy would be to take the initial communities as the new starting point, rather than the whole network, and do community detection within each community. This would break the initially found communities into yet smaller communities, yielding a nested structure of communities within communities. This means we would be able to look at the community structure at increasingly high levels of granularity, while maintaining the overall (low resolution) picture seen with the initial results. 8.6.2 Looking at the Community Structure within the Initial Communities Here, we demonstrate how to perform community detection within the set of communities already detected. We will utilize a different algorithm to perform the community detection, as having smaller networks makes it more plausible to use other options. For each desired community that we want to divide further, we need to form a network of just those members and then perform community detection on that subgraph. Let's write a little function to do this task: create_subcommunity &lt;- function(graph, initial_communities, community_number){ # Arguments: # graph: igraph object # initial_communities: the original community memberships # community_number: the community number of interest (i.e., the # community that you want to divide further). # here we will create a subgraph of just the community of interest in_community &lt;- which(initial_communities == community_number) subgraph1 &lt;- induced_subgraph(graph = graph, vids = in_community) # We now perform a community detection algorithm (using fast and greedy) # on the subgraph comm1 &lt;- cluster_fast_greedy(graph = subgraph1) # grabbing the community membership of each # person in the subgraph mems_subgraph1 &lt;- membership(comm1) # Now we grab the ids of those in the subgraph, so we can map them back # onto the original, full network ids_map &lt;- as.numeric(vertex_attr(subgraph1, &quot;name&quot;)) mems_new &lt;- initial_communities # just copying the original communities # Here, we begin to relabel the communities so we can put # them back onto the original set of communities on # the full network. We want to make sure that # these new community ids are unique, so we take the max # original community number and add that to the community # ids on the subgraph. mems_subgraph1_relabel &lt;- mems_subgraph1 + max(initial_communities) # Here we put the new communities onto a vector of community # membership corresponding to the whole network. mems_new[ids_map] &lt;- mems_subgraph1_relabel # Note we just change those in the subgraph of interest. # We can then relabel all communities, if desired, to take out the old # community number and put in order from low to high: num_comms_new &lt;- length(unique(mems_new)) mems_updated &lt;- as.numeric(as.character(factor(mems_new, labels = 1:num_comms_new))) # here we output the subgraph, the membership and the updated # vector of community membership: return(list(subgraph = subgraph1, mems_subgraph = mems_subgraph1, membership_updated = mems_updated)) } Now, let's run it on an example community, here community number 33, which has over 3000 nodes. The inputs are the email network (set using the graph argument), the original set of communities (set using initial_communities) and the community of interest to look within (set using community_number). subcommunity_dat &lt;- create_subcommunity(graph = email_network, initial_communities = mems_email, community_number = 33) Let's grab the subgraph, the membership within that subgraph and the updated membership vector on the entire graph. subnet &lt;- subcommunity_dat$subgraph mems_subnet &lt;- subcommunity_dat$mems_subgraph mems_updated &lt;- subcommunity_dat$membership_updated Now, let's plot the subgraph with the nodes colored by community membership, based on this more disaggregated analysis. layout_email &lt;- layout.fruchterman.reingold(subnet) plot(subnet, vertex.label = NA, vertex.size = .6, layout = layout_email, edge.color = &quot;light gray&quot;, edge.curved = .2, vertex.frame.color = NA, vertex.color = mems_subnet) We can see that there is a set of subcommunities that emerge within community 33 but were missed originally (where everyone in this plot is placed in the same community in the original analysis). It is also useful to check the membership of the focal community in the new community vector (mems_updated). Here we do a table on mems_updated, only looking at those nodes who were originally in community 33 (defined in mems_email). table(mems_updated[mems_email == 33]) ## ## 15920 15921 15922 15923 15924 15925 15926 15927 15928 15929 15930 15931 15932 15933 15934 15935 15936 15937 ## 682 178 180 846 952 104 167 43 72 47 17 16 14 6 5 3 3 5 We can see that everyone who used to be in community 33 is now split between 18 communities. The labels of these communities have also been set to unique values in the overall vector of communities. We can also check the modularity score with this new set of community memberships: modularity(email_network, mems_updated) ## [1] 0.7901395 Note that subdividing community 33 did not actually yield a higher modularity score. Nonetheless, it still may be of substantive interest to see what the subcommunities (within the larger groupings) look like across the network. Given our function, we could extend this kind of analysis to any community of interest. We could loop over (or use lapply()) to do this over any set of communities. For example, we could further subdivide all communities larger than a certain size threshold. This would yield a set of communities analyzed at different levels of resolution. For example, here we look at community 2, the largest community: subcommunity_dat &lt;- create_subcommunity(graph = email_network, initial_communities = mems_updated, community_number = 2) Two things to note. First, we must remember that the initial_communities input would have to be updated with the new membership list each time we update that vector (here we used mems_updated). Remember that we defined mems_updated as: mems_updated &lt;- subcommunity_dat$membership_updated. And second, note that the community of interest may change number as we update the community membership vector and would need to be updated accordingly as the process continues iteratively. Overall, this tutorial has offered a number of analyses related to cohesion and community structure. These topics are important in their own right and also serve as building blocks for other analyses, such as work on hierarchy, two-mode networks, culture and diffusion (see Chapters 9, 11, 12 and 14). "],["ch9-Network-Centrality-Hierarchy-R.html", "9 Network Centrality and Hierarchy 9.1 Setting up the Session 9.2 Centrality 9.3 Centralization 9.4 Clustering and Hierarchy", " 9 Network Centrality and Hierarchy This tutorial walks through the analysis of centrality and hierarchy in R. We will begin by looking at various centrality measures, to determine how they are interrelated, and to discern what they mean. In this way students develop different metrics for node network positioning. We will then explore network-level measures of hierarchy, moving away from individual level positioning to look at the structure of the whole network. This tutorial will build directly on the material from previous tutorials, most clearly from the network measurement tutorial (Chapter 3) and the dyad/triad tutorial (Chapter 7). 9.1 Setting up the Session We will work primarily with the igraph package for this tutorial. library(igraph) This tutorial uses classroom network data collected by Daniel McFarland. The class is a biology 2 class at a public high school. We will focus on two network relations, one based on social interaction (i talks in a social way with j) and another based on task-based interactions (i actively engages in a task with j). Let's go ahead and read in the network data for the social relation (read in from a URL). url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/social_interactions_s641.csv&quot; social_data &lt;- read.csv(file = url1) Looking at the first six rows: head(social_data) ## ego alter social_tie ## 1 1 1 0.000 ## 2 1 2 0.000 ## 3 1 3 0.000 ## 4 1 4 0.000 ## 5 1 5 5.625 ## 6 1 6 1.500 We now have a data frame called social_data. The first column is the ego, the second column is the alter, and the third column shows the frequency of social interactions between the two students. We will reduce the data frame to just include those dyads where social interaction occurred: edgelist_social &lt;- social_data[social_data$social_tie &gt; 0, ] head(edgelist_social) ## ego alter social_tie ## 5 1 5 5.625 ## 6 1 6 1.500 ## 22 1 22 1.875 ## 44 2 22 0.375 ## 74 4 8 1.875 ## 89 5 1 5.250 Now we can go ahead and create our igraph object based on the edgelist defined above. The size of the network is 22, so we will set the vertices input to define the ids of the nodes, running from 1 to 22. s641_social &lt;- graph_from_data_frame(d = edgelist_social, directed = T, vertices = (id = 1:22)) Note that if we did not want the isolates included we could have done: net641_social_noisolates &lt;- graph_from_data_frame(d = edgelist_social, directed = T) And now we read in the task data: url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/task_interactions_s641.csv&quot; task_data &lt;- read.csv(file = url2) head(task_data) ## ego alter task_tie ## 1 1 1 0 ## 2 1 2 0 ## 3 1 3 0 ## 4 1 4 0 ## 5 1 5 0 ## 6 1 6 0 The task_tie variable shows the frequency of task-based interactions between nodes i and j. We will now reduce the data to just those dyads where a task interaction occurred and create the igraph object. We will treat the task network as undirected as if i does a task with j, j does a task with i. We will thus reduce the edgelist so that each edge is only listed once (accomplished by reducing the edgelist to rows where the ego id is smaller than the alter id). edgelist_task &lt;- task_data[task_data$task_tie &gt; 0, ] edgelist_task &lt;- edgelist_task[edgelist_task$ego &lt; edgelist_task$alter, ] s641_task &lt;- graph_from_data_frame(d = edgelist_task, directed = F, vertices = (id = 1:22)) We will now plot both networks. par(mfrow = c(1, 2)) plot(s641_social, vertex.frame.color = NA, edge.arrow.size = .25, vertex.size = 8, main = &quot;Social Interactions&quot;, margin = -.08) plot(s641_task, vertex.frame.color = NA, edge.arrow.size = .25, vertex.size = 8, main = &quot;Task Interactions&quot;, margin = -.08) From the figure alone we can see that the features of these two networks are very different. The task network would appear to have one very central node, with lots of ties, while the social network splits more clearly into groups with one node acting as a bridge between the groups. We will use measures of centrality and centralization to more formally explore these features. Our main substantive goal is to determine which nodes are most important in the classroom and how (or if) this varies across network relation and measure of centrality. Are individuals who are prominent in the task network also prominent in the social interaction network? Which nodes act as bridges? Are they the same nodes with the highest degree? We also want to uncover something about the overall level of inequality and hierarchy that exists in this classroom. Is this a world where one node dominates? 9.2 Centrality 9.2.1 Centrality Measures for Social Interactions Centrality measures, such as degree and betweenness, capture something about which nodes are most important to the network. These node-level measures can be used as predictors of other outcomes, like attitudes, behaviors, etc. The centrality scores can also be used as the main outcome of interest. Here, we will walk through the code to calculate a number of commonly used measures, with a particular focus on how different measures offer similar or different pictures of which nodes are more central to the network. We begin with the social interaction network. Indegree centrality measures how many ties each node receives, in this case the number of people that talks to node i in a social way. The function is degree(). The main arguments are graph (the network, as an igraph object) and mode (in, out or total). For indegree we set mode to \"in\". indegree_social &lt;- degree(graph = s641_social, mode = &quot;in&quot;) Let's look at first 6 values: head(indegree_social) ## 1 2 3 4 5 6 ## 3 1 0 1 3 3 This means that 3 people talk to node 1, 1 person talks to node 2, and so on. Outdegree centrality measures how many ties the node sends out, in this case the number of people that the node talks to in a social way. For outdegree we set mode to \"out\". outdegree_social &lt;- degree(graph = s641_social, mode = &quot;out&quot;) head(outdegree_social) ## 1 2 3 4 5 6 ## 3 1 0 1 3 3 Closeness is the inverse of the mean geodesic distance between a given node and all other nodes. In a directed network, we can think of in-closeness centrality as the average number of steps one would have to go through to get TO a given node FROM all other reachable nodes in the network. Out-closeness centrality measures the same thing with the directionality reversed (average number of steps to get from i to all other j nodes). We invert the mean distance to make it a closeness measure, where higher values mean the nodes are more central (i.e., closer to other nodes). When calculating closeness, igraph (at least more recent versions of the package) will only include nodes that actor i can reach in the calculation, and so is most easily interpreted for fully connected graphs, where all nodes can reach all other nodes. The function is closeness() and the arguments are: graph = network of interest, as igraph object mode = \"in\", or \"out\" normalized = T/F, should scores be normalized? (divided by n-1, where n is the number of people in the network) Let's first calculate out-closeness using the igraph function. outcloseness_social &lt;- closeness(graph = s641_social, mode = &quot;out&quot;, normalized = T) head(outcloseness_social) ## 1 2 3 4 5 6 ## 0.4210526 0.3720930 NaN 1.0000000 0.4210526 0.3200000 We can interpret those values as the inverse of the mean distance of each node to all other nodes that the focal node can reach. Isolates are assigned an NA value. Now we will do the same thing for in-closeness. incloseness_social &lt;- closeness(graph = s641_social, mode = &quot;in&quot;, normalized = T) head(incloseness_social) ## 1 2 3 4 5 6 ## 0.4117647 0.3589744 NaN 1.0000000 0.4117647 0.3181818 An alternative version of this is to take the mean based on the inverse distance matrix. This avoids the problem of excluding unreachable nodes when calculating the mean distance to or from each node and is more appropriate when all nodes cannot reach all other nodes. Here we get the distance matrix in the right form for that calculation. dist_mat_social &lt;- distances(graph = s641_social, mode = &quot;out&quot;) diag(dist_mat_social) &lt;- NA #ignoring the diagonal dist_mat_social_inverted &lt;- 1 / dist_mat_social #inverting distance matrix Now we can use apply() to take the mean over all of the rows at once. Note that we use a 1 within the apply() function to take the mean over the rows, set FUN to the mean and include an argument to exclude the NAs. outcloseness_social2 &lt;- apply(dist_mat_social_inverted, MARGIN = 1, FUN = mean, na.rm = T) head(outcloseness_social2) ## 1 2 3 4 5 6 ## 0.39444444 0.32698413 0.00000000 0.04761905 0.39444444 0.32698413 Let's compare the two versions of out-closeness centrality: cor(outcloseness_social, outcloseness_social2, use = &quot;complete.obs&quot;) ## [1] -0.6850724 We see a pretty strong, negative correlation. This is the case as the original calculation (outcloseness_social) calculates closeness only including the reachable nodes for each actor. This means that nodes like 4, 8, 11 and 15 get surprisingly high closeness values (1), as they reach only one other node at distance 1. This might not be what we want as these nodes are, in fact, not very central in the network (see figure above) but are assigned a high centrality value. The alternative version of out-closeness handles this better and assigns a low value for these nodes. If we remove those 'odd' cases, the two versions are very highly correlated: cor(outcloseness_social[-c(4, 8, 11, 15)], outcloseness_social2[-c(4, 8, 11, 15)], use = &quot;complete.obs&quot;) ## [1] 0.9806626 And now we calculate the alternative version of in-closeness. Here, we take the mean over each column of the inverted distance matrix to get the in-coming paths. This is accomplished by using a 2 for MARGIN in the apply() statement: incloseness_social2 &lt;- apply(dist_mat_social_inverted, MARGIN = 2, FUN = mean, na.rm = T) head(incloseness_social2) ## 1 2 3 4 5 6 ## 0.34682540 0.27936508 0.00000000 0.04761905 0.34682540 0.29523810 Betweenness centrality is based on the number of shortest paths going through a specific vertex; it is returned by the betweenness() function. We need to set normalized to T/F, determining if the scores should be normalized by the number of pairs of nodes (not including i), capturing the number of possible paths a node could be between. betweenness_social &lt;- betweenness(graph = s641_social, normalized = F) head(betweenness_social) ## 1 2 3 4 5 6 ## 24 0 0 0 24 28 Eigenvector centrality gives greater weight to a node the more it is connected to other highly connected nodes. A node connected to five high-scoring nodes will have higher eigenvector centrality than a node connected to five low-scoring nodes. Thus, it is often interpreted as measuring a node's network importance. In directed networks, there are 'In' and 'Out' versions. In information flow studies, for instance, In-Eigenvector scores would reflect which nodes are high on receiving information, while Out-Eigenvector scores would reflect which nodes are high on broadcasting information. For these data, we will simply symmetrize to generate an undirected eigenvector centrality score. Unlike the other centrality measures, the evcent() function returns a complex object rather than a simple vector. Thus, we need to first get the evcent output and then select the eigenvector scores from it. Here we symmetrize our network before calculating eigenvector centrality, making a tie between i and j if i is tied with j or j is tied with i. s641_social_undirected &lt;- as.undirected(s641_social, mode = &quot;collapse&quot;) We now calculate eigenvector centrality: ev_obj_social &lt;- eigen_centrality(s641_social_undirected) eigen_social &lt;- ev_obj_social$vector head(eigen_social) ## 1 2 3 4 5 6 ## 2.418251e-01 1.735366e-01 3.654745e-17 3.060687e-17 2.418251e-01 1.004841e-01 Note that we can calculate a closely related Bonacich power centrality score using the power_centrality() function. We now have a basic set of centrality scores for our social interaction network. To facilitate comparison across the different measures, we'll construct a data frame with the nodes as rows and the centrality scores as columns. We include a variable for the network type, here social interaction, as well as the ids of the nodes. We first grab the ids from the igraph object. ids &lt;- V(s641_social)$name And now we put together the data frame. central_social &lt;- data.frame(ids = ids, net = &quot;social&quot;, indegree = indegree_social, outdegree = outdegree_social, incloseness2 = incloseness_social2, outcloseness2 = outcloseness_social2, between = betweenness_social, eigen = eigen_social) head(central_social) ## ids net indegree outdegree incloseness2 outcloseness2 between eigen ## 1 1 social 3 3 0.34682540 0.39444444 24 2.418251e-01 ## 2 2 social 1 1 0.27936508 0.32698413 0 1.735366e-01 ## 3 3 social 0 0 0.00000000 0.00000000 0 3.654745e-17 ## 4 4 social 1 1 0.04761905 0.04761905 0 3.060687e-17 ## 5 5 social 3 3 0.34682540 0.39444444 24 2.418251e-01 ## 6 6 social 3 3 0.29523810 0.32698413 28 1.004841e-01 Now we'll examine the table to find the most central nodes according to the different measures we have. When looking at each of these measures, it's a good idea to have your plot of the social interaction network on hand so you can sanity-check the results. Let’s order each column from high to low centrality seeing which nodes have the top centrality on each measure. We will use an order() function to rank order the nodes. The order() function returns a vector in ascending or descending order, showing which cases have the highest/lowest values. We will set decreasing equal to T to make it descending (running high to low). We will set FUN to order within the apply() statement. We useapply() to order all of the columns at once. We exclude the first two columns as they are not centrality scores. apply(central_social[, -c(1, 2)], MARGIN = 2, FUN = order, decreasing = T) ## indegree outdegree incloseness2 outcloseness2 between eigen ## [1,] 18 22 18 22 22 18 ## [2,] 22 18 22 19 12 19 ## [3,] 16 19 16 16 16 22 ## [4,] 19 16 19 18 18 21 ## [5,] 21 17 21 21 6 16 ## [6,] 17 21 17 1 10 17 ## [7,] 1 1 12 5 1 20 ## [8,] 5 5 11 17 5 12 ## [9,] 6 6 1 12 11 5 ## [10,] 12 12 5 20 19 1 ## [11,] 20 20 20 2 21 11 ## [12,] 10 10 6 6 17 2 ## [13,] 11 2 2 10 20 6 ## [14,] 2 4 10 9 2 10 ## [15,] 4 7 15 7 3 15 ## [16,] 7 8 9 4 4 9 ## [17,] 8 9 7 8 7 7 ## [18,] 9 11 4 11 8 3 ## [19,] 15 15 8 15 9 13 ## [20,] 3 3 3 3 13 14 ## [21,] 13 13 13 13 14 4 ## [22,] 14 14 14 14 15 8 If we start with indegree, we see the top nodes are 18, 22 and 16. Or, for outdegree, the top three nodes are 22, 18 and 19. We see nodes 18, 22 and 16 are most central for inclosenss. Finally, we see similar, but somewhat different ordering for betweenness, with nodes 22, 12 and 16 the top three. Node 12 in particular has much higher betweenness centrality than they do for degree, closeness, etc. Let's make a plot with these summary statistics. To visualize these data, we can create a barplot for our centrality measures. To make this task a little easier, we will first rearrange our data set to make it a 'long' format. We will focus on just a handful of our centrality measures, indegree, incloseness2, betweenness and eigen centrality. Here we use the melt() function (in the reshape package) to rearrange our data, basically stacking the variables of interest (here centrality scores) on top of each other, differentiated by an id.vars variables (here the ids of the nodes). library(reshape) vars_to_stack &lt;-c(&quot;ids&quot;, &quot;indegree&quot;, &quot;incloseness2&quot;, &quot;between&quot;, &quot;eigen&quot;) social_long &lt;- melt(central_social[, vars_to_stack], id.vars = &quot;ids&quot;) head(social_long) ## ids variable value ## 1 1 indegree 3 ## 2 2 indegree 1 ## 3 3 indegree 0 ## 4 4 indegree 1 ## 5 5 indegree 3 ## 6 6 indegree 3 We can see that we now have three variables. An ids variable corresponding to the node; a variable corresponding to the centrality measure and value corresponding to the actual score for that person on that centrality measure. Now, let's create a barplot using ggplot() for each of our centrality scores. The y-axis is the score, and the x-axis is the node. We will have 4 different panels, one for each measure of interest. We use the aes() function and the geom_bar() function to create the basic bar plot for each node (ids) based on the value variable; and a facet_wrap() function to set up the plot to have 2 columns. library(ggplot2) ggplot(social_long, aes(x = factor(ids, levels = 1:length(ids)), y = value)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + facet_wrap(~ variable, scales = &quot;free_y&quot;, ncol = 2) + xlab(&quot;Ids&quot;) + ylab(&quot;Centrality&quot;) + theme(axis.text = element_text(size = 6.5)) Given our results, what can we say about the social nodes if we compare the bar plots? Who seems to run the show in sociable affairs? Who seems to bridge sociable conversations? For example, we can see that node 22 plays a key bridging role and shows up as highly centrality in general, but particularly for betweenness. Node 22 has the highest centrality by far in betweenness but has similar indegree to other top nodes. This suggests that multiple nodes (16, 18, 19, 22) have high volume (or prominence) in the network, but only one plays the role of bridging different groups in the classroom (22). To highlight this, let's go back to our picture and size the nodes by betweenness centrality (scaled a bit to make the picture a little nicer): plot(s641_social, vertex.size = central_social$between / 5, vertex.label = V(s641_social)$name, edge.arrow.size = 0.25, layout = layout.fruchterman.reingold, main = &quot;Classroom S641 Social Talk&quot;, margin = -.08) 9.2.2 Correlations between Centrality Measures We have so far seen which nodes are the most central on different measures. We now want to formalize this a bit more by computing the correlations between the centrality scores, showing how closely these measures of centrality are interrelated. More substantively, we want to know which measures tend to yield the same nodes as central and which tend to disagree on the most important nodes. Here we generate a table of pairwise correlations. Again, we take out the first column of ids and the second column showing the network type when doing the correlation matrix (we also round the values in the correlation matrix when printing). cor_tab1 &lt;- cor(central_social[, -c(1, 2)]) round(cor_tab1, 3) ## indegree outdegree incloseness2 outcloseness2 between eigen ## indegree 1.000 0.960 0.874 0.860 0.629 0.940 ## outdegree 0.960 1.000 0.822 0.874 0.738 0.914 ## incloseness2 0.874 0.822 1.000 0.879 0.548 0.794 ## outcloseness2 0.860 0.874 0.879 1.000 0.595 0.800 ## between 0.629 0.738 0.548 0.595 1.000 0.507 ## eigen 0.940 0.914 0.794 0.800 0.507 1.000 Indegree and outdegree are very closely correlated (rho = 0.96), indicating that social talk with others is almost always reciprocated (i.e., if you talk to others, they tend to talk back to you). Indegree and outdegree are also highly correlated with eigenvector centrality, indicating that the students that talk the most to others (or, relatedly, are talked to the most by others) are also the ones that are connected to other highly connected students -- possibly indicating high density cliques around these individuals. The degree centralities are less correlated with our closeness centrality scores, suggesting that nodes with high degree are not always (although often) close to other nodes. Betweenness shows the highest correlation with outdegree, followed by indegree. In the case of this particular network, it seems that individuals that talk the most to others are the likeliest to serve as bridges between the particular cliques (see, e.g., 22 in the plot). Note that betweenness is not all that highly correlated with closeness centrality. This suggests that nodes may sit between groups, and thus have high betweenness, but not necessarily be close to all nodes, on average. For example, node 19 has high closeness centrality but not especially high betweenness centrality. If we look at the last plot, we can see that node is 19 deeply embedded in one social group and has ties to node 22, who has high betweenness, connecting different parts of the network. Thus, node 19 has high closeness as they can reach everyone else (through node 22) but low betweenness, as the shortest paths connecting different groups would not have to run through node 19. Thus, if the process that we thought was most important was about information flow based on shortest paths, we may think that node 19 is well positioned to influence the rest of the network. If, however, the key is being the bridge itself, then 19 is clearly not as important as node 22. Thus, while there is much agreement between the centrality scores (with nodes 22, 16, 18 and 19 showing up consistently as central) it is possible for a node to be high on one measure and low on another. 9.2.3 Centrality for Task Interactions We now repeat the analysis for the task interaction network. Note that the in and out measures will be the same as the network is undirected, meaning that we only need one calculation for measures like degree or closeness. It also means that we do not need to set mode as an argument. degree_task &lt;- degree(s641_task) dist_mat_task &lt;- distances(graph = s641_task) diag(dist_mat_task) &lt;- NA dist_mat_task_inverted &lt;- 1 / dist_mat_task closeness_task2 &lt;- apply(dist_mat_task_inverted, MARGIN = 1, FUN = mean, na.rm = T) betweenness_task &lt;- betweenness(s641_task, normalized = F) ev_obj_task &lt;- evcent(s641_task) eigen_task &lt;- ev_obj_task$vector And now we put the results together, as before, into a data frame with all the centrality values. central_task &lt;- data.frame(ids = ids, net = &quot;task&quot;, degree = degree_task, closeness2 = closeness_task2, between = betweenness_task, eigen = eigen_task) head(central_task) ## ids net degree closeness2 between eigen ## 1 1 task 1 0.42857143 0 2.154856e-01 ## 2 2 task 1 0.42857143 0 2.154856e-01 ## 3 3 task 0 0.00000000 0 3.205969e-17 ## 4 4 task 1 0.04761905 0 3.205969e-17 ## 5 5 task 1 0.42857143 0 2.154856e-01 ## 6 6 task 1 0.42857143 0 2.154856e-01 We will now quickly take a look at the nodes with the top centrality scores for the task network. apply(central_task[, -c(1, 2)], MARGIN = 2, FUN = order, decreasing = T) ## degree closeness2 between eigen ## [1,] 22 22 22 22 ## [2,] 18 18 18 18 ## [3,] 17 17 19 17 ## [4,] 19 19 1 21 ## [5,] 21 21 2 19 ## [6,] 13 13 3 13 ## [7,] 16 16 4 20 ## [8,] 20 20 5 16 ## [9,] 1 1 6 2 ## [10,] 2 2 7 14 ## [11,] 4 5 8 6 ## [12,] 5 6 9 7 ## [13,] 6 7 10 11 ## [14,] 7 9 11 1 ## [15,] 8 10 12 5 ## [16,] 9 11 13 10 ## [17,] 10 14 14 15 ## [18,] 11 15 15 9 ## [19,] 14 4 16 8 ## [20,] 15 8 17 3 ## [21,] 3 3 20 4 ## [22,] 12 12 21 12 In this case, we can see nodes 22, 18 and 17 are consistently the most important nodes, but node 22 is by the far most central. This becomes clear if we plot the network, scaling the nodes by degree: plot(s641_task, vertex.size = central_task$degree, vertex.label = V(s641_social)$name, edge.arrow.size = 0.25, layout = layout.fruchterman.reingold, main = &quot;Classroom S641 Task Interactions&quot;, margin = -.08) 9.2.4 Task/Social Correlations We have seen that node 22 dominates task interactions in a way that was less clear-cut in the social interactions network. Let's explore the similarities/differences between the two networks by doing a quick bar plot comparing four centrality measures (degree, closeness2, betweenness and eigen) on social and task centrality. We will first rearrange the task centrality data frame, as we did with the social centrality data frame, to make it in a long format. vars_to_stack_task &lt;- c(&quot;ids&quot;, &quot;degree&quot;, &quot;closeness2&quot;, &quot;between&quot;, &quot;eigen&quot;) task_long &lt;- melt(central_task[, vars_to_stack_task], id.vars = &quot;ids&quot;) head(task_long) ## ids variable value ## 1 1 degree 1 ## 2 2 degree 1 ## 3 3 degree 0 ## 4 4 degree 1 ## 5 5 degree 1 ## 6 6 degree 1 Now, let's also add a variable in both the social and task data frames that indicate network type. task_long$net &lt;- &quot;task&quot; social_long$net &lt;- &quot;social&quot; Now we will put together the two long format data frames: social_task_long &lt;- rbind(social_long, task_long) And let's make sure the labels are the same between the social and task rows. Let's change incloseness2 to closeness2 and indegree to degree, so the social labels match the task labels. library(car) social_task_long$variable &lt;- recode(social_task_long$variable, as.factor = T, &quot;&#39;incloseness2&#39; = &#39;closeness2&#39;; &#39;indegree&#39; = &#39;degree&#39;&quot;, levels = c(&quot;degree&quot;, &quot;closeness2&quot;, &quot;between&quot;, &quot;eigen&quot;)) Now, let's produce the same barplot as before, but include the task centrality score along side the social centrality scores. This is accomplished by adding a fill argument (in the aes() function) set to the net variable, and by setting the colors using a scale_fill_discrete() function (to distinguish social from task). ggplot(social_task_long, aes(x = factor(ids, levels = 1:length(ids)), y = value, fill = net)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + facet_wrap(~ variable, scales = &quot;free_y&quot;, ncol = 2)+ scale_fill_discrete(name = &quot;Network&quot;, breaks = c(&quot;social&quot;, &quot;task&quot;), labels = c(&quot;Social&quot;, &quot;Task&quot;)) + xlab(&quot;Ids&quot;) + ylab(&quot;Centrality&quot;) + theme(axis.text = element_text(size = 6.5)) We can see node 22 has extremely high centrality in the task network over all measures of centrality. Node 22 also tends to be high centrality in the social interaction network, but there are other nodes (like 16, 18 or 19) with similar centrality scores (except for betweenness). Now, let's calculate the correlation between task and social over all our measures. cor_tab2 &lt;- cor(central_social[, -c(1, 2)], central_task[, -c(1, 2)]) Let’s add some useful row and column names to our correlation table. We will use a paste command to tack \"_social\" onto the existing row names. rownames(cor_tab2) &lt;- paste(rownames(cor_tab2), &quot;social&quot;, sep = &quot;_&quot;) And here we add \"_task\" to the end of the column names. colnames(cor_tab2) &lt;- paste(colnames(cor_tab2), &quot;task&quot;, sep = &quot;_&quot;) And let's look at the columns for eigen_task, degree_task, and closeness2_task. round(cor_tab2[, c(&quot;eigen_task&quot;, &quot;degree_task&quot;, &quot;closeness2_task&quot;)], 3) ## eigen_task degree_task closeness2_task ## indegree_social 0.650 0.558 0.494 ## outdegree_social 0.734 0.700 0.536 ## incloseness2_social 0.553 0.394 0.554 ## outcloseness2_social 0.570 0.460 0.507 ## between_social 0.681 0.803 0.412 ## eigen_social 0.657 0.549 0.481 eigen_task is correlated with betweenness_social (rho = .681) and outdegree_social (rho = .734), possibly because those who are important in tasks also serve as bridges for talk on social issues and have many outbound ties. degree_task and between_social (rho = .803) are correlated, possibly because the number of outdegree ties a node has with respect to task talk, the more they serve as a bridge on social talk. closeness_task2 and incloseness_social2 (rho = .554) are correlated, meaning that those who serve in shortest paths are equivalent for both social talk and task talk. 9.3 Centralization We have so far seen which nodes are most important in the classroom using different definitions of centrality. We have also seen how this differs across social and task interactions. To flesh out the story more clearly, it will be useful to formally summarize the distribution of the centrality measures, telling us something about the network as a whole. For example, a network with one node capturing the vast majority of ties is highly centralized, or highly unequal, as all of the activity in the network is focused on a particular node. A highly centralized network is also relatively fragile, as removing the one central node would greatly reduce the connectivity of the network. We could examine the distribution of any of the centrality scores we calculated above. Here, let's focus on degree as a way of exploring the level of centralization in the two networks. Let's start with a summary of the degree distributions (indegree for social and degree for task). summary(indegree_social) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 1.000 2.500 2.591 3.750 7.000 summary(degree_task) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 1.000 1.000 2.182 2.000 17.000 sd(indegree_social) ## [1] 2.03912 sd(degree_task) ## [1] 3.459099 We can see that the mean and median of degree is higher in the social interaction network than in the task network. In contrast, the maximum value, as well as the standard deviation is much higher in the task network. This confirms our story from above, where the task network is centered strongly on one node, while the social interaction network is based more on groups, where a single node won't necessarily dominate. Note that a simple standard deviation score can serve as an effective measure of centralization. It is also possible to employ traditional centralization measures. To calculate centralization, we take the centrality scores of interest and sum up the total deviations from the highest value. We then typically divide the total summation by the maximum possible level of centralization in a network of that size (i.e., the centralization we would have observed in a hub and spoke structure). igraph has different centralization functions for each centrality score. For degree the function is cent_degree(). The arguments are graph (network of interest), mode (in, out, total) loops (T/F should self-loops be considered), and normalized (T/F should divide by theoretical max?) Here we calculate indegree centralization for the social interaction network, ignoring self loops and dividing by the theoretical max. cent_social &lt;- centr_degree(graph = s641_social, mode = &quot;in&quot;, loops = FALSE, normalized = TRUE) cent_social ## $res ## [1] 3 1 0 1 3 3 1 1 1 2 2 3 0 0 1 5 4 7 5 3 5 6 ## ## $centralization ## [1] 0.2199546 ## ## $theoretical_max ## [1] 441 We could also calculate this directly by doing: sum(max(indegree_social) - indegree_social) / sum(21 - rep(0, 21)) ## [1] 0.2199546 The code simply takes the max centrality score and subtracts the centrality of each node in the network, summing over all nodes. We then divide by the theoretical max, the centralization score if one node received nominations from everyone (indegree = 21 in this case) and everyone else received none (indegree = 0). And now we do the same thing for the task network. cent_task &lt;- centr_degree(graph = s641_task, loops = FALSE, normalized = TRUE) cent_task ## $res ## [1] 1 1 0 1 1 1 1 1 1 1 1 0 2 1 1 2 3 4 3 2 3 17 ## ## $centralization ## [1] 0.7761905 ## ## $theoretical_max ## [1] 420 Note that the theoretical max is a little different as we treated the task network as undirected. Clearly, the task network is considerably more centralized. In fact, the task network almost approaches maximum centralization, or a perfect hub and spoke structure. Now, let's do a simple plot of the two degree distributions. We will put degree on the x-axis and plot a smoothed density curve for each distribution. First, we need to get the density curves for each network, starting with the social interaction network (we set from to 0 in the density() function as indegree cannot be less than 0). den_social &lt;- density(indegree_social, from = 0) And now for the task network: den_task &lt;- density(degree_task, from = 0) And now we set up the plot, plot the two lines and add a legend. plot(range(den_social$x, den_task$x), range(den_social$y, den_task$y), type = &quot;n&quot;, xlab = &quot;degree&quot;, ylab = &quot;density&quot;, main = &quot;Degree Distribution for Social and Task Networks&quot;) lines(den_social, col = &quot;red&quot; , lty = 2, lwd = 2) lines(den_task, col = &quot;light blue&quot;, lty = 2, lwd = 2) legend(&quot;topright&quot;, c(&quot;Social&quot;, &quot;Task&quot;), col = c(&quot;red&quot;, &quot;light blue&quot;), lty = 2, lwd = 2) Here we see that for the task network most people have one or two ties and one person has a very high degree. The social interaction network has a much more even distribution, with many people close to the mean. The story is clear that the task network is highly centralized, with one node being the focal point of all of the task interactions. Social interactions are much evenly dispersed, occurring within groups of people but not centered on a single well-connected node. More generally, our centrality and centralization analyses paint a picture of two different kinds of interactional tendencies. For the social interaction network, we have a set of divided groups bridged by one focal node with high betweenness. Within each group there are prominent nodes with high degree, closeness, etc. but only one node holds the whole network together. For the task network, there is only one focal node, with everyone doing task interactions with them and few interactions happening otherwise. 9.4 Clustering and Hierarchy We have so far used centrality and centralization to explore the classroom networks. Centrality is focused on individual positions in the network and can tell us who holds important positions and who does not. Centralization helps us understand how unequally distributed centrality is in the network. Neither measure (centrality nor centralization) can tell us much about hierarchy at the group level. We may, however, want to know if the groups that exist in our classroom are themselves hierarchically arranged. To explore hierarchy at the group-level, it will be useful to consider other kinds of measures. Here, we will use the tau statistic. The tau statistic captures how micro processes aggregate to create different macro structures. The basic idea is to create hypotheses in the form of different triad counts (the micro processes), that should yield different features at the macro-level. Thus, different micro hypotheses (about which triads should be in the network at high/low rates) correspond to different kinds of emergent features at the macro level. By comparing the observed triad counts to that expected under a null hypothesis, we can see what kinds of hierarchical arrangements exist in the network of interest. Formally, we compare the observed triad counts to the expectations under a null hypothesis of a random network with the same dyad census. This is analogous to the kinds of tests we explored in Chapter 7, but the tau statistic is presented as a z-score (how many standard deviations from the null hypothesis is the observed value), making it akin to more traditional statistical tests. Let's first transform our network from an igraph object into a network object using the intergraph package (as the function we need assumes a network object). library(intergraph) For this analysis we will focus on the social interaction network. s641_social_network &lt;- asNetwork(s641_social) Here we read in a function to calculate the tau statistic. source(file = &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/R/tau_functions.R&quot;) Now we load the ergm and sna packages which is used by the tau function read in above. library(ergm) library(sna) Let's test different hypotheses about the distribution of triads in the network, telling us something about the macro structure in terms of hierarchy. We will consider a ranked clustering hypothesis, a clustering hypothesis and a balance hypothesis. Each hypothesis is represented by a vector, indicating which triads should be summed up and compared to our baseline expectations. The triad types are: 003 A, B, C, empty triad. 012 A-&gt;B, C 102 A&lt;-&gt;B, C 021D A&lt;-B-&gt;C 021U A-&gt;B&lt;-C 021C A-&gt;B-&gt;C 111D A&lt;-&gt;B&lt;-C 111U A&lt;-&gt;B-&gt;C 030T A-&gt;B&lt;-C, A-&gt;C 030C A&lt;-B&lt;-C, A-&gt;C. 201 A&lt;-&gt;B&lt;-&gt;C. 120D A&lt;-B-&gt;C, A&lt;-&gt;C. 120U A-&gt;B&lt;-C, A&lt;-&gt;C. 120C A-&gt;B-&gt;C, A&lt;-&gt;C. 210 A-&gt;B&lt;-&gt;C, A&lt;-&gt;C. 300 A&lt;-&gt;B&lt;-&gt;C, A&lt;-&gt;C, completely connected. A ranked clustering hypothesis poses that 003, 102, 021D, 021U, 030T, 120D, 120U and 300 should be present in the network at higher rates than we what we expect based on dyadic processes alone. The idea is that a network with these triads will tend to create macro structures that correspond to ranked clustering, where there are mutual ties within groups and asymmetric ties across groups; where the lower status groups send ties to higher status groups but not vice versa. Let's create a vector that corresponds to the ranked clustering hypothesis, putting a 1 in each spot of the triad census (following the order above) that corresponds to a triad in that hypothesis. weights_rankedcluster &lt;- c(1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1) A clustering hypothesis poses that 003, 102, and 300 should be present in the network at higher rates than we what we expect based on dyadic processes alone. The clear difference with the ranked clustering hypothesis is that triads that create hierarchies (021U, 120U, etc.) are not included here. The macro network structure implied by a triad census fitting the clustering model is one with a number of social groups, with mutual ties within groups and few ties between groups. Thus, there are a number of groups differentiated by high internal rates of interaction (see Chapter 8) but there is no clear hierarchy between the groups. weights_cluster &lt;- c(1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1) A balance hypothesis is the simplest hypothesis and only includes 102 and 300. This is very similar to the clustering hypothesis but differs in the exclusion of the null triad, 003. The key macro structural difference is that the clustering hypothesis implies a number of social groups to emerge (with no hierarchy) while the balance hypothesis implies that only two groups should emerge, with mutual ties within the groups and few ties between. weights_balance &lt;- c(0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1) The function is tau_stat_function(). The arguments are network and weight.vector (the vector of weights). For ranked clustering: tau_rankedcluster &lt;- tau_stat_function(network = s641_social_network, weight.vector = weights_rankedcluster) tau_rankedcluster ## $tau ## [,1] ## [1,] 2.968397 ## ## [[2]] ## observed.triads expected.triads weight.vector ## triadcensus.003 1029 1.012569e+03 1 ## triadcensus.012 37 4.579457e+01 0 ## triadcensus.102 403 4.121511e+02 1 ## triadcensus.021D 1 1.144864e-01 1 ## triadcensus.021U 0 1.144864e-01 1 ## triadcensus.021C 0 2.289728e-01 0 ## triadcensus.111D 4 6.182267e+00 0 ## triadcensus.111U 10 6.182267e+00 0 ## triadcensus.030T 0 5.695842e-04 1 ## triadcensus.030C 0 1.898614e-04 0 ## triadcensus.201 38 5.357965e+01 0 ## triadcensus.120D 0 1.537877e-02 1 ## triadcensus.120U 0 1.537877e-02 1 ## triadcensus.120C 0 3.075755e-02 0 ## triadcensus.210 7 7.996962e-01 0 ## triadcensus.300 11 2.221378e+00 1 The output is a list, with the first element the tau statistic and the second a data frame with the observed and expected triads, as well as the weighting vector. Now for the clustering hypothesis: tau_cluster &lt;- tau_stat_function(network = s641_social_network, weight.vector = weights_cluster) tau_cluster ## $tau ## [,1] ## [1,] 2.867246 ## ## [[2]] ## observed.triads expected.triads weight.vector ## triadcensus.003 1029 1.012569e+03 1 ## triadcensus.012 37 4.579457e+01 0 ## triadcensus.102 403 4.121511e+02 1 ## triadcensus.021D 1 1.144864e-01 0 ## triadcensus.021U 0 1.144864e-01 0 ## triadcensus.021C 0 2.289728e-01 0 ## triadcensus.111D 4 6.182267e+00 0 ## triadcensus.111U 10 6.182267e+00 0 ## triadcensus.030T 0 5.695842e-04 0 ## triadcensus.030C 0 1.898614e-04 0 ## triadcensus.201 38 5.357965e+01 0 ## triadcensus.120D 0 1.537877e-02 0 ## triadcensus.120U 0 1.537877e-02 0 ## triadcensus.120C 0 3.075755e-02 0 ## triadcensus.210 7 7.996962e-01 0 ## triadcensus.300 11 2.221378e+00 1 Now for the balance hypothesis: tau_balance &lt;- tau_stat_function(network = s641_social_network, weight.vector = weights_balance) tau_balance ## $tau ## [,1] ## [1,] -0.03377649 ## ## [[2]] ## observed.triads expected.triads weight.vector ## triadcensus.003 1029 1.012569e+03 0 ## triadcensus.012 37 4.579457e+01 0 ## triadcensus.102 403 4.121511e+02 1 ## triadcensus.021D 1 1.144864e-01 0 ## triadcensus.021U 0 1.144864e-01 0 ## triadcensus.021C 0 2.289728e-01 0 ## triadcensus.111D 4 6.182267e+00 0 ## triadcensus.111U 10 6.182267e+00 0 ## triadcensus.030T 0 5.695842e-04 0 ## triadcensus.030C 0 1.898614e-04 0 ## triadcensus.201 38 5.357965e+01 0 ## triadcensus.120D 0 1.537877e-02 0 ## triadcensus.120U 0 1.537877e-02 0 ## triadcensus.120C 0 3.075755e-02 0 ## triadcensus.210 7 7.996962e-01 0 ## triadcensus.300 11 2.221378e+00 1 In general, larger values offer support for the hypothesis in question. We can see here that there is little support for the balance hypothesis compared to the other hypotheses. This suggests that the balance hypothesis is too simple. More specifically, it looks like there are many more null triads (003) than we would expect in a network where everyone falls into two groups (under the balance hypothesis). The tau statistics are similar between the ranked clustering and clustering hypotheses. A value of 2.968 (for ranked clustering) suggests that the observed (summed) counts are about 3 standard deviations away from what we expect under the null. Values over 2 offer support for the hypothesis in question under traditional hypothesis testing criteria. Let's take a closer look at the results for the ranked clustering model. We will focus on the expected and observed triad counts, particularly those triads that are in the ranked clustering model but not the clustering model (021D, 021U, 030T, 120D, 120U). The idea is to grab the data frame from the ranked cluster results, only keeping those rows for certain triads. triad_names &lt;- c(&quot;triadcensus.021D&quot;, &quot;triadcensus.021U&quot;, &quot;triadcensus.030T&quot;, &quot;triadcensus.120D&quot;,&quot;triadcensus.120U&quot;) tau_rankedcluster[[2]][rownames(tau_rankedcluster[[2]]) %in% triad_names, ] ## observed.triads expected.triads weight.vector ## triadcensus.021D 1 0.1144864249 1 ## triadcensus.021U 0 0.1144864249 1 ## triadcensus.030T 0 0.0005695842 1 ## triadcensus.120D 0 0.0153787735 1 ## triadcensus.120U 0 0.0153787735 1 In every case but 021D, the observed counts are basically the same as that expected by the null model. In fact, we see 0 observed triads for 021U, 030T, 120D and 120U. This would suggest that the ranked clustering model really isn't offering much over the clustering model. The ranked clustering model offers similar fit to the clustering model but is more complicated, adding 5 triads that do not seem to deviate much from chance expectations. We may then have good reason to interpret the network in terms of the clustering model, where there are multiple groups but few asymmetries. Overall, the analysis shows that the social interaction network is best characterized as a network with multiple groups without a clear hierarchical arrangement. Given the very high levels of reciprocity in social interactions, asymmetries are rare and do not consistently emerge between groups. The tau statistic reinforces our story of the social interaction network consisting of distinct social groups with one bridge and no clear hierarchy. Compare this to the task network, which has a clear hub and spoke structure, but no emergent groups. We end the tutorial by noting that centrality and hierarchy will come up again in a number of tutorials; for example, in Chapter 11 (two-mode), Chapter 13 (statistical network models), Chapter 14 (diffusion) and Chapter 15 (social influence). "],["ch10-Positions-Roles-R.html", "10 Positions and Roles 10.1 Classroom Network Data 10.2 Getting the Data Ready 10.3 Structural Equivalence 10.4 Defining Positions 10.5 Role Analysis 10.6 Local Equivalence", " 10 Positions and Roles This tutorial offers an analysis in R focused on defining positions and roles in a network. The goal of a role analysis is to: a) place nodes into positions based on their pattern of ties to others; and b) describe the pattern of ties that exist between positions, or the roles, that emerge in the network of interest. We will walk through a simple analysis step-by-step, to see the logic of how to do a role analysis. See also the blockmodeling package for an extended set of functions to perform different types of blockmodels. We will utilize the same network used in Chapter 8 (on groups) for this tutorial. The data are based on different interactions occurring between students (and teachers) in a classroom. The data were collected by Daniel McFarland and contain information on three different kinds of relations: friendship (measured as 2 = best friend, 1 = friend, 0 = not friend); social (measured as social interactions per hour); and task (measured as task interactions per hour). Our main substantive goal is to describe the role structure that exists in this classroom. How do different sets of nodes relate to one another within the larger functioning of the classroom? What are the rights and responsibilities of one set relative to another? As this is a classroom, the rights and responsibilities pertain to expectations of social interaction (do nodes in one set have positive social interactions with nodes in another?) as well as task interactions. For example, is this is a system where students tend to break off into separate friendship groups that also map onto task interactions? Or perhaps there are certain alliances, where certain friendship groups come together to complete work in the classroom. Or maybe there is a single, important node (like the teacher) that is crucial in holding the classroom together. 10.1 Classroom Network Data For this session we will start with the igraph package. library(igraph) Let's first read in the classroom network data. url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class182_networkdata.csv&quot; class182_networkdata &lt;- read.csv(file = url1) And let's take a look at the first six rows: head(class182_networkdata) ## ego alter friend_tie social_tie task_tie ## 1 1 1 0 0.0 0.0 ## 2 1 2 0 0.0 0.0 ## 3 1 3 0 0.0 0.0 ## 4 1 4 0 0.0 0.0 ## 5 1 5 0 1.2 0.3 ## 6 1 6 0 0.0 0.0 The data frame holds information for each dyad in the classroom showing information for three relations: friendship, social interactions and task interactions. Let's also read in an attribute file to attach to our network. url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class182_attributedata.csv&quot; class182_attributes &lt;- read.csv(file = url2) head(class182_attributes) ## ids race grade gender ## 1 1 white 10 male ## 2 2 black 10 female ## 3 3 white 10 female ## 4 4 white 11 female ## 5 5 white 10 male ## 6 6 white 10 female We have information on race, grade and gender. Note that grade '13' is coded to denote the teacher in this classroom. Let's go ahead and create separate networks based on each relation. We start with friendship, defining a (weighted) edge where the friendship value is greater than 0. edgelist_friendship &lt;- class182_networkdata[class182_networkdata$friend_tie &gt; 0, c(&quot;ego&quot;, &quot;alter&quot;, &quot;friend_tie&quot;)] head(edgelist_friendship) ## ego alter friend_tie ## 17 2 1 1 ## 23 2 7 1 ## 24 2 8 1 ## 29 2 13 2 ## 30 2 14 1 ## 37 3 5 1 And here we create the friendship network. We create a directed network as it possible for i to nominate j as a friend even if j did not nominate i. net182_friend &lt;- graph_from_data_frame(d = edgelist_friendship, directed = T, vertices = class182_attributes) net182_friend ## IGRAPH c4d0331 DN-- 16 62 -- ## + attr: name (v/c), race (v/c), grade (v/n), gender (v/c), friend_tie (e/n) ## + edges from c4d0331 (vertex names): ## [1] 2 -&gt;1 2 -&gt;7 2 -&gt;8 2 -&gt;13 2 -&gt;14 3 -&gt;5 3 -&gt;6 3 -&gt;11 3 -&gt;14 3 -&gt;15 5 -&gt;1 5 -&gt;3 5 -&gt;6 5 -&gt;8 5 -&gt;10 5 -&gt;11 6 -&gt;1 6 -&gt;3 6 -&gt;5 6 -&gt;7 6 -&gt;10 6 -&gt;11 6 -&gt;12 7 -&gt;2 7 -&gt;8 7 -&gt;13 7 -&gt;14 8 -&gt;2 8 -&gt;5 8 -&gt;7 8 -&gt;13 8 -&gt;14 8 -&gt;15 9 -&gt;1 9 -&gt;3 9 -&gt;10 9 -&gt;12 9 -&gt;15 10-&gt;1 10-&gt;9 10-&gt;12 10-&gt;15 11-&gt;1 11-&gt;3 11-&gt;5 11-&gt;6 11-&gt;10 12-&gt;1 12-&gt;9 12-&gt;15 13-&gt;2 13-&gt;7 13-&gt;8 13-&gt;14 14-&gt;2 14-&gt;3 14-&gt;8 14-&gt;12 15-&gt;1 15-&gt;7 15-&gt;9 15-&gt;12 We can see that the friendship network has been created, with friend_tie defining the edge weights. And now we create the task interaction network. We create the edgelist from dyads where the task interaction value is &gt; 0. edgelist_task &lt;- class182_networkdata[class182_networkdata$task_tie &gt; 0, c(&quot;ego&quot;, &quot;alter&quot;, &quot;task_tie&quot;)] We will treat the task and social interaction networks as undirected. In our case, if i does a task with j, j is jointly doing the task with i (also true of social interaction in this classroom). Here, we will first create a directed network, and then convert that object to an undirected network, setting mode to \"collapse\" and edge.attr.comb to \"mean\". With the \"collapse\" option, if i-&gt;j exists or j-&gt;i exists then the i-j edge will be present in the network, which is what we want. The edge.attr.comb argument tells igraph how to combine edge attributes when collapsing i-&gt;j and j-&gt;i into a single edge. We will use the mean reported task interactions in this case (i.e., take the weight for i-&gt;j and j-&gt;i and calculate the mean, setting that as the weight for i-j). net182_task &lt;- graph_from_data_frame(d = edgelist_task, directed = T, vertices = class182_attributes) net182_task &lt;- as.undirected(net182_task, mode=&quot;collapse&quot;, edge.attr.comb = &quot;mean&quot;) net182_task ## IGRAPH c3d39f3 UN-- 16 47 -- ## + attr: name (v/c), race (v/c), grade (v/n), gender (v/c), task_tie (e/n) ## + edges from c3d39f3 (vertex names): ## [1] 1 --5 3 --5 3 --6 5 --6 2 --7 2 --8 7 --8 1 --9 1 --10 4 --10 6 --10 9 --10 3 --11 5 --11 6 --11 1 --12 9 --12 10--12 2 --13 5 --13 7 --13 8 --13 2 --14 5 --14 6 --14 7 --14 8 --14 1 --15 2 --15 9 --15 10--15 12--15 1 --16 2 --16 3 --16 4 --16 5 --16 6 --16 7 --16 8 --16 9 --16 10--16 11--16 12--16 13--16 14--16 15--16 And now we do the same thing for the social interaction network. edgelist_social &lt;- class182_networkdata[class182_networkdata$social_tie &gt; 0, c(&quot;ego&quot;, &quot;alter&quot;, &quot;social_tie&quot;)] net182_social &lt;- graph_from_data_frame(d = edgelist_social, directed = T, vertices = class182_attributes) net182_social &lt;- as.undirected(net182_social, mode=&quot;collapse&quot;, edge.attr.comb = &quot;mean&quot;) 10.2 Getting the Data Ready Here we will use the classroom data read in above. We incorporate all three relations into the analysis. Role analysis is built to incorporate multiple relations and we make use of the friendship, task and social interaction data. The first step in doing a role/position analysis is to place nodes into equivalent positions. Here, we define equivalence based on the idea of structural equivalence. With structural equivalence, nodes who are tied to similar nodes are placed in the same position. In order to do that we must know how similar/dissimilar each node is compared to the other nodes. Formally, this means calculating the distance between rows (where the rows show who i is friends with, does tasks with etc.) Nodes who are friends with the same people (even if they are not themselves friends), do tasks with the same people, etc. should be in the same position. One complication in doing this kind of analysis is that we want to make use of multiple relations (here friendship, social and task). We need to put together nodes with the same interaction patterns across all relations at once. The relations of interest must then be taken together as a single input. Given this requirement, let's first get the matrices for each relation and put them together in one matrix. Here we get the matrices for each relation: friendship, task and social. friend_mat &lt;- as_adjacency_matrix(net182_friend, attr = &quot;friend_tie&quot;, sparse = F) task_mat &lt;- as_adjacency_matrix(net182_task, attr = &quot;task_tie&quot;, sparse = F) social_mat &lt;- as_adjacency_matrix(net182_social, attr = &quot;social_tie&quot;, sparse = F) Note that for each one the input network changes. We include an attr argument to get the values on each edge. Let's look at the friendship matrix. friend_mat ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 2 1 0 0 0 0 0 1 1 0 0 0 0 2 1 0 0 ## 3 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 ## 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 5 1 0 1 0 0 1 0 1 0 1 2 0 0 0 0 0 ## 6 1 0 2 0 1 0 1 0 0 1 1 1 0 0 0 0 ## 7 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 ## 8 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 0 ## 9 2 0 1 0 0 0 0 0 0 2 0 2 0 0 2 0 ## 10 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 ## 11 1 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 ## 12 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 ## 13 0 2 0 0 0 0 1 1 0 0 0 0 0 1 0 0 ## 14 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 ## 15 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 ## 16 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 The values correspond to the strength of relationship, friend or best friend, from i to j. We also need to create a matrix showing the ties coming in to each node, from j to i. This is simply the transpose of friend_mat. friend_mat_in &lt;- t(friend_mat) The task and social matrices are also valued, showing the task/social interactions per hour. We do not need to transpose them as the relations are treated as undirected. Now, let's stack our matrices together. Here we will combine them column-wise, using a cbind() function. Before we stack our matrices, let's standardize our columns so they are comparable. This entails dividing each column by the standard deviation associated with that type of relation. We will also mean-center the columns. Thus, for all friendship columns we will subtract the mean of the friend_mat matrix and divide by the standard deviation of friend_mat. friend_mat_std &lt;- (friend_mat - mean(friend_mat)) / sd(friend_mat) friend_mat_in_std &lt;- t(friend_mat_std) # for the incoming friendship ties task_mat_std &lt;- (task_mat - mean(task_mat)) / sd(task_mat) social_mat_std &lt;- (social_mat - mean(social_mat)) / sd(social_mat) And now we can combine all of the standardized matrices into one matrix. friends_task_social_std &lt;- cbind(friend_mat_std, friend_mat_in_std, task_mat_std, social_mat_std) dim(friends_task_social_std) ## [1] 16 64 This matrix contains the friend ties going out from i, the friend ties going in to i, the task ties and the social ties. Note that we included the transpose of the friendship matrix only because the friendship relation was asymmetric. We could also do the same thing using the non-standardized matrices: friends_task_social &lt;- cbind(friend_mat, friend_mat_in, task_mat, social_mat) 10.3 Structural Equivalence Now we have a single n x 4n matrix that represents friendship, social and task interactions. From this, we can generate an n x n distance matrix that shows the degree of structural equivalence of each node in the network. The calculation shows how different/similar each row, or node, is to every other row. Nodes with similar patterns of network ties (along the 4 types of ties) will have low distance between them. The function is dist(). The arguments are: x = data to calculate distance on; method = method to calculate distance between rows. There are a number of options to calculate the distance between rows, including euclidean distance, defined as: sqrt(sum((x_i - y_i)^2)). We use the euclidean option here. euclid_dist &lt;- dist(x = friends_task_social_std, method = &quot;euclidean&quot;) euclid_dist ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## 2 12.277092 ## 3 9.337925 12.666789 ## 4 8.125464 10.447252 8.035579 ## 5 11.104733 12.728916 9.647258 9.121137 ## 6 10.678168 12.565643 9.919998 8.088631 8.720639 ## 7 8.890135 7.850717 9.402618 6.784490 10.162484 10.479959 ## 8 10.349860 7.464200 9.522193 8.039333 12.013896 10.297088 6.933562 ## 9 11.124331 13.784948 12.511439 10.298732 12.110515 10.470634 11.854747 12.346245 ## 10 7.052583 11.970673 8.885192 8.234908 10.597375 10.001802 9.733430 10.388199 9.711395 ## 11 10.406593 13.215125 9.147026 8.618923 10.459737 8.211965 10.579496 10.744884 11.878246 10.115245 ## 12 6.916288 12.388970 10.242419 9.154222 11.949265 11.860957 10.175290 11.115321 11.186167 7.327656 11.847435 ## 13 11.915949 11.955712 12.930734 10.482529 13.132799 12.897925 8.143778 9.159214 14.384530 12.409627 13.472127 13.132353 ## 14 9.548177 8.919780 10.645521 7.001845 9.814018 9.120023 6.009304 7.686441 11.092270 9.562528 10.321041 10.841942 7.631703 ## 15 7.685769 11.528159 11.324092 8.700158 11.173555 10.468069 10.074406 11.100016 9.998283 6.135059 11.502730 7.631501 12.001112 9.012054 ## 16 14.160821 15.140840 14.241848 11.869697 14.696566 14.161035 13.150494 13.698101 14.748241 14.138346 14.396936 14.307963 15.152258 13.497085 14.495992 This suggests, for example, that node 1 has similar ties as node 12 and really different ties as node 13. It is also possible to use correlation as a means of defining the similarity/dissimilarity between rows. Now, let's visualize the distances using MDS, based on the euclidean distance calculation. The cmdscale() function performs classical multidimensional scaling, putting each case into a position in a space, such that the distance between positions is (approximately) equal to the distance between those rows in the data. The basic arguments are d, the distance matrix and k, the number of dimensions. Here we want a two-dimensional solution to make it easier to visualize. fit &lt;- cmdscale(d = euclid_dist, k = 2) fit ## [,1] [,2] ## 1 -2.5431871 2.33755953 ## 2 5.7005695 1.87699422 ## 3 -1.8507975 -0.50197997 ## 4 -0.0366485 -1.02843116 ## 5 -1.8352173 -1.13971634 ## 6 -2.1619387 -1.56660835 ## 7 3.8528068 0.78880650 ## 8 4.4621270 0.64166041 ## 9 -4.2143640 0.06072452 ## 10 -3.5627373 2.25952299 ## 11 -2.4962320 -1.73655609 ## 12 -3.2688649 2.64899489 ## 13 6.4090625 1.56371687 ## 14 3.2557391 0.89872185 ## 15 -2.6232119 3.09255591 ## 16 0.9128942 -10.19596578 Let’s grab the x, y coordinates and then plot the solution. x &lt;- fit[, 1] y &lt;- fit[, 2] First, we will set up the plot with title, axes, etc. but no points. And then we will put in the labels using the text() function. The first two arguments to text are the x, y coordinates. The labels argument tells R what to print and cex controls the size of the labels. plot(x, y, xlab = &quot;Coordinate 1&quot;, ylab = &quot;Coordinate 2&quot;, main = &quot;2 Dimensional MDS Solution&quot;, type = &quot;n&quot;) text(x, y, labels = 1:length(x), cex = 1) Looking at the figure, our first impression is that there are clearly defined positions in the network. We can see, for example, that node 16 is quite distinct from everyone else, while nodes 1, 10, 12 and 15 cluster together, meaning they have similar patterns of friendship, social and task interactions. We will, however, want to use a more formal analysis to make an informed decision on how many positions gives us a reasonable approximation of the underlying structure of the network. 10.4 Defining Positions Here, we cluster our nodes into structurally equivalent positions. We will take our distance matrix and try to cluster our nodes so that nodes who are close, or have the same pattern of ties, are placed into the same position. Note that this is conceptually different than finding groups (or communities, see Chapter 8). Nodes are placed into positions based on having the same pattern of ties to other nodes. It is thus possible for nodes to be placed together in a position even if they have few ties to each other, as long as they have the same set of ties to all other nodes. Thus, it is possible to have a position with low internal density. There are many different approaches to clustering data. We will focus on a simple approach, hierarchical clustering, but discuss other options below. Note that these approaches are general solutions to the problem of clustering data and are not particular to network data. The hclust() function performs a hierarchical agglomerative operation based on the values in the input distance matrix. Each node starts in their own cluster; at each stage the two closest clusters are joined until all nodes are in a single cluster. The standard visualization is a dendrogram. By default, hclust() agglomerates clusters via a \"complete linkage\" algorithm, determining cluster proximity by looking at the distance of the two points across clusters that are farthest away from one another. The input to hclust() is the distance matrix, calculated above. hc &lt;- hclust(euclid_dist) Let's plot the resulting dendrogram. plot(hc) The y-axis captures the distance (based on euclidean distance) between actors on that part of the tree (e.g., the maximum distance between 10, 15, 1 and 12 is a bit below 8). Let's visualize different possible clusters. We will use a rect.hclust() function to tell R at what point to cut the tree, or to stop aggregating. The inputs are an hclust object and the height at which you want to cut the tree (set using h). Here we will set h to 12, looking at clusters with distance less than 12. plot(hc) plot_clusters &lt;- rect.hclust(hc, h = 12) And now let's look at a more disaggregated solution, setting h to 8. plot(hc) plot_clusters &lt;- rect.hclust(hc, h = 8) We can see that the number of clusters increases as we lower the distance threshold. More generally, it is useful to be able to set a clear distance cutoff, especially when the analysis involves multiple networks; as we can define clusters (in different networks) at similar levels of granularity. Let’s extract the position memberships for height equal to 12. The function is cuttree() and the inputs are the hclust object and the height (or distance) at which you want to define the clusters. hc_ids &lt;- cutree(hc, h = 12) hc_ids ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## 1 2 3 2 3 3 2 2 1 1 3 1 2 2 1 4 This vector holds the position of each node in the network based on the cluster solution. There are 4 positions defined at this level of aggregation. For example, we can see that nodes 1, 9, 10, 12 and 15 are grouped together. A hierarchical clustering approach makes it easy to look at the positions that emerge at different levels of aggregation, from very broadly defined positions (with more heterogeneity within) to more narrowly defined sets where the cases are very similar. For example, we could have set h to a lower value, capturing more narrowly defined positions. It is also possible to use various criteria to determine the 'optimal' number of clusters of a hierarchical algorithm. This will be particularly crucial when the researcher only wants to examine a single solution. Here we can make use of the NbClust package (Charrad et al. 2014). library(NbClust) The key function is NbClust(). The arguments are data (the data of interest), distance, (the distance metric to use), method (which method to use in the hierarchical algorithm), and index (what criterion to use to choose the optimal number of clusters). Here we use the \"ch\" criterion of Caliński and Harabasz (1974) but there are many options. For example, we could imagine using a graphical approach, similar to that used in Chapter 8, where the fit is plotted against the number of clusters. clusters &lt;- NbClust(data = friends_task_social_std, distance = &quot;euclidean&quot;, method = &quot;complete&quot;, index = c(&quot;ch&quot;)) clusters ## $All.index ## 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## 2.6184 3.3756 3.7565 3.3300 3.4032 3.3052 3.3233 3.4194 3.3500 3.4037 3.3901 3.4203 3.5698 3.4593 ## ## $Best.nc ## Number_clusters Value_Index ## 4.0000 3.7565 ## ## $Best.partition ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## 1 2 3 2 3 3 2 2 1 1 3 1 2 2 1 4 Under this criterion the optimal solution has 4 clusters, matching the clustering solution above where height was set to 12. Note that the inputs, in terms of distance and method, match that used above. Note also that there are many other means of finding clusters, including k-means, Partitioning Around Medoids, and more model-based approaches. These approaches attempt to find the best fit, including the best number of clusters. Different approaches utilize different heuristics and may yield slightly different results. ?kmeans ?pamk ?Mclust. In general, it is a good idea to compare the results across different approaches, including using different criterion to judge model fit. Additionally, a strictly statistical answer to defining positions may not always be desirable, as the researcher, with a deep understanding of the substantive case, may have good reason to examine one set of positions rather than another (for example, when choosing between levels of aggregation). It is important to balance concerns over fit with concerns over interpretation. Assuming we are happy with our clustering solution, we can now use the found positions in the second part of the analysis, focusing on roles. 10.5 Role Analysis The goal of a role analysis is to create a reduced version of the network, where the positions become the nodes of interest, and we analyze the pattern of ties between positions. In this way, we move beyond particular actors and their attributes, and analyze the network at a higher level of abstraction, studying the roles that are being played in the classroom. For example, perhaps a 'teacher' role has certain rights and responsibilities relative to actors playing the 'student' role. 10.5.1 Permuting the Matrix by Position We will begin with a plot of the data, with the goal of offering some intuition for role analysis. We will produce a heatmap, organized by the positions found above. The idea is to take the matrix of friendship, social or task ties and create a plot where the values are colored by whether a tie exists between i and j. The key is reordering the rows/columns of the matrix to correspond to the positions from the clustering analysis. In this way, nodes in the same position will be clumped together in the figure. This makes it possible to see how nodes in different positions relate to one another; moving us closer to capturing roles in the classroom. Let's begin by creating a data frame with two columns, one for the ids of the nodes and one for the positions of the nodes. We will utilize our 4 cluster solution from above to define the positions (saved in hc_ids). We will also sort the data to run from low to high, in terms of the positions. id_dat &lt;- data.frame(ids = class182_attributes$ids, position = hc_ids) id_dat &lt;- id_dat[order(id_dat$position), ] id_dat ## ids position ## 1 1 1 ## 9 9 1 ## 10 10 1 ## 12 12 1 ## 15 15 1 ## 2 2 2 ## 4 4 2 ## 7 7 2 ## 8 8 2 ## 13 13 2 ## 14 14 2 ## 3 3 3 ## 5 5 3 ## 6 6 3 ## 11 11 3 ## 16 16 4 We can now take the matrices used above, for friendship, social or task, and reorder the rows/columns based on the positions of each node. Here, we will focus on the social interaction matrix. We simply take the original matrix and sort based on the (reordered) ids in our id_dat data frame. social_mat_rearrange &lt;- social_mat[id_dat$ids, id_dat$ids] Now, we can create a figure based on the permuted matrix. We will use the heatmap() function, as well as functions from the RColorBrewer package. library(RColorBrewer) column_cols &lt;- c(&quot;red&quot;, &quot;green&quot;, &quot;black&quot;, &quot;blue&quot;) heatmap(social_mat_rearrange, Rowv = NA, Colv = NA, revC = T, col = colorRampPalette(brewer.pal(6, &quot;Blues&quot;))(25), ColSideColors = column_cols[id_dat$position], RowSideColors = column_cols[id_dat$position], symm = T) We can see that the rows and columns are ordered based on position (e.g., 1, 9, 10, 12 and 15 are in the first position). The colors in the heatmap run from light blue to darker blue, with darker values indicating stronger relationships (here more social interactions). This was set in the col argument in the function. Finally, we've also set the row and column colors (red, green, black and blue) to denote the position of each node. Red corresponds to nodes in position 1; green corresponds to nodes in position 2; black corresponds to nodes in position 3; and blue corresponds to nodes in position 4. Substantively, we can see that most social interactions happen within positions. For example, actors in position 1 (red) tend to socially interact with others in position 1. There is, however, a fair amount of social interaction between actors in position 1 (red) and position 4 (blue), as well as between actors in position 2 (green) and position 4 (blue). There is also some social interaction between those in position 1 (red) and position 3 (black), although the rates are much lower than with interactions happening within positions. 10.5.2 Constructing a Blockmodel We are now ready to do a more formal blockmodel analysis. We will utilize the sna package for this part of the tutorial. library(sna) We begin by forming a blockmodel, a matrix where the positions themselves are the nodes, or blocks, of interest. In this way, the previously found positions are the rows/columns of the newly constructed matrix. For example, all actors in the 'red' position (1, 9, 10, 12 and 15 in the figure above) get collapsed into a single node, or block, labeled, \"Block 1\"; all nodes in the green position get collapsed into \"Block 2\", and so on. We will then analyze the resulting blockmodel in terms of the pattern of ties between blocks. Here will use the blockmodel() function in sna. We will do this 3 times, once each for the three relations we have. The arguments are dat (the network of interest) and ec, the equivalence classes, a vector showing what class each node is in. In this case, ec is defined by the position of each node, as defined by our hierarchical clustering results above. blockmod_friend &lt;- blockmodel(friend_mat, ec = hc_ids) The default is to calculate the density of ties between blocks (defined as the mean value, taken over the rows/columns in the input matrix associated with those blocks). In this case the values are weighted but we could do the same exercise with binary (0/1) data. blockmod_friend ## ## Network Blockmodel: ## ## Block membership: ## ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## 1 2 3 2 3 3 2 2 1 1 3 1 2 2 1 4 ## ## Reduced form blockmodel: ## ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## Block 1 Block 2 Block 3 Block 4 ## Block 1 0.9 0.03333333 0.05000000 0 ## Block 2 0.1 0.66666667 0.08333333 0 ## Block 3 0.4 0.12500000 1.16666667 0 ## Block 4 0.0 0.00000000 0.00000000 NaN The blockmodel shows us the (weighted) density of within and between block ties. Here, we can see that block 1 sends most friendship ties to itself, and some to block 2 and 3. We also begin to see the possibility of asymmetries between the blocks; for example, block 3 sends more friendship ties to block 1 than block 1 sends to 3. An NA on the diagonal means there is only one node associated with that block. And now for the task and social interaction networks: blockmod_task &lt;- blockmodel(task_mat, ec = hc_ids) blockmod_social &lt;- blockmodel(social_mat, ec = hc_ids) 10.5.3 Plotting a Blockmodel We now want to interpret these three blockmodels, in terms of the pattern of social, friendship and task ties between blocks. To facilitate interpretation, we will produce a plot, where we have one node for each block, and the edges correspond to the level of social, friendship and task ties between blocks. This is the information found in blockmod_friend, blockmod_social and blockmod_task. Note that the plotted network will have weighted edges and loops (edges from and to the same node). It will also be useful for the plot to have a simpler, uniform weighting scheme across the relations. We will create a weighting scheme based on the following logic: where the edge weight is equal to 0 if the value is less then the mean weight; equal to 1 if it is greater than the mean but less than 1 standard deviation above the mean; and equal to 2 if the value is greater than 1 standard deviation above the mean. Let's write a little function to output the blockmodel as an edgelist, with the recoded edge weights: block_model_edgelist &lt;- function(block_model, relation_label, directed = T){ # Arguments: # block_model: blockmodel object # relation_label: label for type of tie, # directed: is blockmodel based on directed network? # First we grab the actual matrix from the blockmodel: block_mat &lt;- block_model$block.model # Here we set any NA to 0. block_mat[is.na(block_mat)] &lt;- 0 # Now we create a little network based on the matrix. net_block &lt;- network(block_mat, loops = T, directed = directed) # Here we extract the edgelist: edges_netblock &lt;- as.edgelist(net_block) # Now we get the edge weights, stringing out the matrix # into a vector. We only extract those weights corresponding # to where an edge exists, defined by the edgelist # extracted above. weight_edge &lt;- c(block_mat[edges_netblock]) # Now we create a little data frame putting the information together. block_edgelist &lt;- data.frame(edges_netblock, weight = weight_edge, Tie = relation_label) # Here we create the additional weighting scheme, # where weight is equal to 0, 1, or 2, depending if it is # less than the mean, greater than the mean (but less than 1 sd above mean) # or greater than 1 sd above the mean. edge_mean &lt;- mean(block_mat) edge_sd &lt;- sd(block_mat) edge_max &lt;- max(block_mat) block_edgelist$WeightRecode &lt;- cut(block_edgelist$weight, breaks = c(0, edge_mean, edge_mean + edge_sd, edge_max), include.lowest = T, right = F, labels = c(&quot;0&quot;, &quot;1&quot;, &quot;2&quot;)) block_edgelist$WeightRecode &lt;- as.character(block_edgelist$WeightRecode) block_edgelist$WeightRecode &lt;-as.numeric(block_edgelist$WeightRecode) colnames(block_edgelist)[1:2] &lt;- c(&quot;sender&quot;, &quot;receiver&quot;) block_edgelist } And now we create the edgelist for the friendship relation: blockedges_friend &lt;- block_model_edgelist(block_model = blockmod_friend, relation_label = &quot;friendship&quot;, directed = T) blockedges_friend ## sender receiver weight Tie WeightRecode ## 1 1 1 0.90000000 friendship 2 ## 2 1 2 0.03333333 friendship 0 ## 3 1 3 0.05000000 friendship 0 ## 4 2 1 0.10000000 friendship 0 ## 5 2 2 0.66666667 friendship 2 ## 6 2 3 0.08333333 friendship 0 ## 7 3 1 0.40000000 friendship 1 ## 8 3 2 0.12500000 friendship 0 ## 9 3 3 1.16666667 friendship 2 We now have an edgelist where the rows correspond to the edges from block i to block j. We have information on the weight from the raw data (weight), as well as a recoded version (WeightRecode). And now for the task and social blockmodels. blockedges_task &lt;- block_model_edgelist(block_model = blockmod_task, relation_label = &quot;task&quot;, directed = F) blockedges_social &lt;- block_model_edgelist(block_model = blockmod_social, relation_label = &quot;social&quot;, directed = F) And now we put together the three edgelists, stacking them using an rbind() function. block_dat &lt;- rbind(blockedges_friend, blockedges_task, blockedges_social) To simplify the plot, we will only include edges that have weights above the mean, so values of 1 or 2 in our recoded weighting scheme. block_dat &lt;- block_dat[block_dat$WeightRecode %in% c(&quot;1&quot;, &quot;2&quot;), ] block_dat ## sender receiver weight Tie WeightRecode ## 1 1 1 0.9000000 friendship 2 ## 5 2 2 0.6666667 friendship 2 ## 7 3 1 0.4000000 friendship 1 ## 9 3 3 1.1666667 friendship 2 ## 13 1 4 4.7700000 task 2 ## 16 2 4 3.0500000 task 2 ## 19 1 1 4.2225000 social 2 ## 22 1 4 1.2750000 social 1 ## 23 2 2 3.5250000 social 2 ## 25 2 4 1.3500000 social 1 ## 26 3 3 4.5750000 social 2 We will use the igraph package to produce the plot, as igraph naturally handles plots with multiple types of edges. We will first create an igraph object based on our reduced edgelist. Note that this constructed igraph object will be based on a network with multiple relations, friendship, task and social. We will use the first two columns from block_dat (showing the sender and receiver of each edge) as the input to graph_from_edgelist(), which must be in matrix form. We treat the network as directed. blockmod_edges &lt;- as.matrix(block_dat[, c(&quot;sender&quot;, &quot;receiver&quot;)]) blockmod_igraph &lt;- graph_from_edgelist(blockmod_edges, directed = T) It will be useful to add some information about the edges to our igraph object. We will add information about the type of edge, stored as Tie on block_dat. E(blockmod_igraph)$type &lt;- block_dat[, &quot;Tie&quot;] Let's go ahead and decide what color each edge should be. Let's make friendship red, task blue and social green. We will use the recode() function in the car package. library(car) cols &lt;- recode(block_dat[, &quot;Tie&quot;], &quot;&#39;friendship&#39; = &#39;red&#39;; &#39;task&#39;=&#39;blue&#39;; &#39;social&#39; = &#39;green&#39;&quot;) E(blockmod_igraph)$color &lt;- cols And let's also set the width of the edges to be equal to the weights, stored as WeightRecode on block_dat: E(blockmod_igraph)$width &lt;- block_dat[, &quot;WeightRecode&quot;] And now we can go ahead and create a plot. Let's see what the default plot options yield us: plot(blockmod_igraph, margins = .20) It looks like we have the color and edge widths we want but there are some clear problems with the plot. The current plot has two key issues we need to fix. First, the edges for task and social should be undirected, with no arrows. Second, the loops, showing the edges going within blocks, are currently on top of each other, making it difficult to interpret the figure. Let's begin by fixing the undirected edges for task and social. The basic idea is to edit the figure so that the friendship edges are treated as directed (with arrows), while the social and task edges are not. We also do not want the self loops to have arrows. We will thus have directed and undirected edges in the same figure. This can be a bit difficult to accomplish using the default specifications, but is possible if we 'manually' set the arrow mode for each edge (using edge.arrow.mode in the plot command). We determine which edges should be directed/undirected using an ifelse() function. In the syntax below, we first grab the edge type from the igraph object and ask if it is a friendship edge. We then determine if the edge is a self loop or not. We can determine which edges are self loops using an is.loop() function. We will designate those edges that are friendship and not self loops as directed edges: \"&gt;\". For all other edges (self loops or social/task) we will make them undirected edges, with no arrows: \"-\". friend_edge &lt;- E(blockmod_igraph)$type %in% &quot;friendship&quot; self_loop &lt;- is.loop(blockmod_igraph) arrow_mode &lt;- ifelse(friend_edge &amp; !self_loop, yes = &quot;&gt;&quot;, no = &quot;-&quot;) arrow_mode ## [1] &quot;-&quot; &quot;-&quot; &quot;&gt;&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; &quot;-&quot; We also need to tweak the plotting of the loops, so that the loops are not right on top of each other. This can be accomplished by changing the angle in which each loop is plotted (edge.loop.angle in the plot command). To accomplish this, we need to set the angle for every edge in the network, even those edges that are not self loops. We will set the angles such that all non-self loops have an angle of 0, all friendship self loops have an angle of 4 * pi / 3 and all social self loops have an angle of 3 * pi / 3 (there are no task self loops). This will ensure that the friendship and social self loops are not drawn on top of each other. edge_loop_angles &lt;- rep(0, nrow(block_dat)) edge_loop_angles[self_loop &amp; friend_edge] &lt;- 4 * pi / 3 social_edge &lt;- E(blockmod_igraph)$type %in% &quot;social&quot; edge_loop_angles[self_loop &amp; social_edge] &lt;- 3 * pi / 3 We are now ready to plot the block model again. The key is setting edge.arrow.mode to our vector of arrow types set above (arrow_mode) and setting edge.loop.angle to our vector of angle loops (edge_loop_angles). We include a simple legend to make the figure easier to interpret. We also include better labels on the nodes (using vertex.label.dist and vertex.label.degree to set the placement of the labels), as well as a few arguments to control the look of the arrows. plot(blockmod_igraph, edge.arrow.mode = arrow_mode, edge.arrow.size = .65, edge.arrow.width = .75, vertex.label = c(&quot;block 1&quot;, &quot;block 2&quot;, &quot;block 3&quot;, &quot;block 4&quot;), vertex.label.dist = 4.5, vertex.label.degree = 0, edge.loop.angle = edge_loop_angles, margin = .20) legend(&quot;left&quot;, c(&quot;friendship&quot;, &quot;task&quot;, &quot;social&quot;), col = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;), lwd = 1, cex = .8) Looks like we have all the elements we want in the figure. 10.5.4 Interpretation of Blockmodel Now, let's take the plotted blockmodel and interpret the roles that emerge in this network, in terms of the pattern of ties that exist between blocks. The question here is what rules of behavior, or behavioral expectations, map onto each block, and then look at how these cohere into a larger social system. Let's begin by looking at block 4, as the pattern of friendship, social and task ties are so distinct here. Remember that block 4 consists only of node 16. Looking at the blockmodel figure, this node sends no friendship ties, socializes with block 1 and block 2 and has heavy task interactions with block 1 and block 2. They also do quite a bit more tasks than others in the classroom. We can see this by looking at the total level of task interactions for each block: rowSums(blockmod_task$block.model, na.rm = T) ## Block 1 Block 2 Block 3 Block 4 ## 5.33875 3.35375 1.90750 9.02000 In short, this is an individual playing the role of teacher in the classroom, who does tasks with other nodes in the network but will not form friendships with them. In fact, it is the teacher in this case, but we arrived at the behavioral role without knowing that, and it was possible that another actor could have played that role. We now turn to the more complicated blocks, starting with block 3. Block 3 consists of nodes who are mostly friends and socialize with others in block 3 and send unreciprocated friendship ties to those in block 1. Most distinctly, they have much lower levels of social interaction and task interactions with block 4, the teacher (compared to the level of contact between block 4 and block 1 and 2). For example, looking at task interactions with block 4: round(blockmod_task$block.model[, 4], 3) ## Block 1 Block 2 Block 3 Block 4 ## 4.77 3.05 1.20 NaN So, this is a role based on internal friendship, lack of engagement with the person playing the teacher role, and unreciprocated friendships. We can think of this as a low integration, lower status role in the classroom. Block 1 is constituted by high internal rates of social and friendship ties with others in block 1. Block 1 also has high rates of social and task interactions with the teacher. Block 1 also receives friendship ties from block 3, but does not send ties back, suggestive of an asymmetric relationship between those in block 3 (a low engagement block) and block 1 (a high engagement block). Finally, block 2 has high internal interaction (friendship and social) and high task engagement with the teacher role. Students in block 2 are almost entirely friends with other students in block 2, and have few social or task connections to block 1 or 3. Block 2 also has high social and task engagement with the teacher, block 4. In this way, the main substantive difference between block 1 and 2 (both high engagement with the teacher but not with each other) is the relation to block 3, the low integration block; with block 1, but not block 2, being the source of desired, but not returned, friendship ties. At the level of the social system, we have a classroom where different actors play different roles, constituted by: a 'teacher' role [block 4], a 'low engagement student' role [block 3], a 'high engagement, socially desirable' role [block 1], a 'high engagement, less socially desirable' role [block 2]. Overall, we see that the classroom is divided socially into pretty clear-cut social groups (in terms of friendship), while much of the activity of the classroom is centered around the task and social activities of the teacher, with different blocks having different expectations of their interactions with that focal person. We also see that most of the tasks happen with the teacher. Thus, there is little task-based cooperation between friendship groups. The classroom is thus organized by a set of social groups that cluster together on their expectation of task interactions with the teacher. The patterning on rights and responsibilities between student groups is less clear-cut, although there is some differentiation in terms of which blocks are positioned to receive unreciprocated friendships: with those playing the 'high engagement, socially desirable' role [block 1] being sought after by those in block 3. Given this kind of analysis, a researcher could use these revealed roles to predict other outcomes, such as the emergence of alliances in a moment of disagreement/argument, school outcomes, etc. We can also ask who ends up playing different roles in the classroom. For example, we may be interested in knowing something about the gender, racial, etc. makeup of the various roles. In this way, we define the roles first and then ask who is likely to end up playing those roles and the consequences for such mapping (demographics onto roles). Here, let’s do a very quick analysis where we examine the gender and racial makeup of some the key roles that emerged out of our analysis. For example, let’s look at the racial and gender distribution for block 1. Block 1 corresponds to a high engagement role; they have high social interactions with each other but also have very high levels of task engagement with the teacher. Let’s look at the gender distribution for block 1. table(class182_attributes$gender[hc_ids == 1]) ## ## female male ## 3 2 table(class182_attributes$race[hc_ids == 1]) ## ## black white ## 3 2 We can see that this high engagement role is played by a very diverse set of actors. In terms of both gender and race, the role splits nearly in half between female/male and black/white. It is not the case that the high engagement role is played by actors who are all female, white, and so on. Now, let’s compare this to the low engagement role, associated with block 3. Block 3 is defined by high internal social interaction and low task engagement with the teacher. This is a friendship set that is less engaged in the activity of the class as a whole, with its only ties to other blocks in the form of unreciprocated friendship ties. Looking at gender and race distribution for block 3: table(class182_attributes$gender[hc_ids == 3]) ## ## female male ## 2 2 table(class182_attributes$race[hc_ids == 3]) ## ## white ## 4 We can see here that gender is again quite diverse but that everyone identifies as white. Thus, the actors playing the low engagement role can be male or female but, in this case, are all white. It is important to see that the roles themselves are defined completely separately from the attributes of those playing the role. Thus, it becomes an empirical question as to who ends up playing a given role in the classroom. In this case, the low engagement role is not being played by demographic groups who are traditionally disadvantaged in the classroom (e.g., black and male). A more systematic analysis could extend the results to a large number of classrooms, seeing how different students play different roles in different settings. 10.6 Local Equivalence In this last section of the tutorial, we turn our attention to local equivalence. We have so far focused on structural equivalence, where we clustered actors based on being tied to the same other people. With local equivalence, actors are clustered together if they have the same interactional tendencies, or signatures, based on how they interact in their own local 'neighborhood'. Thus, actors are placed in the same position not because they interact with the same people, but because they interact in the same manner. We will explore local equivalence by looking at triads. We will first characterize each actor based on the pattern of triads they are part of. We will then calculate the correlation between actors, based on their triad types. We will then use the correlations between triad-type memberships (across actors) to identify positions in the role analysis. Actors with similar triad-type distributions will be put in the same position, regardless of the specific people with whom they interact. We will keep our analysis simple and focus on a single relation, here using the task network to walk through the analysis. The first step is to calculate the individual triad census. As of the writing of this tutorial, there is no package in R that will calculate the individual triad census. There is, however, a stand alone script made available by Hummel and Sodeur that provides the same results. We begin by reading in their function: source(&quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/R/f.rc.r&quot;) The function is f.rc(), which takes a binary matrix as input and outputs the triads that each node is involved in. We will thus binarize the network. task_mat_binary &lt;- task_mat task_mat_binary[task_mat_binary &gt; 0] &lt;- 1 We can now apply the function over the binarized matrix. task_triads_ego &lt;- f.rc(task_mat_binary) Let's look at the dimensions of the outputted matrix. dim(task_triads_ego) ## [1] 16 36 We can see that the matrix has 16 rows and 36 columns. The rows correspond to the nodes in the network and the columns correspond to the triad type, differentiated by the position that ego holds in that triad. In defining the triad types, the three nodes are labeled as: ego, A1, and A2. For example, column 7 corresponds to triads defined by: ego&lt;-&gt;A1, ego&lt;-&gt;A2. Column 16 captures the same basic structure but ego occupies a different position in it: ego&lt;-&gt;A1, A1&lt;-&gt;A2. The function does not return labels, but we list them here for reference (based on http://www.uni-duisburg-essen.de/hummell/pdf/RoleCensus.pdf) 1 Ego, A1, A2 2 Ego-&gt;A1, A2 3 Ego-&gt;A1, Ego-&gt;A2 4 Ego&lt;-A1 5 Ego&lt;-A1, Ego&lt;-A2 6 Ego&lt;-&gt;A1 7 Ego&lt;-&gt;A1, Ego&lt;-&gt;A2 8 Ego&lt;-A1, Ego-&gt;A2 9 Ego&lt;-&gt;A1, Ego-&gt;A2 10 Ego&lt;-&gt;A1, Ego&lt;-A2 11 A1&lt;-&gt;A2 12 Ego-&gt;A1, A1&lt;-&gt;A2 13 Ego-&gt;A1, Ego-&gt;A2, A1&lt;-&gt;A2 14 Ego&lt;-A1, A1&lt;-&gt;A2 15 Ego&lt;-A1, Ego&lt;-A2, A1&lt;-&gt;A2 16 Ego&lt;-&gt;A1, A1&lt;-&gt;A2 17 Ego&lt;-&gt;A1, Ego&lt;-&gt;A2, A1&lt;-&gt;A2 18 Ego&lt;-A1, Ego-&gt;A2, A1&lt;-&gt;A2 19 Ego&lt;-&gt;A1, Ego-&gt;A2, A1&lt;-&gt;A2 20 Ego&lt;-&gt;A1, Ego&lt;-A2, A1&lt;-&gt;A2 21 A1-&gt;A2 (or A1&lt;-A2) 22 Ego-&gt;A1, A1-&gt;A2 23 Ego-&gt;A1, Ego-&gt;A2, A1-&gt;A2 (or A1&lt;-A2 ) 24 Ego&lt;-A1, A1-&gt;A2 25 Ego&lt;-A1, Ego&lt;-A2, A1-&gt;A2 (or A1&lt;-A2 ) 26 Ego&lt;-&gt;A1, A1-&gt;A2 27 Ego&lt;-&gt;A1, Ego&lt;-&gt;A2, A1-&gt;A2 (or A1&lt;-A2 ) 28 Ego&lt;-A1, Ego-&gt;A2, A1-&gt;A2 29 Ego&lt;-&gt;A1, Ego-&gt;A2, A1-&gt;A2 30 Ego&lt;-&gt;A1, Ego&lt;-A2, A1-&gt;A2 31 Ego-&gt;A1, A1&lt;-A2 32 Ego&lt;-A1, A1&lt;-A2 33 Ego&lt;-&gt;A1, A1&lt;-A2 34 Ego&lt;-A1, Ego-&gt;A2, A1&lt;-A2 35 Ego&lt;-&gt;A1, Ego-&gt;A2, A1&lt;-A2 36 Ego&lt;-&gt;A1, Ego&lt;-A2, A1&lt;-A2 And now let's look at the first ten rows and columns. task_triads_ego[1:10, 1:10] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 23 0 0 0 0 37 4 0 0 0 ## [2,] 21 0 0 0 0 38 5 0 0 0 ## [3,] 34 0 0 0 0 28 0 0 0 0 ## [4,] 52 0 0 0 0 8 0 0 0 0 ## [5,] 17 0 0 0 0 37 11 0 0 0 ## [6,] 23 0 0 0 0 35 6 0 0 0 ## [7,] 26 0 0 0 0 36 1 0 0 0 ## [8,] 26 0 0 0 0 36 1 0 0 0 ## [9,] 27 0 0 0 0 36 0 0 0 0 ## [10,] 14 0 0 0 0 42 9 0 0 0 We can see, for example, that node 1 is in 23 null triads (column 1) and 37 triads with a mutual pair (column 6). Note that column 6 corresponds to the triad where ego is mutually tied to one alter but not the second (and the alters are not tied together): Ego&lt;-&gt;A1. We will now use the triad census matrix to see which nodes have similar patterns of task interactions. Here, we will employ concor (convergence of iterated correlations), an algorithm that clusters nodes into positions based on the correlations between rows. Nodes with similar rows (i.e., triad counts) will be placed together. Note that we could have used concor above, in the original analysis, just as we could use hierarchical clustering in this analysis. We begin by reading in some useful functions to do concor in R. source(&quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/R/concor_functions.R&quot;) source(&quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/R/clustConfigurations_concor.R&quot;) The main function is concor() and the arguments are mat, the matrix of interest, and depth, controlling how deeply into the network we want to partition the nodes (higher values yielding more disaggregated solutions). In this case, the matrix of interest is the matrix of triad counts. We first need to transpose the matrix, as we want to take the correlations between the rows (and R will take the correlations column-wise by default). We will set depth to 6. task_triads_ego_transpose &lt;- t(task_triads_ego) concor_results &lt;- concor(mat = task_triads_ego_transpose, depth = 6) Let's take a look at the results. concor_results ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 1 1 1 1 1 1 1 ## [2,] 1 1 1 1 1 1 2 ## [3,] 1 1 2 3 3 3 3 ## [4,] 1 1 2 4 4 4 4 ## [5,] 1 1 1 1 2 5 5 ## [6,] 1 1 1 1 1 2 6 ## [7,] 1 1 1 2 5 7 8 ## [8,] 1 1 1 2 5 7 8 ## [9,] 1 1 1 2 5 8 9 ## [10,] 1 1 1 1 2 6 10 ## [11,] 1 1 2 3 3 3 3 ## [12,] 1 1 1 2 5 8 9 ## [13,] 1 1 1 2 6 9 11 ## [14,] 1 1 1 1 1 2 7 ## [15,] 1 1 1 1 1 1 2 ## [16,] 1 2 3 5 7 10 12 The rows correspond to the nodes in the network. There are 7 columns, with the first column set to the solution with all nodes in 1 cluster. The values correspond to the clusters under different solutions, with more disaggregated solutions as we move from left to right. For example, in the second column, there are 2 positions, with node 16 split off from everyone else. In the third column, there are 3 positions; in the fourth column there are 5, and so on. Now, we need to figure out how many clusters best describes the underlying data. To do this, we will examine how each solution (or columns from concor_results) fits the data. The function to plot the fit statistics is clustConfigurations_concor(). The basic idea is to examine the within- and between-cluster correlations for each solution. We first take the observed correlation matrix, and calculate the mean correlations by cluster (mean value within the cluster and mean values to other clusters). We then take this matrix of means and correlate it with the observed correlation matrix to see how well the solution approximates the true data. Higher values suggest better fits. The arguments to clustConfigurations_concor() are the outputted concor results (from above) and the observed correlation matrix. Let's first calculate the observed correlation matrix and then produce the plot of fit statistics. We put an NA on the diagonal of the observed correlation matrix as those values are by definition 1, and we don't want those values as part of the calculation. observed_correlation &lt;- cor(task_triads_ego_transpose) diag(observed_correlation) &lt;- NA concor_fit &lt;- clustConfigurations_concor(concor_results, observed_correlation) The plot shows the fit as the network is divided into increasingly finer clusters. We are looking for solutions where the increase in the correlation is large as depth increases, followed by a plateau (so dividing further does not help as much). This amounts to looking for a sharp increase in the plot. For this network, we will use the solution with depth set to 4. Note that the 1-cluster solution does not appear on the plot because its correlation with the observed correlation matrix is undefined. Let's grab the memberships based on the depth = 4 solution. positions_triads &lt;- concor_results[, 4] positions_triads ## [1] 1 1 3 4 1 1 2 2 2 1 3 2 2 1 1 5 We see that there are five positions, with actors 4 and 16 in distinct positions, and the rest split between positions 1, 2 and 3. Given this initial result, a useful next step is to explore the types of triads associated with each position (i.e., to offer substantive insight into the uncovered positions). We will create a summary matrix, where the cells correspond to the average number of times that actors in a position are in a given triad type. The matrix is m x n, where m is the number of positions and n is the 36 possible triad types. We will use the aggregate() function to calculate the means over the columns in our triad matrix (task_triads_ego), setting by to the positions of each actor and FUN to mean. position_triad_mat &lt;- aggregate(task_triads_ego, by = list(cluster = positions_triads), FUN = mean) Let's look at a couple of key columns: position_triad_mat[, c(1, 2, 7, 8, 17, 18)] ## cluster V1 V6 V7 V16 V17 ## 1 1 20.57143 37.28571 6.428571 17.28571 10.28571 ## 2 2 27.00000 35.00000 1.000000 15.00000 9.00000 ## 3 3 34.00000 28.00000 0.000000 16.00000 6.00000 ## 4 4 52.00000 8.00000 0.000000 18.00000 1.00000 ## 5 5 0.00000 0.00000 73.000000 0.00000 32.00000 We can see, for example, that position 5 (node 16, the teacher), is constituted by having a high number of type 7 triads (Ego&lt;-&gt;A1, Ego&lt;-&gt;A2) and type 17 triads (Ego&lt;-&gt;A1, Ego&lt;-&gt;A2, A1&lt;-&gt;A2). They have 0 type 16 triads (Ego&lt;-&gt;A1, A1&lt;-&gt;A2). This means that the 'teacher' position does tasks with both alters in a given triad. We are unlikely to find a case where the teacher does tasks with one student (A1) who does tasks with another student (A2) but the teacher ignores the second student. The teacher can, however, do tasks with two students who do not do tasks with each other (Ego&lt;-&gt;A1, Ego&lt;-&gt;A2). Compare this to the triads for position 4 (node 4) who is almost never in triad 17 (Ego&lt;-&gt;A1, Ego&lt;-&gt;A2, A1&lt;-&gt;A2), and is much more likely to be in triad 1 (the null triad). This is suggestive of a position that does not engage heavily in task interactions. Position 2, on the other hand, is constituted by a higher number of type 6 triads (Ego&lt;-&gt;A1) and fewer null triads (type 1) than position 4. Thus, students in this position are more involved in task interactions than those in position 4. Position 2 is also very different than position 5 (the teacher), most clearly in the lower number of type 7 triads (Ego&lt;-&gt;A1, Ego&lt;-&gt;A2). Those in position 2 are unlikely to do task interactions with A1 and A2, when A1 and A2 are not tied together. Thus, we do not see students playing the ego role in Ego&lt;-&gt;A1, Ego&lt;-&gt;A2, as this kind of interaction is reserved for the teacher. All of this points to a system where task interactions are centered around the teacher (who connects unconnected students), with some students more engaged than others. Finally, as before, we can use our results to run a blockmodel analysis. The steps are the same as before and we leave it to the interested reader to explore this in more depth. Overall, this tutorial has offered an overview of doing positional and role analysis in R. We will take up many of these themes again in later tutorials, such as in Chapter 12 (networks and culture). "],["ch11-Two-mode-Networks.html", "11 Two-mode Networks: Affilitations and Dualities 11.1 Example Affiliation Data 11.2 Plotting the Network 11.3 Single Mode Networks 11.4 Club-to-Club Network 11.5 Using the tnet package 11.6 Racial Segregation 11.7 Club Centrality and Attributes", " 11 Two-mode Networks: Affilitations and Dualities This tutorial walks through the analysis of two-mode (affiliation) data in R. We will cover how to plot two-mode network data using the igraph package, how to transform it into one mode data and how to calculate traditional measures, like centrality. The tutorial will draw directly on many of the previous tutorials, including Chapter 5 (visualization), Chapter 8 (cohesion and groups), and Chapter 9 (centrality). We will work with affiliation data collected by Daniel McFarland on student extracurricular affiliations. It is a longitudinal data set, with 3 waves - 1996, 1997, 1998, although we will only use the first wave, 1996. We consider dynamics on two-mode networks in Chapter 13, Part 3 (statistical models for two-mode networks). The data consist of students (anonymized) and the student clubs in which they are members (e.g., National Honor Society, wrestling team, cheerleading squad, etc.). The data thus allow us to capture the duality of social life in the school, as students are linked by being in the same clubs, and clubs are linked by sharing the same students. Substantively, we are motivated by the following questions: a) Which student clubs serve to integrate the school and which are more peripheral? b) Which student clubs tend to share members at high rates? c) What is the shared feature, or theme, that brings these clubs together in a cluster? Overall, we are interested in the manner in which membership in clubs serves to integrate (or divide) the school. 11.1 Example Affiliation Data We will use the igraph package for this tutorial. library(igraph) Our data are stored in three files: an affiliation matrix (showing which students are part of which clubs), a student attribute file (showing the attributes of the students), and a club attribute file (showing the attributes of the clubs). Let's read in the affiliation file from a URL: url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/affiliations_1996.txt&quot; affiliations96 &lt;- read.delim(file = url1, check.names = FALSE) We need to preserve the original column names for labeling our visualizations so we use the \"check.names = FALSE\" argument. The affiliation matrix is organized with students on the rows and clubs on the columns. dim(affiliations96) ## [1] 1295 91 We can see that there are 1295 students and 91 clubs. The affiliation matrix is based on a series of club membership dummy variables, coded \"1\" for membership, \"0\" for no membership. Let's take a look at the data (just the first 6 rows and 6 columns): affiliations96[1:6, 1:6] ## Academic decathalon Art Club Asian Club Band, 8th Band, Jazz Band, Marching (Symphonic) ## 101498 0 0 0 0 0 0 ## 104452 0 0 0 0 0 1 ## 104456 0 0 0 0 0 0 ## 104462 0 0 0 0 0 0 ## 104471 0 0 0 0 0 0 ## 105215 0 0 0 0 0 0 We can see, for example, that the second student in the data is part of the marching band while the first student is not. Note that the rownames of the matrix capture the ids of the students. To get a sense of the data, let’s look at the affiliations for the first student in 1996. Here we grab the first row in the affiliation data and locate the 1s in that row: affils_student1 &lt;- affiliations96[1, ] affils_student1[which(affils_student1 == 1)] ## NHS Spanish Club (high) Spanish NHS Theatre Productions Thespian Society (ITS) ## 101498 1 1 1 1 1 We can see that student 1 was a member of the NHS, Spanish Club, Spanish NHS, Theatre Productions and Thespian Society. And now we will read in the attribute data. We have two files, one for the students and one for the clubs. url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/attributes_students.txt&quot; attributes_students &lt;- read.delim(file = url2, stringsAsFactors = FALSE) url3 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/attributes_clubs.txt&quot; attributes_clubs &lt;- read.delim(file = url3, stringsAsFactors = FALSE) The student data frame only includes students (1295 rows) and includes student specific attributes. The student specific attributes are: gender (male, female) grade96 (grade in 1996) grade97 (grade in 1997) race (Asian, black, Hispanic, Native American, white). Let's take a look at the first five rows of our student data frame. attributes_students[1:5, ] ## ids type gender grade96 grade97 race missing96 missing97 ## 1 101498 student female 11 12 white 0 0 ## 2 104452 student male 9 10 black 0 0 ## 3 104456 student female 9 10 white 0 0 ## 4 104462 student male 9 10 black 0 0 ## 5 104471 student female 9 10 black 0 0 The club data frame only includes clubs (91 rows) and includes club specific attributes.The main club attributes of interest are: club_type_detailed (Academic Interest, Academic Competition, Ethnic Interest, Individual Sports, Leadership, Media, Performance Art, Service, Team Sports) club_profile (how much attention does the club get? low, moderate, high, very high) club_feeder (does club feed another club, like 8th grade football to 9th grade football?) club_type_gender (is club mixed gender, just boys or just girls?) club_type_grade (which grades, if any, is club restricted to?). Now, let's take a look at the first five rows of our club data frame. attributes_clubs[1:5, ] ## ids type missing96 missing97 club_type_detailed club_type_general club_type_gender club_profile club_season club_commitment club_type_grade club_feeder ## 1 Academic decathalon club 0 0 Academic Competition Academic boys_girls low spring not_high all_grades no ## 2 Art Club club 0 0 Performance Art Performance Art boys_girls low all_year not_high all_grades no ## 3 Asian Club club 0 0 Ethnic Interest Ethnic Interest boys_girls low all_year not_high all_grades no ## 4 Band, 8th club 0 0 Performance Art Performance Art boys_girls low all_year high eighth yes ## 5 Band, Jazz club 0 0 Performance Art Performance Art boys_girls moderate all_year high ninth+ no Note that the student attribute data frame must be sorted in the same order as the rows of the affiliation matrix, while the club attribute data frame must be sorted in the same order as the columns of the affiliation matrix. This is already done for our data. We will now make a combined data frame, that puts together the student attributes with the club attributes. This will be useful when constructing the igraph object below. We will proceed by stacking the student data frame on top of the club data frame (as the igraph object will order the nodes by rows and then columns). For student specific attributes, we will put in NAs for the clubs; for club specific attributes, we will put in NAs for the students. For example, it does not make sense to think of a club having a 'race', so we put in an NA for race. There are also a few attributes that pertain to both students and clubs and here there are no structural NAs, as both node types have meaningful values. These include ids (a unique identifier), type (student or club), missing96 (is node missing in 1996, 0 = no; 1 = yes), missing97 (is node missing in 1997, 0 = no; 1 = yes). Let's start with those variables that have meaningful values for both students and clubs. In this case, we simply need to stack the values for students on top of the values for clubs. shared_var_names &lt;- c(&quot;ids&quot;, &quot;type&quot;, &quot;missing96&quot;, &quot;missing97&quot;) shared &lt;- rbind(attributes_students[, shared_var_names], attributes_clubs[, shared_var_names]) And now let's move to the student specific attributes, putting in NAs for the clubs. We will keep this simple and first create vectors of NAs of the right length (91), representing the values for the clubs. We will then put that together with the student values for race, gender and grade. num_clubs &lt;- nrow(attributes_clubs) NA_dat_club &lt;- rep(NA, num_clubs) student_var_names &lt;- c(&quot;race&quot;, &quot;gender&quot;, &quot;grade96&quot;, &quot;grade97&quot;) student_specific &lt;- rbind(attributes_students[, student_var_names], data.frame(race = NA_dat_club, gender = NA_dat_club, grade96 = NA_dat_club, grade97 = NA_dat_club)) And now we do the same thing for club specific attributes, putting in NAs for the students and then stacking the data together; again, with the students stacked on top of the clubs. num_students &lt;- nrow(attributes_students) NA_dat_student &lt;- rep(NA, num_students) club_var_names&lt;- c(&quot;club_type_detailed&quot;, &quot;club_profile&quot;, &quot;club_feeder&quot;, &quot;club_type_gender&quot;, &quot;club_type_grade&quot;) club_specific &lt;- rbind(data.frame(club_type_detailed = NA_dat_student, club_profile = NA_dat_student, club_feeder = NA_dat_student, club_type_gender = NA_dat_student, club_type_grade = NA_dat_student), attributes_clubs[, club_var_names]) And now let's combine the three data frames together, column-wise. attributes_students_clubs &lt;- cbind(shared, student_specific, club_specific) head(attributes_students_clubs) ## ids type missing96 missing97 race gender grade96 grade97 club_type_detailed club_profile club_feeder club_type_gender club_type_grade ## 1 101498 student 0 0 white female 11 12 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 104452 student 0 0 black male 9 10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 104456 student 0 0 white female 9 10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 104462 student 0 0 black male 9 10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 104471 student 0 0 black female 9 10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 105215 student 0 0 white female 11 12 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; For the students, we correctly see NA values for the club specific columns (like club_type_detailed). Before we construct our igraph object, we first need to remove any students or clubs who were not in the school in the 1996 year (the focus of our analysis here). We can determine this by looking at the missing96 column in the attributes_students_clubs data frame. Let's only keep those cases where missing96 is equal to 0 (i.e., not missing). We need to do this for both the attribute data frame and the affiliation matrix. Note that we must identify the missing students and clubs separately and use that to reduce the rows and columns in the affiliation matrix. not_missing &lt;- attributes_students_clubs$missing96 == 0 is_student &lt;- attributes_students_clubs$type == &quot;student&quot; not_missing_student &lt;- not_missing[is_student] is_club &lt;- attributes_students_clubs$type == &quot;club&quot; not_missing_club &lt;- not_missing[is_club] affiliations96_nomiss &lt;- affiliations96[not_missing_student, not_missing_club] attributes_nomiss &lt;- attributes_students_clubs[not_missing, ] We are now in position to create our two-mode graph in igraph. The function is graph_from_incidence_matrix(). The main arguments are incidence (the two-mode affiliation matrix) and mode. Here we set mode to \"all\", telling igraph to create mutual connections between student and club. affil_net96 &lt;- graph_from_incidence_matrix(incidence = affiliations96_nomiss, mode = &quot;all&quot;) affil_net96 ## IGRAPH 10dd9c6 UN-B 1029 2641 -- ## + attr: type (v/l), name (v/c) ## + edges from 10dd9c6 (vertex names): ## [1] 101498--NHS 101498--Spanish Club (high) 101498--Spanish NHS 101498--Theatre Productions 101498--Thespian Society (ITS) 104452--Band, Marching (Symphonic) 104452--Basketball, boys 9th 104452--Football, 9th 104452--Latin Club 104456--German Club 104456--Latin Club 104462--Basketball, boys JV 104462--Choir, concert 104462--Football, 9th 104462--Latin Club 104462--Spanish Club ## [17] 105215--Theatre Productions 105215--Thespian Society (ITS) 106517--Basketball, girls 9th 106517--Orchestra, Full Concert 106517--Pep Club 106517--Track, girls V 106572--Basketball, girls 8th 106587--Cheerleaders, 9th 106587--Orchestra, Full Concert 106587--Pep Club 106592--Band, 8th 106592--Football, 8th 106604--Choir, treble 106604--Orchestra, 8th 106604--Pep Club 106604--Spanish Club ## [33] 106609--Band, Jazz 106609--Spanish Club 106621--Track, boys 8th 106709--Orchestra, Symphonic 106709--Spanish Club 109492--Baseball, V 109493--French Club (high) 109493--Wrestling, V 113047--Asian Club 113047--Football, V 113047--Tennis, boys V 113050--Football, V 113053--Forensics 113053--Forensics (National Forensics League) 113053--German Club 113060--NHS ## [49] 113060--PEER 113107--Soccer, V 113107--Swim &amp; Dive Team, boys 113122--Choir, women&#39;s ensemble 113122--Spanish Club 113123--Choir, women&#39;s ensemble 113127--Debate 113127--Forensics (National Forensics League) 113127--Key Club 113127--Spanish Club 113137--Debate 113137--Forensics 113137--Forensics (National Forensics League) 113140--Debate 113140--Forensics (National Forensics League) 113140--German Club ## [65] 113140--Spanish Club 113168--French Club (low) 113168--Latin Club 113211--Cheerleaders, Spirit Squad 113211--Choir, concert 113211--Pep Club 113211--Spanish Club 113214--Choir, concert 113214--Drill Team 113214--German Club 113214--Pep Club 113214--Softball, V 113429--Academic decathalon 113429--Key Club 113429--Orchestra, Full Concert 113429--Pep Club ## [81] 113429--Science Olympiad 113433--Football, V 113479--Choir, women&#39;s ensemble 113479--Drunk Driving 113479--Key Club 113479--PEER 113694--Asian Club 113694--Choir, a capella 113694--Choir, chamber singers 113694--Orchestra, Symphonic 113719--French Club (high) 113719--Orchestra, Symphonic 113719--Pep Club 113719--Yearbook Contributors 113939--French Club (high) 113939--NHS ## [97] 114002--Football, V 114002--Wrestling, V 114010--PEER 114010--Pep Club 114010--Yearbook Editors 114015--Softball, V 114015--Yearbook Contributors 114025--Choir, a capella 114025--Orchestra, Symphonic 114025--Theatre Productions 114037--Basketball, girls JV 114037--Forensics (National Forensics League) 114037--Latin Club 114037--NHS 114037--Orchestra, Symphonic 114037--Spanish Club (high) ## [113] 114037--Spanish NHS 114037--STUCO 114037--Yearbook Editors 114071--Forensics 114071--Forensics (National Forensics League) 114071--Key Club 114073--Debate 114073--Forensics 114073--Forensics (National Forensics League) 114076--Key Club 114076--Latin Club 114076--Orchestra, Symphonic 114078--Basketball, boys JV 114078--Latin Club 114078--Spanish Club 114085--Spanish Club ## + ... omitted several edges We can see that there are 1029 nodes and 2641 edges. Note that the two-mode, or bipartite, network has a vertex attribute called type that indicates the kind of node. type is automatically created by igraph and attached as a vertex attribute. A False corresponds to the students (rows) and True corresponds to the clubs (columns). We thus have a network of students and clubs where all ties link students to clubs (based on membership). Let’s grab the vertex attribute type from the two-mode igraph object. type96 &lt;- vertex_attr(affil_net96, &quot;type&quot;) table(type96) ## type96 ## FALSE TRUE ## 938 91 We see that there are 938 students and 91 clubs in the school. And now we can go ahead and put the attributes of the nodes onto the igraph object. We will add attributes for race, gender, grade96, club_type_detailed and club_profile. Remember that race, grade96 and gender are only relevant for students (clubs have an NA), while club_type_detailed and club_profile are only relevant for clubs (students have an NA). affil_net96 &lt;- set_vertex_attr(graph = affil_net96, name = &quot;race&quot;, value = attributes_nomiss$race) affil_net96 &lt;- set_vertex_attr(graph = affil_net96, name = &quot;gender&quot;, value = attributes_nomiss$gender) affil_net96 &lt;- set_vertex_attr(graph = affil_net96, name = &quot;grade96&quot;, value = attributes_nomiss$grade96) affil_net96 &lt;- set_vertex_attr(graph = affil_net96, name = &quot;club_type_detailed&quot;, value = attributes_nomiss$club_type_detailed) affil_net96 &lt;- set_vertex_attr(graph = affil_net96, name = &quot;club_profile&quot;, value = attributes_nomiss$club_profile) 11.2 Plotting the Network Now that we have our two-mode networks constructed, let's first figure out how to create a nice plot. The igraph package has excellent plotting functionality that allows you to assign visual attributes to igraph objects before you plot. The alternative is to pass 20 or so arguments to the plot.igraph() function, which can get messy. Each node (or \"vertex\") object is accessible by calling V(g), and you can call (or create) a node attribute by using the $ operator (i.e., V(g)$name_of_attribute). Let's now use this notation to set the vertex color, with special attention to making graph objects slightly transparent. We'll use the rgb() function in R to do this. We specify the levels of red, green, and blue and \"alpha-channel\" (a.k.a. opacity) using this syntax: rgb(red, green, blue, alpha). To return solid red, one would use this call: rgb(red = 1, green = 0, blue = 0, alpha = 1). We will make nodes, edges, and labels slightly transparent so that when things overlap it is still possible to read them. You can read up on the RGB color model at http://en.wikipedia.org/wiki/RGB_color_model. In this case let’s color the students red and the affiliations green. Here we set the students red and set the opacity to .5: V(affil_net96)$color[type96 == FALSE] &lt;- rgb(red = 1, green = 0, blue = 0, alpha = .5) And now we set the clubs green: V(affil_net96)$color[type96 == TRUE] &lt;- rgb(red = 0, green = 1, blue = 0, alpha = .5) Notice that we index the V(g)$color object by using the type96 variable used above. Students correspond to False in the type variable and clubs correspond to True. From here on out, we do not specify \"red = \", \"green = \", \"blue = \", and \"alpha = \". These are the default arguments (R knows the first number corresponds to red, the second to green, and so on). Now we'll set some other graph attributes: V(affil_net96)$label &lt;- V(affil_net96)$name # setting label of nodes V(affil_net96)$label.color &lt;- rgb(0, 0, .2, .5) # set the color of the labels V(affil_net96)$label.cex &lt;- .5 # make the labels of the nodes smaller V(affil_net96)$size &lt;- 6 # set size of nodes to 6 V(affil_net96)$frame.color &lt;- V(affil_net96)$color # set color around nodes We can also set edge attributes. We set the edge attributes using the E() function. Here we'll make the edges nearly transparent and slightly yellow because there will be so many edges in this graph: E(affil_net96)$color &lt;- rgb(.5, .5, 0, .2) We can also specify which layout we will use. In this case, we will use the Fruchterman-Reingold force-directed layout algorithm. Generally speaking, when you have a ton of edges, the Kamada-Kawai layout algorithm works well. It can, however, be slow for networks with many nodes. As a default, igraph will use the grid based implementation of the algorithm in the case of more than a thousand vertices. This implementation is faster, but can fail to produce a plot with any meaningful pattern if you have too many isolates. Also note that for more recent versions of igraph, the F-R layout has been rewritten and will produce bad looking plots if a network has more than 999 vertices. You need to fix the grid parameter to \"nogrid\" to fix the problem. Our network has more than 999 vertices and has many isolates. Therefore, we want to offset the default and specify that we do not want to use the grid implementation. layout &lt;- layout_with_fr(affil_net96, grid = &quot;nogrid&quot;) Now, we'll open a pdf \"device\" on which to plot. This is just a connection to a pdf file. We will go ahead and save it to our working directory, which can be set using setwd() and checked using getwd(). pdf(&quot;magact_stdnt_actvts_1996.pdf&quot;) plot(affil_net96, layout = layout) dev.off() Once we break off the connection, the created plot will be saved out in the working directory, saved as \"magact_stdnt_actvts_1996.pdf\" (for convenience, we have also included a version of the plot in this html output). Now, if you open the pdf output, you'll notice that you can zoom in on any part of the graph ad infinitum without losing any resolution. How is that possible in such a small file? It's possible because the pdf device output consists of data based on vectors: lines, polygons, circles, ellipses, etc., each specified by a mathematical formula that your pdf program renders when you view it. Regular bitmap or jpeg picture output, on the other hand, consists of a pixel-coordinate mapping of the image in question, which is why you lose resolution when you zoom in on a digital photograph or a plot produced as a picture. Looking at the output, this plot is oddly reminiscent of a crescent and star, but impossible to read. Part of the problem is the way in which the layout algorithms deal with the isolates. For example, layout_with_fr will squish all of the connected nodes into the center creating a useless \"hairball\"-like visualization. Let's remove all of the isolates (the crescent), change a few aesthetic features, and replot. First, we'll remove isolates, by deleting all nodes with a degree of 0, meaning that they have zero edges. In this case, this means students who are part of no clubs or clubs that have no members. Then, we'll suppress labels for students and make their nodes smaller and more transparent. Then we'll make the edges narrower and more transparent. Then, we'll replot using various layout algorithms. Here we identify the isolates and then remove them from the network: degree0 &lt;- which(degree(affil_net96) == 0) affil_net96_noisolates &lt;- delete_vertices(affil_net96, degree0) Here we extract the type of node for this subsetted network: type96_noisolates &lt;- vertex_attr(affil_net96_noisolates, &quot;type&quot;) table(type96_noisolates) ## type96_noisolates ## FALSE TRUE ## 850 91 Let's create a vector that identifies if the node is a student or not. is_student_type &lt;- type96_noisolates == FALSE We see that we still have 91 clubs but many students have been removed. Now, let’s tweak some of the plotting parameters. We will take off the labels for the students, change the color of the students and make the student nodes smaller. V(affil_net96_noisolates)$label[is_student_type] &lt;- NA V(affil_net96_noisolates)$color[is_student_type] &lt;- rgb(1, 0, 0, .1) V(affil_net96_noisolates)$size[is_student_type] &lt;- 2 Here we change some of the edge attributes for the plot. E(affil_net96_noisolates)$color &lt;- rgb(.5, .5, 0, .05) Now let's create the plot, saving out it out as a pdf. First with Kamada-Kawai layout: pdf(&quot;magact_stdnt_actvts_1996_layout_with_kk.pdf&quot;) plot(affil_net96_noisolates, layout = layout_with_kk) dev.off() We can see that the clubs are colored green and have labels, while the students are colored a transparent red and have no labels. Now with the FR layout. pdf(&quot;magact_stdnt_actvts_1996_layout_with_fr.pdf&quot;) plot(affil_net96_noisolates, layout = layout_with_fr) dev.off() Note that if we ran this again we would get a slightly different figure. The nice thing about the Fruchterman-Reingold layout in this case is that it really emphasizes centrality -- the nodes that are most central are nearly always placed in the middle of the plot. More generally, these plots are nice in that they emphasize the dualistic nature of student and club (or person and affiliation). We see that certain clubs and students are placed in close proximity to each other. In this way, we get a space, where students and clubs tend to cluster together, with sets of students tending to join the same kinds of clubs. For example, we can see that eighth grade boy’s track and eighth grade boy’s cross-country tend to cluster together. They are linked in the sense that students in one club tend to be in another. From the other side, students on these teams are placed close together in the space, as they are linked through common patterns of club membership (here eighth grade boy sports). We can also incorporate the node attributes into the plot. Here, we will do a quick plot where the nodes are colored based on racial identity. Student will be colored by racial identity while clubs will all be green, as before. We will use the recode() function in the car package to set the colors of the nodes. We only want to change the color for the students, however, and so we restrict the recoding to just those cases where type is a student. library(car) race &lt;- V(affil_net96_noisolates)$race[is_student_type] student_node_color &lt;- recode(race, &quot;&#39;white&#39; = &#39;red&#39;; &#39;Hispanic&#39; = &#39;grey&#39;; &#39;Asian&#39; = &#39;blue&#39;; &#39;black&#39; = &#39;yellow&#39;; &#39;Native American&#39; = &#39;black&#39;&quot;) And now we set the color of the nodes (both fill and frame of the circle) based on the vector of colors defined above, just changing the values for the students. V(affil_net96_noisolates)$color[is_student_type] &lt;- student_node_color V(affil_net96_noisolates)$frame.color[is_student_type] &lt;- student_node_color For this plot we will also take off the labels for the clubs. V(affil_net96_noisolates)$label[!is_student_type] &lt;- NA And now we can go ahead and produce the plot (this time we won't save it out as a pdf). plot(affil_net96_noisolates) We can see that our students are colored yellow, red, blue, etc., while the clubs are all green. At first glance, it would appear that students tend to be closest to other students of the same race (i.e., they join the same clubs), but that pockets of racial groups are often intermixed in the same region of the plot. We will return to the question of racial segregation in affiliations below. While the two-mode network representation is useful and intuitive, it is often easier to actually analyze the one-mode projections of the network, and that is what we turn to next. 11.3 Single Mode Networks Having plotted our two-mode network for the 1996 year, we now move to an analysis that focuses on the one-mode projections of the network. We will produce two networks, one for student-to-student ties and one for club-to-club ties based on the student-to-club network analyzed above. For the student-to-student network, all of the nodes are students and we define a tie between two students based on the number of clubs they have in common. For the club-to-club network, all the nodes are clubs, and we define a tie between clubs based on the number of members they have in common. 11.3.1 Constructing One-Mode Projections We will begin by producing the one-mode projection using matrix multiplication. This is done using R's matrix algebra commands. We first need to get the affiliation data into the matrix format. To get the one-mode representation of ties between rows (students in our example), multiply the matrix by its transpose. To get the one-mode representation of ties between columns (clubs in our example), multiply the transpose of the matrix by the matrix. Note that you must use the matrix-multiplication operator %*% rather than a simple asterisk. For the club to club network: affiliations96_nomiss &lt;- as.matrix(affiliations96_nomiss) club_club96 &lt;- t(affiliations96_nomiss) %*% affiliations96_nomiss club_club96[1:5, 1:5] ## Academic decathalon Art Club Asian Club Band, 8th Band, Jazz ## Academic decathalon 29 3 1 0 1 ## Art Club 3 33 0 0 0 ## Asian Club 1 0 8 0 0 ## Band, 8th 0 0 0 46 0 ## Band, Jazz 1 0 0 0 17 The values in the matrix capture the number of people who are members of the row club and column club. For example, there are three students who are part of the Academic decathlon club and the Art club. Note that the diagonal tells us the number of students in that club. So, there are 33 students in the Art club. dim(club_club96) ## [1] 91 91 The matrix is square with the number of rows and columns equal to the number of clubs, 91 in this case. If we wanted to get the student to student network, we would take the affiliation matrix and multiply it by the transpose of the affiliation matrix: student_student96 &lt;- affiliations96_nomiss %*% t(affiliations96_nomiss) student_student96[1:5, 1:5] ## 101498 104452 104456 104462 104471 ## 101498 5 0 0 0 0 ## 104452 0 4 1 2 0 ## 104456 0 1 2 1 0 ## 104462 0 2 1 5 0 ## 104471 0 0 0 0 0 The values capture the number of clubs that student i (on the rows) and j (on the columns) have in common. dim(student_student96) ## [1] 938 938 Again, we have a square matrix, but this time the rows and columns are equal to the number of students in the school. We can now take those matrices and create two igraph objects. This will make it possible to plot and calculate statistics on the one-mode networks. It is also possible to use the functions within igraph to construct the one-mode networks directly from the two-mode network, without having to do the matrix multiplication. Here we will rely on the bipartite_projection() function, which converts a two-mode (bipartite) network to two one-mode (unipartite) networks. onemode96 &lt;- bipartite_projection(affil_net96) This is a list with two objects (proj1 and proj2). The first is the student-student projection and the second is the club-club projection. Let's take a look at the club-to-club network: club_net96 &lt;- onemode96$proj2 Note that an edge weight has been automatically passed to the constructed graph. The weight for the club-to-club network is based on the number of shared members between i and j. This is stored as an edge attribute called weight. We can extract useful information from this object, including the names of the nodes, here the clubs. club_names &lt;- V(club_net96)$name length(club_names) ## [1] 91 We see there are 91 names. We can also grab the matrix from this igraph object. We include an attr argument to get the values in the matrix. mat &lt;- as_adjacency_matrix(graph = club_net96, attr = &quot;weight&quot;, sparse = F) mat[1:5, 1:5] ## Academic decathalon Art Club Asian Club Band, 8th Band, Jazz ## Academic decathalon 0 3 1 0 1 ## Art Club 3 0 0 0 0 ## Asian Club 1 0 0 0 0 ## Band, 8th 0 0 0 0 0 ## Band, Jazz 1 0 0 0 0 This matrix is the same as club_club96 except for the diagonal. The diagonal here is set to 0, as igraph takes out the self-weight (number of members) when constructing the network. 11.3.2 Plotting One-Mode Projections We now turn to plotting the single mode, club-to-club network (we could do an analogous plot for students). One of the main challenges of plotting these networks is that the one-mode projections of two-mode networks tend to be quite dense. We will thus need to pay special attention to the coloring of the edges, noting that the edges themselves are weighted. We will begin by setting vertex attributes, making sure to make them slightly transparent by altering the alpha input to the rgb() function. V(club_net96)$label.color &lt;- rgb(0, 0, .2, .8) V(club_net96)$label.cex &lt;- .60 V(club_net96)$size &lt;- 6 V(club_net96)$color &lt;- rgb(0, 0, 1, .3) V(club_net96)$frame.color &lt;- V(club_net96)$color We will also set the edge opacity/transparency as a function of how many students each club has in common (the weight of the edge that connects the two clubs). We use the log() function to make sure all transparencies are on relatively the same scale, then divide by twice the maximum edge weight to get them on a scale from about .1 and .5. egalpha &lt;- log1p(E(club_net96)$weight) / max(log1p(E(club_net96)$weight) * 2) E(club_net96)$color &lt;- rgb(.25, .75, 0, egalpha) For illustrative purposes, let's compare how the LGL (layout generator for larger graphs) and Fruchterman-Reingold algorithms render this graph: pdf(&quot;magact_stdnt_actvts_1996_clubs.pdf&quot;) plot(club_net96, main = &quot;layout_with_lgl&quot;, layout = layout_with_lgl) plot(club_net96, main = &quot;layout_with_fr&quot;, layout = layout_with_fr) dev.off() Be sure to go to the second page in the pdf to see the FR layout. You might like the LGL layout for this graph, because the center of the graph is very busy if you use the Fruchterman-Reingold layout. The plot looks okay but is pretty dense, making it harder to interpret. In this next plot, we will pare things down a bit, weighting the edges to produce a simpler plot. We will make the width of the edges proportional to the edge weight, based on the following logic. We first calculate the mean and standard deviation of the raw weights. We then assign a new weight, determined by how much the raw weight is below/above the mean. All edge weights equal to or below the mean get a -1 in our new weighting scheme, which will reduce the density of the plot considerably (we set the weight to -1 to ensure that these edges do not show up in the plot). All edges between 0 and 1 standard deviations above the mean weight get a .5, while all edges between 1 and 2 standard deviations get a 1.5. Finally, all edges above 2 standard deviations get a 2.5. Note that these values are only used for plotting purposes. std_weight &lt;- sd(E(club_net96)$weight) weight_mean_center &lt;- (E(club_net96)$weight - mean(E(club_net96)$weight)) recode_weight &lt;- E(club_net96)$weight recode_weight[weight_mean_center &lt;= 0] &lt;- -1 recode_weight[(weight_mean_center &gt; 0) &amp; (weight_mean_center &lt;= std_weight)] &lt;- .5 recode_weight[(weight_mean_center &gt; std_weight) &amp; (weight_mean_center &lt;= std_weight * 2)] &lt;- 1.5 recode_weight[weight_mean_center &gt; std_weight * 2] &lt;- 2.5 We will now apply our new weighting scheme to the widths of the edges. We will also change the color of the edges (to be the same across all edges). E(club_net96)$color &lt;- rgb(.5, .5, 0, .2) E(club_net96)$width &lt;- recode_weight V(club_net96)$size &lt;- 3 plot(club_net96, layout = layout_with_lgl) The plot offers important insight into the structure of club membership in the school. We can see that there are a number of grade-specific sports teams on the periphery and a core consisting of more generalist clubs, like Pep Club and National Honor Society (NHS). Now, we want to take the basic intuition from our plot and analyze the network more formally. We will consider key measures of centrality. 11.4 Club-to-Club Network Here we will analyze the weighted version of the club-to-club network, based on the idea that some clubs share more members than others. We could, of course, also analyze the binary version of the network (equal to 1 if ij have at least one member in common and 0 if not). The problem with the binary network is that it equates two clubs who have one shared member with clubs who have 5 (6, 7, etc.) common members. Binary networks also tend to be quite dense, as they capture pretty weak relationships in the case of one-mode projections. Many clubs may share at least one member together, making the ties in the network not very differentiating. Thus, we will generally want to use the weighted version of the network when analyzing a one-mode projection of a two-mode network. One thing we have to consider is what form the weights should take when doing different calculations. There are a number of different options and different weights are appropriate for different measures. igraph will use the edge weights on the igraph object by default. We can, however, tell igraph, within particular functions, to use a different set of weights, which will be useful in certain cases. Let's go ahead and create a couple different versions of the edge weights (based on the number of shared members). Let's first create a set of standardized edge weights. Here we take the number of members that i and j share but we divide by the standard deviation of those counts, thus standardizing the weights by the spread of the data. Standardizing the weights is useful as the centrality scores can be interpreted on a common metric; convenient, for example, if we are trying to make comparisons across networks. weights &lt;- E(club_net96)$weight scaled_weights &lt;- weights / sd(weights) For certain measures, we will want to use a set of weights based on the inverse of the original weights. Here we take the inverse of the scaled weights calculated above: invscaled_weights &lt;- 1 / scaled_weights 11.4.1 Centrality We begin by calculating a simple weighted degree measure. Here we use a strength() function. The main arguments are: graph = network of interest mode = in, out or all (type of degree) weights = edge weights to use, by default weight attribute on network We begin with the default weights, the count of members in common. deg_normalweights &lt;- strength(club_net96, mode = &quot;all&quot;) And here we use the scaled version of the weights. deg_scaledweights &lt;- strength(club_net96, mode = &quot;all&quot;, weights = scaled_weights) Let's look at the first 10 values for our two centrality calculations: deg_data &lt;- data.frame(deg_normalweights, deg_scaledweights) deg_data[1:10, ] ## deg_normalweights deg_scaledweights ## Academic decathalon 142 31.551195 ## Art Club 87 19.330662 ## Asian Club 29 6.443554 ## Band, 8th 79 17.553130 ## Band, Jazz 39 8.665469 ## Band, Marching (Symphonic) 37 8.221086 ## Baseball, JV (10th) 32 7.110129 ## Baseball, V 27 5.999171 ## Basketball, boys 8th 45 9.998618 ## Basketball, boys 9th 30 6.665746 Note that if we use the degree() function (as we did in other tutorials), this would not use the weights at all, just using a binarized version of the network (where ij = 0 if they have at least one member in common). deg_noweight &lt;- degree(club_net96, mode = &quot;all&quot;) cor(deg_scaledweights, deg_noweight) ## [1] 0.8587135 We can see that the non-weighted version is highly correlated with the weighted calculation but it is not identical. Let's also calculate closeness centrality. The function is closeness() (like in Chapter 9) but here we will include an input for weights. For closeness, we will use the inverted version of the weights, as two clubs with more common members are actually closer. Using the regular weights can lead to some odd results, as we demonstrate below. This is the case as higher edge weights mean more members in common, which in turn means lower distance/higher closeness. But in calculating distance (the first step in calculating closeness), igraph treats the weights in the opposite manner, assuming that higher weights imply higher distances between nodes. We thus adjust for that and use the inverted weights. Here we calculate both versions, first using the inverted weights, and then using the regular weights. close_invweights &lt;- closeness(club_net96, weights = invscaled_weights) close_weights &lt;- closeness(club_net96, weights = scaled_weights) As a quick check let's see how correlated these closeness measures are with degree centrality: cor(deg_normalweights, close_invweights) ## [1] 0.7981294 Using the inverted weights, closeness and degree are highly correlated, as we would expect. What about the correlation between closeness and degree when we do not use the inverted weights? cor(deg_normalweights, close_weights) ## [1] 0.2043294 Using the regular weights, the correlation (between degree and closeness) is lower than what we would expect, suggesting it is problematic to calculate closeness using the regular, not-inverted weighting scheme. Now that we have calculated some example centrality scores, let's get a sense of which clubs tend to be most important to this school, in terms of integrating the school socially. Let's see which clubs are the most central for degree, just looking at the top 10. Let's order the centrality scores from high to low and grab the first 10. deg_top10 &lt;- order(deg_scaledweights, decreasing = T)[1:10] Now, let's see which clubs correspond to those top 10. toptenclubs_degree &lt;- club_names[deg_top10] data.frame(high_degree = toptenclubs_degree) ## high_degree ## 1 Pep Club ## 2 NHS ## 3 Spanish Club ## 4 Drunk Driving ## 5 Latin Club ## 6 Key Club ## 7 Forensics (National Forensics League) ## 8 Orchestra, Symphonic ## 9 Spanish Club (high) ## 10 French Club (high) Let's also take a look at the bottom 10 for degree (setting decreasing to F): deg_bottom10 &lt;- order(deg_scaledweights, decreasing = F)[1:10] bottomtenclubs_degree &lt;- club_names[deg_bottom10] data.frame(high_degree = toptenclubs_degree, low_degree = bottomtenclubs_degree) ## high_degree low_degree ## 1 Pep Club Swim &amp; Dive Team, boys ## 2 NHS Cross Country, girls 8th ## 3 Spanish Club Choir, barbershop quartet (4 men) ## 4 Drunk Driving Cross Country, boys 8th ## 5 Latin Club Tennis girls V ## 6 Key Club Volleyball, JV ## 7 Forensics (National Forensics League) Choir, vocal ensemble (4 women) ## 8 Orchestra, Symphonic Basketball, boys V ## 9 Spanish Club (high) Cheerleaders, 8th ## 10 French Club (high) Tennis, boys V The bottom clubs tend to be sports teams and grade (or gender) specific clubs, as opposed to the more generalist clubs making up the high degree clubs. Finally, it will be useful to consider which club attributes are associated with network centrality. Here, we will consider club profile, showing how much attention the clubs get in the school. Are higher status clubs more/less central to the network? Let's create a new data frame combining club profile with our calculated centrality scores (after turning club_profile into an ordered factor). We will grab club_profile from the igraph object. club_profile &lt;- factor(V(club_net96)$club_profile, ordered = T, levels = c(&quot;low&quot;, &quot;moderate&quot;, &quot;high&quot;, &quot;very_high&quot;)) centrality_data &lt;- data.frame(deg_data, club_profile = club_profile) head(centrality_data) ## deg_normalweights deg_scaledweights club_profile ## Academic decathalon 142 31.551195 low ## Art Club 87 19.330662 low ## Asian Club 29 6.443554 low ## Band, 8th 79 17.553130 low ## Band, Jazz 39 8.665469 moderate ## Band, Marching (Symphonic) 37 8.221086 moderate And now we will use the aggregate() function to calculate the mean and standard deviation of degree for each value of club_profile (low, moderate, high, very high). aggregate(deg_scaledweights ~ club_profile, data = centrality_data, FUN = mean) ## club_profile deg_scaledweights ## 1 low 24.89827 ## 2 moderate 20.30023 ## 3 high 15.04843 ## 4 very_high 14.07213 aggregate(deg_scaledweights ~ club_profile, data = centrality_data, FUN = sd) ## club_profile deg_scaledweights ## 1 low 28.904978 ## 2 moderate 18.779931 ## 3 high 11.708920 ## 4 very_high 8.552113 The results indicate that lower profile clubs tend to have, on average, higher degree centrality than higher profile clubs. Low profile clubs also have higher variance in degree. This suggests that low profile clubs are quite heterogeneous, with some clubs (like the large, generalist service clubs) central to the network, while others (like low status individual sports) are on the outskirts. Centrality for the low profile clubs is thus driven mostly by the type of club, and this can lead to widely varying results. In contrast, highly visible clubs, like Drill or Varsity Cheerleading, have more uniform positions in the network - rarely the very top, central node, but rarely on the outside of the network either. High profile clubs are high status, in part, because they are exclusive, as it is difficult to gain membership. High profile clubs thus tend to be smaller, with a kind of upper limit to how central (in networks terms) they can be. This might be indicative of a network connectivity versus status/exclusivity trade-off. What can we substantively conclude about the organization of this school? First, we can see that central clubs tend to be generalist, serving a potentially wide part of the school. We see things like Pep Club, Spanish Club and National Honor School. These are clubs with relatively low bars for membership and relatively low time commitments. This makes it possible for many people to be members of those clubs, while also simultaneously being part of other clubs in the school. On the other end, we tend to see sports teams and competitive academic teams on the periphery of the club-to-club network. These clubs are a kind of greedy institution, demanding full commitment from its members. This may create internal cohesion, but it also means that sports teams are unlikely to create bridges or to integrate the school as a whole. Members of the tennis team may only see other members of the tennis team. We also see that high profile clubs tend have lower means and variance for degree centrality, suggesting something about the network consequences of being more exclusive. This analysis has focused on centrality, but it is possible to do other kinds of analyses, such as community detection, blockmodeling and the like. 11.4.2 Groups We will now turn to a short analysis of the group structure of this club-to-club network. The basic question is which clubs tend to cluster together, so that a large number of students tend to be members of the same set of clubs. We will use a simple fast and greedy algorithm here, with the weights included as a key input. In this case, we do not want the inverted version of the weights. A higher number of shared members means the clubs are more likely to be in the same group, indicating a stronger relationship between the clubs. We use the scaled version of the weights. groups_scaledweights &lt;- cluster_fast_greedy(club_net96, weights = scaled_weights) Let's see what kinds of clubs are in each group, where a group is defined by a set of clubs that have high (weighted) rates of interaction with each other, defined here by sharing students. To explore this, we will look at the type of clubs that tend to be in each group. We will first put together a little data frame to assist with this, pairing the group memberships with the club name and the type of club (Academic Competition, Team Sports, etc.). group_dat &lt;- data.frame(group = as.numeric(membership(groups_scaledweights)), name = V(club_net96)$name, club_type_detailed = V(club_net96)$club_type_detailed) head(group_dat) ## group name club_type_detailed ## 1 2 Academic decathalon Academic Competition ## 2 4 Art Club Performance Art ## 3 2 Asian Club Ethnic Interest ## 4 5 Band, 8th Performance Art ## 5 2 Band, Jazz Performance Art ## 6 1 Band, Marching (Symphonic) Performance Art And now let's look at the membership of each group, starting with group 1. group_dat[group_dat[, &quot;group&quot;] == 1, ] ## group name club_type_detailed ## 6 1 Band, Marching (Symphonic) Performance Art ## 7 1 Baseball, JV (10th) Team Sports ## 8 1 Baseball, V Team Sports ## 10 1 Basketball, boys 9th Team Sports ## 11 1 Basketball, boys JV Team Sports ## 12 1 Basketball, boys V Team Sports ## 24 1 Choir, barbershop quartet (4 men) Performance Art ## 32 1 Cross Country, boys V Individual Sports ## 40 1 Football, 9th Team Sports ## 41 1 Football, V Team Sports ## 50 1 Golf, boys V Individual Sports ## 66 1 Soccer, V Team Sports ## 73 1 Swim &amp; Dive Team, boys Individual Sports ## 77 1 Tennis, boys V Individual Sports ## 81 1 Track, boys V Individual Sports ## 89 1 Wrestling, V Individual Sports Here we calculate a little table, showing the proportion of clubs (in group 1) that fall into each type of club. prop.table(table(group_dat$club_type_detailed[group_dat$group == 1])) ## ## Individual Sports Performance Art Team Sports ## 0.375 0.125 0.500 It looks like group 1 is made up of mostly sport teams. And now for the rest of the groups: group_dat[group_dat[, &quot;group&quot;] == 2, ] ## group name club_type_detailed ## 1 2 Academic decathalon Academic Competition ## 3 2 Asian Club Ethnic Interest ## 5 2 Band, Jazz Performance Art ## 15 2 Basketball, girls JV Team Sports ## 16 2 Basketball, girls V Team Sports ## 22 2 Chess Club Academic Competition ## 23 2 Choir, a capella Performance Art ## 25 2 Choir, chamber singers Performance Art ## 28 2 Choir, vocal ensemble (4 women) Performance Art ## 30 2 Close-up Academic Interest ## 34 2 Cross Country, girls V Individual Sports ## 38 2 Drunk Driving Officers Service ## 44 2 French Club (high) Academic Interest ## 46 2 French NHS Academic Interest ## 47 2 Full IB Diploma Students (12th) Academic Interest ## 49 2 German NHS Academic Interest ## 52 2 Internships Academic Interest ## 53 2 Junior Class Board Leadership ## 56 2 Newspaper Staff Media ## 57 2 NHS Service ## 64 2 Quiz-Bowl (all) Academic Competition ## 65 2 Science Olympiad Academic Competition ## 68 2 Softball, V Team Sports ## 70 2 Spanish Club (high) Academic Interest ## 71 2 Spanish NHS Academic Interest ## 78 2 Theatre Productions Performance Art ## 79 2 Thespian Society (ITS) Performance Art ## 87 2 Volleyball, V Team Sports ## 90 2 Yearbook Contributors Media ## 91 2 Yearbook Editors Media prop.table(table(group_dat$club_type_detailed[group_dat$group == 2])) ## ## Academic Competition Academic Interest Ethnic Interest Individual Sports Leadership Media Performance Art Service Team Sports ## 0.13333333 0.26666667 0.03333333 0.03333333 0.03333333 0.10000000 0.20000000 0.06666667 0.13333333 Group 2 is centered around academic clubs and performance art. group_dat[group_dat[, &quot;group&quot;] == 3, ] ## group name club_type_detailed ## 35 3 Debate Academic Competition ## 42 3 Forensics Academic Competition ## 43 3 Forensics (National Forensics League) Academic Competition ## 72 3 STUCO Leadership ## 86 3 Volleyball, JV Team Sports prop.table(table(group_dat$club_type_detailed[group_dat$group == 3])) ## ## Academic Competition Leadership Team Sports ## 0.6 0.2 0.2 We can see that group 3 is a small group centered on academic competition. group_dat[group_dat[, &quot;group&quot;] == 4, ] ## group name club_type_detailed ## 2 4 Art Club Performance Art ## 14 4 Basketball, girls 9th Team Sports ## 17 4 Cheerleaders, 8th Team Sports ## 18 4 Cheerleaders, 9th Team Sports ## 19 4 Cheerleaders, JV Team Sports ## 20 4 Cheerleaders, Spirit Squad Team Sports ## 21 4 Cheerleaders, V Team Sports ## 26 4 Choir, concert Performance Art ## 27 4 Choir, treble Performance Art ## 29 4 Choir, women&#39;s ensemble Performance Art ## 36 4 Drill Team Performance Art ## 37 4 Drunk Driving Service ## 45 4 French Club (low) Academic Interest ## 51 4 Hispanic Club Ethnic Interest ## 54 4 Key Club Service ## 55 4 Latin Club Academic Interest ## 58 4 Orchestra, 8th Performance Art ## 59 4 Orchestra, Full Concert Performance Art ## 60 4 Orchestra, Symphonic Performance Art ## 61 4 PEER Service ## 62 4 Pep Club Service ## 63 4 Pep Club Officers Service ## 67 4 Softball, JV (10th) Team Sports ## 69 4 Spanish Club Academic Interest ## 74 4 Swim &amp; Dive Team, girls Individual Sports ## 75 4 Teachers of Tomorrow Academic Interest ## 76 4 Tennis girls V Individual Sports ## 83 4 Track, girls V Individual Sports ## 85 4 Volleyball, 9th Team Sports prop.table(table(group_dat$club_type_detailed[group_dat$group == 4])) ## ## Academic Interest Ethnic Interest Individual Sports Performance Art Service Team Sports ## 0.13793103 0.03448276 0.10344828 0.27586207 0.17241379 0.27586207 Group 4 is a heterogeneous group, with sport teams, performance art, and generalist service clubs (like key club). group_dat[group_dat[, &quot;group&quot;] == 5,] ## group name club_type_detailed ## 4 5 Band, 8th Performance Art ## 9 5 Basketball, boys 8th Team Sports ## 13 5 Basketball, girls 8th Team Sports ## 31 5 Cross Country, boys 8th Individual Sports ## 33 5 Cross Country, girls 8th Individual Sports ## 39 5 Football, 8th Team Sports ## 48 5 German Club Academic Interest ## 80 5 Track, boys 8th Individual Sports ## 82 5 Track, girls 8th Individual Sports ## 84 5 Volleyball, 8th Team Sports ## 88 5 Wrestling, 8th Individual Sports prop.table(table(group_dat$club_type_detailed[group_dat$group == 5])) ## ## Academic Interest Individual Sports Performance Art Team Sports ## 0.09090909 0.45454545 0.09090909 0.36363636 Finally, looking at group 5, we see a small group centered around sports teams (specifically for younger students). Now, let's get a broader picture of the group structure. Let's plot the network but color the nodes based on group membership. V(club_net96)$color &lt;- membership(groups_scaledweights) Let's also highlight those nodes who have high degree. This will let us see how the central nodes fall into different groups. We will color the frame of the top ten nodes red, as a means of highlighting those high degree clubs. V(club_net96)$frame.color[deg_top10] &lt;- rgb(1, 0, 0, .75) Let's plot the network, and use the group ids as the labels. set.seed(105) group_label &lt;- group_dat$group plot(club_net96, main = &quot;Coloring by Groups&quot;, vertex.label = group_label, vertex.label.cex = .75) Note that a few of these clubs don't seem to be placed very well. For example, JV volleyball is quite isolated and difficult to place in a group (the isolated green node). We may consider breaking them out into their own group. Assuming we are satisfied with the analysis, we can proceed to interpret the found groups. The plot makes clear that there are two periphery groups (group 1 and group 5) that consist of sports teams that have low co-membership with other clubs. The remaining groups (2, 3, 4) are more heterogeneous and make up the core of the network, with lots of co-membership across clubs in different groups. We also see that the most central clubs are spread across those three groups. So, it is not the case that all of the most central clubs fall into a single group. Instead, the different groups revolve around different clubs that help integrate that set of students. For example, group 2 is centered around NHS, Spanish club high and French Club high. Group 4 is centered around Spanish Club (low), French Club (low) and Pep club. Roughly speaking then, both group 2 and group 4 are integrative, heterogeneous groups, but one centers more strongly on academic excellence than the other. We thus have a set of membership patterns that generally fall into: exclusively sports (group 1 and 5), sports, service and performance art (group 4), academic competition (group 3) and mixed academic (group 2). 11.5 Using the tnet package We have thus far analyzed our weighted networks by treating the edges as valued, and summing up those valued edges when calculating things like degree centrality. The downside of this approach is that it does not differentiate the weights on the edges from the number of edges. For example, an actor with one very strong relationship could have high weighted degree, perhaps higher than other actors with many (somewhat weaker) relationships. On the other hand, ignoring the weights only captures the number of edges, treating each edge as a 0 or a 1. We can draw on the tnet package to calculate measures that get beyond some of these limitations (Opsahl (2009)). The basic idea is to include a parameter that puts a relative weight on the number of edges compared to the edge weights (strength of relationship). Let's first read in the tnet package. library(tnet) And now we need to get the data in a form that tnet can use. The tnet package takes a weighted edgelist, with the last column capturing the weights. Let's get the edgelist from the igraph object. club_edgelist96 &lt;- as_edgelist(graph = club_net96, names = F) And now we add the scaled weights to the edgelist and put useful columns names onto the matrix. club_edgelist96 &lt;- cbind(club_edgelist96, scaled_weights) colnames(club_edgelist96) &lt;- c(&quot;sender&quot;, &quot;receiver&quot;, &quot;weights&quot;) The tnet package doesn't handle undirected networks especially well. So, before we run any analysis, we need to construct the edgelist so that every edge from i to j has the reverse edge also included (j to i). We will accomplish this by taking the edgelist, switching column 1 and column 2, and then stacking it underneath the current edgelist. club_edgelist96 &lt;- rbind(club_edgelist96, club_edgelist96[, c(2, 1, 3)]) head(club_edgelist96) ## sender receiver weights ## [1,] 1 54 1.9997237 ## [2,] 1 59 0.4443830 ## [3,] 1 62 1.3331491 ## [4,] 1 65 1.3331491 ## [5,] 1 44 0.8887661 ## [6,] 1 60 0.8887661 dim(club_edgelist96) ## [1] 2722 3 We can see that there are 2722 edges (or 1361 undirected edges). Now, we are ready to calculate our weighted centrality measures. For degree, the function is degree_w(). The main arguments are: net = network of interest, as weighted edgelist measure = type of measure: degree calculates measure ignoring the weights; alpha considers the weights type = out or in degree alpha = weight to put on counts compared to edge weights. 0 = all weight on counts; 1 = all weight on edge weights We will first calculate degree putting all of the weight on the edge weights, setting alpha to 1. This means that the number of edges is ignored in the calculation, and we simply sum up all of the weights (i.e., the weights for each edge involving node i). degree_alpha1 &lt;- degree_w(net = club_edgelist96, measure = &quot;alpha&quot;, type = &quot;out&quot;, alpha = 1) head(degree_alpha1) ## node alpha ## [1,] 1 31.551195 ## [2,] 2 19.330662 ## [3,] 3 6.443554 ## [4,] 4 17.553130 ## [5,] 5 8.665469 ## [6,] 6 8.221086 The output is a matrix with two columns, one with the ids and one with the centrality score (the alpha column). This is the same as the calculation in igraph using the weights to calculate degree. We stored those results in deg_scaledweights. Now, let's do the other extreme and put all the weight on the edge count, ignoring the edge weights (alpha = 0). degree_alpha0 &lt;- degree_w(net = club_edgelist96, measure = &quot;alpha&quot;, type = &quot;out&quot;, alpha = 0) head(degree_alpha0) ## node alpha ## [1,] 1 47 ## [2,] 2 40 ## [3,] 3 22 ## [4,] 4 17 ## [5,] 5 26 ## [6,] 6 24 This is the same as the calculation in igraph that ignores the weights, stored in deg_noweight. Finally, let's do something in between and set alpha to .5, putting some weight on both the counts and edge weights. degree_alpha.5 &lt;- degree_w(net = club_edgelist96, measure=&quot;alpha&quot;, type = &quot;out&quot;, alpha = .5) We can check to see how much this affects the results by calculating the correlation between our measures. We need to take the second column from the output, as this is the centrality measure of interest. cor(degree_alpha1[, 2], degree_alpha.5[, 2]) ## [1] 0.97487 cor(degree_alpha0[, 2], degree_alpha.5[, 2]) ## [1] 0.9489498 We can see that in this case setting alpha to .5 offers very similar results to having full or no weight on the edge weights. We now turn to the same exercise for closeness. Here the function is closeness_w(). The main arguments are again net and alpha. We start by setting alpha to 1, putting full weight on the edge weights. Note that in this case we do not have to invert the weights (as we did in igraph) as the algorithm employed here does this by default. close_alpha1 &lt;- closeness_w(net = club_edgelist96, alpha = 1) head(close_alpha1) ## node closeness n.closeness ## [1,] 1 0.010976898 1.219655e-04 ## [2,] 2 0.010183332 1.131481e-04 ## [3,] 3 0.006155898 6.839886e-05 ## [4,] 4 0.011741035 1.304559e-04 ## [5,] 5 0.008258114 9.175683e-05 ## [6,] 6 0.007146988 7.941097e-05 The output is a matrix with three columns: ids, closeness and scaled closeness (closeness / (n - 1)). This is directly analogous (although there are some slight technical differences) to our calculation in igraph using the inverted weights and the closeness() function. This was stored as close_invweights. And now we can do the same thing setting alpha to 0 (ignoring edge weights). close_alpha0 &lt;- closeness_w(club_edgelist96, alpha = 0) head(close_alpha0) ## node closeness n.closeness ## [1,] 1 0.007518797 8.354219e-05 ## [2,] 2 0.007142857 7.936508e-05 ## [3,] 3 0.006250000 6.944444e-05 ## [4,] 4 0.006097561 6.775068e-05 ## [5,] 5 0.006493506 7.215007e-05 ## [6,] 6 0.006410256 7.122507e-05 To get the same calculation in igraph we would do the following. close_igraph_noweight &lt;- closeness(club_net96, weights = NA) Note that weights is set to NA, as the default is to use the weights on the igraph object and we don’t want that here. head(close_igraph_noweight) ## Academic decathalon Art Club Asian Club Band, 8th Band, Jazz Band, Marching (Symphonic) ## 0.007518797 0.007142857 0.006250000 0.006097561 0.006493506 0.006410256 We can see it is the same as the second column above. And again we can set alpha to a value different than 0 or 1. close_alpha.5 &lt;- closeness_w(club_edgelist96, alpha = .5) Let's check the correlation between putting no weight on the edge weights (alpha = 0) and .5 weight: cor(close_alpha0[, 2], close_alpha.5[, 2]) ## [1] 0.8766644 We see the versions of closeness centrality are highly correlated but not identical. 11.6 Racial Segregation In this section, we shift our focus to the attributes of the students and how this maps onto affiliation patterns. Our main question is how strong homophily is along racial lines. Do students tend to be members of clubs with students of the same racial identity? Or is there racial heterogeneity in club memberships? There are a number of ways that we might address this question, and here we take two different approaches. In the first case, we ask how many (weighted) edges connecting students are homophilous. If student i and student j are both members of Spanish Club, are they the same or different racial identity? We will calculate this over all co-membership edges (where student i and student j are in the same club) and then ask what proportion of all (weighted) edges match on race. In the second case, we will calculate a slightly different version of the same homophily statistic; we will calculate how many times a student is in a club with at least one other student of the same race. This captures a kind of baseline level of homophily: do students ever join clubs where they cannot find a student of the same race? Note that these calculations set the stage for our analysis in Chapter 13, where we cover statistical models for two-mode networks. The two versions of racial homophily represent two (extreme) versions of the kinds of homophily terms that can be included in our statistical models. 11.6.1 Proportion of Co-membership Edges where Students Match on Race In this section we walk through the steps to calculate the number of weighted edges that match on race. We begin by grabbing the student-student network from the one mode object constructed above (here we need proj1, rather than proj2). student_net96 &lt;- onemode96$proj1 It will be useful to work with the edgelist version of the network. Here we extract the edgelist from the student-student network, adding the weight column to the data frame. student_edges &lt;- as_edgelist(graph = student_net96) student_edges &lt;- data.frame(student_edges, weight = E(student_net96)$weight) colnames(student_edges)[1:2] &lt;- c(&quot;sender&quot;, &quot;receiver&quot;) And let's take a look at the edgelist: head(student_edges) ## sender receiver weight ## 1 101498 113060 1 ## 2 101498 113939 1 ## 3 101498 114037 3 ## 4 101498 114671 2 ## 5 101498 114679 4 ## 6 101498 114850 3 These edges capture the co-membership ties between students. So, for example, student 101498 and 113060 are in one club together, while 101498 and 114037 are in three together (looking at the weight column). We can also think of these weighted edges as the number of two-paths between student i and j; as i and j are connected in the original, two-mode network through a common club. For example, 101498 and 113060 are both members of NHS, creating a two-path in the two-mode network (101498-NHS-113060) or one edge in the one-mode network (101498-113060). It will be useful to consider the two-path interpretation as the statistical models covered in Chapter 13, Part 3 will specify these kinds of processes using the two-mode specification. In order to calculate if i and j match on race (assuming they are in the same club), we need to get the racial information in a more accessible format. Here we will grab the ids of the students and their racial information and put them together in a data frame. race_dat &lt;- data.frame(name = V(student_net96)$name, race = V(student_net96)$race) head(race_dat) ## name race ## 1 101498 white ## 2 104452 black ## 3 104456 white ## 4 104462 black ## 5 104471 black ## 6 105215 white And now we can go ahead and map the racial information of the nodes onto the edgelist extracted above. The goal is to get the race of the sender and receiver of each edge onto a single data frame, to facilitate our homophily calculation (do i and j identify as the same race?). We will use a match() function to accomplish this. The idea is to match the id from race_dat (based on name) with the sender (or receiver) id from student_edges. We will then grab the race value associated with that sender (or receiver) id. We will do this once for sender and once for receiver, and then put everything together in a data frame. sender_race &lt;- race_dat[match(student_edges[, &quot;sender&quot;], race_dat[, &quot;name&quot;]), &quot;race&quot;] receiver_race &lt;- race_dat[match(student_edges[, &quot;receiver&quot;], race_dat[, &quot;name&quot;]), &quot;race&quot;] student_edges &lt;- data.frame(student_edges, sender_race, receiver_race) head(student_edges) ## sender receiver weight sender_race receiver_race ## 1 101498 113060 1 white white ## 2 101498 113939 1 white black ## 3 101498 114037 3 white black ## 4 101498 114671 2 white white ## 5 101498 114679 4 white Asian ## 6 101498 114850 3 white white We now have the sender id, the receiver id, the weight on the edge, as well as the sender and receiver racial identity. Let's see which of those sender-receiver pairs is the same race: same_race &lt;- student_edges[, &quot;sender_race&quot;] == student_edges[, &quot;receiver_race&quot;] We are now in a position to calculate our statistic of interest, the proportion of weighted edges between students where student i and j are of the same race (or the number of two-paths that match on race). We need two pieces of information: first, the total number of weighted edges that match on race; and second, the total number of weighted edges. The total number of weighted edges can be calculated by summing the weight column in our student_edges data frame. The number of weighted edges that match on race is based on the same calculation but restricted to where sender_race is the same as receiver_race, identified above. sum(student_edges[same_race == T, &quot;weight&quot;]) / sum(student_edges[, &quot;weight&quot;]) ## [1] 0.4675944 This means that about .468 of all connections between i and j match on race. This suggests that many co-membership ties exist between students of different racial identities, consistent with the figure constructed above. Note that this result might be partly driven by generalist clubs (like NHS) which tend to be large, racially heterogeneous entities. Such clubs would have a disproportionate impact on increasing the number of two-paths that mismatch on race. We will consider this more carefully in Chapter 13, Part 3, where we will examine racial homophily, while controlling for other factors, like differences in club size. 11.6.2 Do Students Match Racially with at Least One Other Student? We now turn to our second set of analyses, where we ask how often students match, racially, with at least one other student in the club (they are a member of). Here it will be easier to work with the two-mode version of the network. We will construct an edgelist with the sender as the students and the receiver as the clubs. We set names to F, as we do not want the node labels, instead outputting the node number in the edgelist (this will make it easier to map race onto the sender/receiver for that edge). club_student_edges &lt;- as_edgelist(graph = affil_net96, names = F) colnames(club_student_edges)[1:2] &lt;- c(&quot;sender&quot;, &quot;receiver&quot;) head(club_student_edges) ## sender receiver ## [1,] 1 995 ## [2,] 1 1008 ## [3,] 1 1009 ## [4,] 1 1016 ## [5,] 1 1017 ## [6,] 2 944 And now we will grab the race attributes of the nodes from the igraph object (the two-mode version of the network): race2 &lt;- V(affil_net96)$race We now need to determine for each edge, student-club, if the student is the same race as at least one other member of the club. There are a number of ways we could go about calculating this. Here we will lay out each step in a loop (which might be a slow option for a large network). The basic logic is to first grab the race of the focal student (in the edge in question). We then grab the race of all other students in that club (excluding the student of interest). We then ask if the race of the focal student matches any student in the club. This is done over all student-club edges in the network and stored in an object called match_race. match_race &lt;- NA for (i in 1:nrow(club_student_edges)){ # grabbing race of student in edge: send_race &lt;- race2[club_student_edges[i, &quot;sender&quot;]] # getting race of students in club of interest, excluding student in edge: club_id &lt;- club_student_edges[i, &quot;receiver&quot;] edges_to_club &lt;- club_student_edges[-i, &quot;receiver&quot;] == club_id student_ids &lt;- club_student_edges[-i, ][edges_to_club, &quot;sender&quot;] club_race_composition &lt;- race2[student_ids] # asking if race of focal student race matches any other student in club: match_race[i] &lt;- send_race %in% club_race_composition } head(match_race) ## [1] TRUE TRUE TRUE TRUE TRUE TRUE This tells us, for example, that for the first edge, the student (101498) is the same race as at least one other student in the club (NHS). And now we can calculate the overall proportion that match, calculated over all student-club edges in the network: prop.table(table(match_race)) ## match_race ## FALSE TRUE ## 0.02309731 0.97690269 This suggests that almost all students have at least one other student in their clubs that matches on racial identity. In this way, students are very unlikely to be the sole, 'outsider' differing on race with all other members of the club. This is particularly interesting in the case of smaller racial groups, like Native American or Asian, where, by chance, we might expect them to join clubs and be the only other person of their racial identity. The fact that this rarely happens is telling. Substantively then, our two analyses suggest that students are likely to be in clubs where some of the students are of a different race (based on the first calculation), but are very much unlikely to find themselves racially isolated in a club (based on the second calculation). In this way, students are unlikely to join clubs where there is a bad mismatch between their race and the race of the other students; but it does mean that a club can have a mix of different racial groups. Of course, some of the racial matching might arise by chance and it is important to determine how much of the racial homophily is driven solely by the overall composition of the school (as well as other factors of interest, like the tendency for students to join similar types of clubs). We return to this in Chapter 13, Part 3, statistical models for two-mode networks. 11.7 Club Centrality and Attributes As a final example, we connect the analysis of club centrality to the analysis of student attribute segregation. In Section 4.1 we determined which clubs were more/less central to the network. Here, we reexamine this question, while incorporating information about the student attributes (see Section 11.6.1 and 11.6.2 above). The basic question is whether more central clubs tend be more or less segregated in terms of student attributes, like grade, gender or race. Are more peripheral clubs more homogeneous? And more formally, is the proportion of students with the same grade (or gender) higher in central, bridging clubs or in peripheral, exclusive ones? For example, we might think that the proportion matching would be higher (so more segregated) in the periphery, harder to access clubs. If so, that would suggest that central clubs are locales for multiple grades and genders, acting as integrating spaces across key student identities. Our goal is to calculate the proportion of student pairs within each club that have the same gender or grade. We will then correlate the proportion matching with the centrality scores previously calculated. As a first step, we need to determine which students are in which clubs. To accomplish this, we will create a list, where each element shows the student ids in the club in question. We will work with the raw affiliation matrix, affiliations96_nomiss. For each column in the matrix (i.e., each club), we will identify which students have a \"1\", and are thus members of the club. We will do this for all clubs using apply() and setting MARGIN to 2 (in order to repeat the function for each column). We set the function of interest using the FUN input. student_id_list &lt;- apply(affiliations96_nomiss, MARGIN = 2, FUN = function(x) which(x == 1)) The output is a list, one entry (of student ids) for each club. Let's look at the first two clubs: student_id_list[1:2] ## $`Academic decathalon` ## 113429 114920 115229 115460 118699 122638 122662 126330 128412 133022 133962 138751 139362 141015 148535 167506 167508 888892 888898 888947 888965 888976 888988 889001 889016 889036 889039 889048 889085 ## 34 72 85 88 126 196 208 290 351 424 453 504 526 576 657 693 695 727 733 782 800 811 823 836 851 871 874 883 920 ## ## $`Art Club` ## 119336 121389 122619 122671 122672 122683 122684 122687 126259 126281 126782 129101 129354 133959 138539 139362 141149 149824 888890 888900 888901 888930 888947 888948 888959 888978 888985 888991 889016 889028 889071 889080 889083 ## 135 160 181 217 218 225 226 228 286 289 306 366 384 452 496 526 592 665 725 735 736 765 782 783 794 813 820 826 851 863 906 915 918 We see the students who are in the Academic decathlon club and the Art Club. The numbers correspond to the rows in the affiliation matrix for the student in question. And now we need to write a little function to calculate the proportion of students who match on grade or gender in a given club. This is a little different than what we accomplished with the racial segregation analysis above (11.6.2), where we asked if a particular student in a club matched or not with other students in the club. Here, we ask what proportion of all student pairs in a club match on the attribute of interest: do student i and student j (over all i,j in the club) have the same grade, gender, etc.? Our function will take two arguments: first, a vector of student attributes (grade or gender) for all students in the school; and second, the ids of the students in the club of interest. Note that the input ids are not based on the labels, but refer to the rows in the affiliation matrix, as stored in student_id_list. prop_same_func &lt;- function(attribute, ids){ #Arguments: #attribute: vector of attributes #ids: ids of nodes # subset attributes to just students in club of interest club_subset &lt;- attribute[ids] # calculate number in each category tab_attribute &lt;- table(club_subset) # calculate number of students in club num_students &lt;- sum(tab_attribute) # calculate number of pairs that match; accomplished # by squaring and summing the number in each category # while also adjusting for diagonal, # as we do not want to count the number of # times i matches with self num_match &lt;- sum(tab_attribute ^ 2) - num_students # now calculating proportion that match, by dividing # number that match by total number of pairs, excluding # diagonal (where i is the same student) prop_same &lt;- num_match / (num_students * (num_students - 1)) } And now we will apply our function to the vector of gender values for our students. We will first grab gender from the igraph object (just for students, so setting type to FALSE). We then apply our function over each set of students ids, one for each club; this is stored in student_id_list. We use lapply() as we want to run the function over each element of student_id_list. gender &lt;- V(affil_net96)$gender[V(affil_net96)$type == FALSE] same_gender_prop &lt;- lapply(student_id_list, FUN = prop_same_func, attribute = gender) It will be useful to have the calculated values stored as a numeric object (rather than a list), so let's go ahead and do that using an unlist() function. same_gender_prop &lt;- unlist(same_gender_prop) head(same_gender_prop) ## Academic decathalon Art Club Asian Club Band, 8th Band, Jazz Band, Marching (Symphonic) ## 0.4876847 0.4886364 0.5714286 0.4975845 0.5588235 0.5384615 And we can see that we have a numeric vector showing the proportion of student pairs in the club that match on gender. So, for example, .488 of all pairs in Academic decathlon are the same gender. And now we can go ahead and do a simple correlation analysis. We will correlate the centrality scores, based on degree centrality, with our same gender proportion measure. cor(deg_normalweights, same_gender_prop) ## [1] -0.4342763 And now we do the same thing for grade: grade &lt;- V(affil_net96)$grade96[V(affil_net96)$type == FALSE] same_grade_prop &lt;- lapply(student_id_list, FUN = prop_same_func, attribute = grade) same_grade_prop &lt;- unlist(same_grade_prop) cor(deg_normalweights, same_grade_prop) ## [1] -0.279257 It looks like less central clubs, do in fact, have higher levels of segregation, being more homogeneous in terms of gender and grade (especially for gender). We see a negative correlation, so that more central clubs have lower proportion matching. This suggests that the affiliation network is held together by low commitment ('blow-off') clubs that anyone can join and this serves to bring disparate group interests (e.g., on gender, on age and skill) into contact with one another, even if fleeting. The more demanding, distinguishing activities play to core identities and are more exclusive. As such, the affiliation network reinforces identity-selection and differentiation but provides means for secondary association and integration across those interests via easy to access clubs. In this way, intermittent locales arise to maintain overall integration. This lab on two-mode networks has included a wide variety of analyses, opening up important questions about the memberships that individual students take on. We could push this analysis further by considering these processes together, in a single model. For example, how much does the composition of the clubs (gender, race, etc.) matter when students decide which clubs to join? Or is it simply about the clubs type (sports, academic, etc.)? The larger goal would be to tease out the decision rules that student use in joining clubs. See Chapter 13 for examples. Further analyses could also look more closely at the kinds of trajectories that students take on and the consequences for such different career paths within the school. This tutorial has covered the analysis of two-mode data. In Chapter 12, we will take up related issues of duality when analyzing cultural (or textual) data. We will cover topic modeling and latent spaces. "],["ch12-Networks-Culture-R.html", "12 Networks and Culture", " 12 Networks and Culture In this chapter we apply network ideas and techniques to cultural data. The first tutorial focuses on cultural structures, applying topic modeling and network measures to textual data. The second tutorial focuses on cultural spaces, and utilizes correspondence analysis to analyze data on dissertation topics. "],["ch12-Networks-Structure-Culture-text-R.html", "12, Part 1. Cultural Structures 12.1 Getting the Data Ready 12.2 Topic Modeling 12.3 A Network Representation", " 12, Part 1. Cultural Structures This is the first tutorial for Chapter 12, covering the application of network ideas to the analysis of cultural data. This tutorial will build directly on the material from Chapter 11, on two-mode data. In Chapter 11, we analyzed typical affiliation data, where the rows were actors and the columns organizations. In this tutorial, we will be working with textual data, applying the ideas of duality to a very different kind of data source, one based on words in documents. We will walk through the basics of topic modeling in R. Topic modeling is a natural language processing technique that uncovers the latent topics that structure the words used in a set of documents. We will cover the following: basic data management of textual data; modeling textual data using LDA (latent Dirichlet allocation); and representing textual data as a network. For our empirical case, we analyze textual data based on a set of sociology abstracts (drawn from recent dissertations). We are interested in discovering the latent topics that exist in the data, where each topic is defined by having a distinct pattern of words associated with it. We are also interested in seeing which abstracts get placed together and why. In this way we are trying to uncover the underlying structure of the field of sociology (as represented in abstracts), where certain words and researchers are associated with a topic and certain topics are closer to each other than others. We thus see the intuition of a network approach played out using textual data. As this is a textbook on network analysis, we will not focus on the technical details of topic modeling; instead, we will focus on the substantive application of network ideas to textual data. For those interested in more technical details on topic models, see the topicmodels documentation. And for information on text mining and textual analysis in general, see Feinerer, Hornik, and Meyer (2008). 12.1 Getting the Data Ready We will need a number of packages to analyze our textual data, here dissertation abstracts in sociology. We will need: NLP (basic processing of natural language), tm (text mining package), SnowballC (word stemming package), topicmodels (package for modeling the topics), and ldatuning (package for evaluating the models). Let's go ahead and load them all. library(NLP) library(tm) library(SnowballC) library(topicmodels) library(ldatuning) Note that topic modeling is computationally heavy and can require a long run time. The data we will use for this lab is only a very small (random) sample of the original corpus of dissertations. If this was an analysis for an actual paper, we would want to use the whole set of data, or at least a much larger sample. Let's read in the data, treating our text strings as strings rather than as categories: url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/sociologysample.csv&quot; abstracts &lt;- read.csv(file = url1, stringsAsFactors = FALSE) We can use the str() function to take a look at the data. str(abstracts) ## &#39;data.frame&#39;: 199 obs. of 2 variables: ## $ obs : int 39628 39745 49780 18794 17548 6848 34226 12683 1170 34437 ... ## $ text: chr &quot;Ontogenic variables, such as the experience of the parent&#39;s abuse as a child and current_depression or substanc&quot;| __truncated__ &quot;This study_analyzes attitudes of area residents toward mineral_extraction and processing in the Northern Great_&quot;| __truncated__ &quot;This dissertation is an experiment in thinking with the story, not about the story in order to erase_the_bounda&quot;| __truncated__ &quot;The concepts of parent_empowerment and participation, currently popular in the disability field, that emerge fr&quot;| __truncated__ ... We see there are two columns (obs and text), the first for the observation and the second for the actual text of the abstract. There are 199 abstracts in the data. For example, let’s look at the text of the first abstract. abstracts[1, &quot;text&quot;] ## [1] &quot;Ontogenic variables, such as the experience of the parent&#39;s abuse as a child and current_depression or substance abuse, were expected_to_have_a_greater_impact on the risk of child_abuse than microsystem and exosystem_variables such as family functioning, domestic violence, income, community safety, and social support. It was expected that social_support would have the greatest_impact of these broader system variables. It is believed that attachment_style_mediates the relationship between the ontogenic system level and microsystem/exosystem level variables. Secondary data were used to examine these systemic impacts. The dataset was obtained from the National_Data_Archive on Child Abuse and Neglect. The sample_consisted of 265 women, the majority of whom were African-American and who had a high_school_education or GED. The majority of these women were employed, while others were in school or a training_program at the time of the interview. Over 50 percent_of_the_sample had never been married. A multiple_regression_analysis_was_conducted_to_examine_the_impact of factors from three systems levels on the risk of child abuse. Variables were entered into the multiple_regression equation after they had demonstrated significance in bi-variate analyses with Bonferroni corrections. Mother&#39;s age was entered first in this equation in order to control for any potential effect of this variable as it has been shown_to_be_significantly_related_to_risk of abuse. Entered in the second step were the ontogenic variables found to be significant: depression and locus of control. In the third and final step, the microsystem and exosystem_variables found_to_have_a_significant_impact on the risk of abuse were entered: the number of social supports available to the mother, the mother&#39;s assessment_of_family_functioning and the frequency of domestic_violence perpetrated against her. Locus of control and mother&#39;s depression were demonstrated to be the most important predictors in the equation. Domestic violence_and_social_support were the next most important predictors, with family_functioning becoming non-significant in this equation. This supports the hypothesis that ontogenic variables have the greater_impact in predicting risk of physical abuse. A path_analysis_was_conducted_in_order_to_examine the possible time order and causal_nature of these variables. All variables were included in this analysis. The only variables found to impact the risk of physical_abuse were depression and locus of control. The only route from experiencing childhood_sexual_abuse to the risk of committing child_physical_abuse was through the level of depression currently being experienced by the mother. Mother&#39;s locus_of_control impacted the risk of abuse directly as well as through its impact on depression. Risk of abuse was not influenced directly by the experience_of_sexual abuse.&quot; We can see that this is literally the text of the dissertation abstract, word for word. This is the data that we want to analyze. Before we can analyze the data, we need to create a corpus based on the words used in the abstracts. We basically need to transform the raw text data into a set of words (for each abstract) that are directly comparable across abstracts. We want to know which words are used together frequently and which abstracts are using which words. We thus need to have the data in a format where such comparisons are possible. By creating a Corpus object from our abstracts, we will be able to use various functions in R that are designed to standardize text data. Here we will use a Corpus() function. Note that the Corpus() function will not take text data directly as input. We thus need to use a VectorSource() function within the main function, with the input as the text of interest (abstracts$text). abstracts_corp &lt;- Corpus(VectorSource(abstracts$text)) length(abstracts_corp) ## [1] 199 The length is 199, one for each of the abstracts in our original data. It will be useful to clean the corpus a bit before we actually try to analyze it. Textual data can be messy. We need to make the inputs (abstracts in this case) as uniform as possible, to facilitate comparison. We will go through a series of steps on how to streamline the textual data, making the abstracts consistent and informative. For example, let's change everything to lower case. If one dissertation uses the word \"network\" and another uses \"Network\", we don’t want to treat them as having used different words. We will make use of the tm_map() function to clean up the corpus. The inputs are the corpus and the function we want to use (telling R how to change the text). Here we will set the function to tolower. abstracts_corp &lt;- tm_map(abstracts_corp, tolower) Let's take a look at the first abstract: abstracts_corp[[1]]$content ## [1] &quot;ontogenic variables, such as the experience of the parent&#39;s abuse as a child and current_depression or substance abuse, were expected_to_have_a_greater_impact on the risk of child_abuse than microsystem and exosystem_variables such as family functioning, domestic violence, income, community safety, and social support. it was expected that social_support would have the greatest_impact of these broader system variables. it is believed that attachment_style_mediates the relationship between the ontogenic system level and microsystem/exosystem level variables. secondary data were used to examine these systemic impacts. the dataset was obtained from the national_data_archive on child abuse and neglect. the sample_consisted of 265 women, the majority of whom were african-american and who had a high_school_education or ged. the majority of these women were employed, while others were in school or a training_program at the time of the interview. over 50 percent_of_the_sample had never been married. a multiple_regression_analysis_was_conducted_to_examine_the_impact of factors from three systems levels on the risk of child abuse. variables were entered into the multiple_regression equation after they had demonstrated significance in bi-variate analyses with bonferroni corrections. mother&#39;s age was entered first in this equation in order to control for any potential effect of this variable as it has been shown_to_be_significantly_related_to_risk of abuse. entered in the second step were the ontogenic variables found to be significant: depression and locus of control. in the third and final step, the microsystem and exosystem_variables found_to_have_a_significant_impact on the risk of abuse were entered: the number of social supports available to the mother, the mother&#39;s assessment_of_family_functioning and the frequency of domestic_violence perpetrated against her. locus of control and mother&#39;s depression were demonstrated to be the most important predictors in the equation. domestic violence_and_social_support were the next most important predictors, with family_functioning becoming non-significant in this equation. this supports the hypothesis that ontogenic variables have the greater_impact in predicting risk of physical abuse. a path_analysis_was_conducted_in_order_to_examine the possible time order and causal_nature of these variables. all variables were included in this analysis. the only variables found to impact the risk of physical_abuse were depression and locus of control. the only route from experiencing childhood_sexual_abuse to the risk of committing child_physical_abuse was through the level of depression currently being experienced by the mother. mother&#39;s locus_of_control impacted the risk of abuse directly as well as through its impact on depression. risk of abuse was not influenced directly by the experience_of_sexual abuse.&quot; We can see all the words are now lower case. We also need to deal with some odd mistakes in the data. We can see from the first abstract that the raw textual data sometimes has multiple words stuck together. For example, in the first abstract current and depression are put together as current_depression. That does not yield a meaningful word, so we need to split those words up into separate words, whenever that happens. In this case, we need to write our own little function (called split_words()) to perform this task. This function will make use of content_transformer(), which is a function used to create text transformations that are not already built into R. Here, we will write a little function within content_transformer() that will find all the _ and replace them with a space. This is accomplished using the gsub() function. The main arguments to gsub() are pattern (the pattern to look for) and replacement (the replacement text for the specified pattern). x is the text of interest. gsub_function &lt;- function(x, pattern) gsub(pattern, replacement = &quot; &quot;, x) split_words &lt;- content_transformer(gsub_function) And here we apply our function to the corpus using tm_map(). abstracts_corp &lt;- tm_map(abstracts_corp, split_words, pattern = &quot;_&quot;) Let’s check to make sure it worked, looking at the first abstract: abstracts_corp[[1]]$content ## [1] &quot;ontogenic variables, such as the experience of the parent&#39;s abuse as a child and current depression or substance abuse, were expected to have a greater impact on the risk of child abuse than microsystem and exosystem variables such as family functioning, domestic violence, income, community safety, and social support. it was expected that social support would have the greatest impact of these broader system variables. it is believed that attachment style mediates the relationship between the ontogenic system level and microsystem/exosystem level variables. secondary data were used to examine these systemic impacts. the dataset was obtained from the national data archive on child abuse and neglect. the sample consisted of 265 women, the majority of whom were african-american and who had a high school education or ged. the majority of these women were employed, while others were in school or a training program at the time of the interview. over 50 percent of the sample had never been married. a multiple regression analysis was conducted to examine the impact of factors from three systems levels on the risk of child abuse. variables were entered into the multiple regression equation after they had demonstrated significance in bi-variate analyses with bonferroni corrections. mother&#39;s age was entered first in this equation in order to control for any potential effect of this variable as it has been shown to be significantly related to risk of abuse. entered in the second step were the ontogenic variables found to be significant: depression and locus of control. in the third and final step, the microsystem and exosystem variables found to have a significant impact on the risk of abuse were entered: the number of social supports available to the mother, the mother&#39;s assessment of family functioning and the frequency of domestic violence perpetrated against her. locus of control and mother&#39;s depression were demonstrated to be the most important predictors in the equation. domestic violence and social support were the next most important predictors, with family functioning becoming non-significant in this equation. this supports the hypothesis that ontogenic variables have the greater impact in predicting risk of physical abuse. a path analysis was conducted in order to examine the possible time order and causal nature of these variables. all variables were included in this analysis. the only variables found to impact the risk of physical abuse were depression and locus of control. the only route from experiencing childhood sexual abuse to the risk of committing child physical abuse was through the level of depression currently being experienced by the mother. mother&#39;s locus of control impacted the risk of abuse directly as well as through its impact on depression. risk of abuse was not influenced directly by the experience of sexual abuse.&quot; It looks right. We can see that current and depression are now separate words, rather than current_depression. Let’s do a similar splitting based on words put together with a /. abstracts_corp &lt;- tm_map(abstracts_corp, split_words, pattern = &quot;/&quot;) Let's also remove punctuation as this is not substantively useful. The function here is removePunctation(). This will take out commas, periods, apostrophes and so on. In this case we do not need to use a content_transformer function as removePunctuation() is a built-in transformation in the tm package. abstracts_corp &lt;- tm_map(abstracts_corp, removePunctuation) Let's also remove all of the numbers from the corpus. abstracts_corp &lt;- tm_map(abstracts_corp, removeNumbers) Now let's designate a set of words to remove. For example, we may want to remove commonly used words that don't add much to differentiate word use in the abstracts. For example, if everyone uses \"is\" it may make sense to remove it from all abstracts. Here we can use the stopwords() function (set to english) to get commonly used words. stopwords(&quot;english&quot;) ## [1] &quot;i&quot; &quot;me&quot; &quot;my&quot; &quot;myself&quot; &quot;we&quot; &quot;our&quot; &quot;ours&quot; &quot;ourselves&quot; &quot;you&quot; &quot;your&quot; &quot;yours&quot; &quot;yourself&quot; &quot;yourselves&quot; &quot;he&quot; &quot;him&quot; &quot;his&quot; &quot;himself&quot; &quot;she&quot; &quot;her&quot; &quot;hers&quot; &quot;herself&quot; &quot;it&quot; ## [23] &quot;its&quot; &quot;itself&quot; &quot;they&quot; &quot;them&quot; &quot;their&quot; &quot;theirs&quot; &quot;themselves&quot; &quot;what&quot; &quot;which&quot; &quot;who&quot; &quot;whom&quot; &quot;this&quot; &quot;that&quot; &quot;these&quot; &quot;those&quot; &quot;am&quot; &quot;is&quot; &quot;are&quot; &quot;was&quot; &quot;were&quot; &quot;be&quot; &quot;been&quot; ## [45] &quot;being&quot; &quot;have&quot; &quot;has&quot; &quot;had&quot; &quot;having&quot; &quot;do&quot; &quot;does&quot; &quot;did&quot; &quot;doing&quot; &quot;would&quot; &quot;should&quot; &quot;could&quot; &quot;ought&quot; &quot;i&#39;m&quot; &quot;you&#39;re&quot; &quot;he&#39;s&quot; &quot;she&#39;s&quot; &quot;it&#39;s&quot; &quot;we&#39;re&quot; &quot;they&#39;re&quot; &quot;i&#39;ve&quot; &quot;you&#39;ve&quot; ## [67] &quot;we&#39;ve&quot; &quot;they&#39;ve&quot; &quot;i&#39;d&quot; &quot;you&#39;d&quot; &quot;he&#39;d&quot; &quot;she&#39;d&quot; &quot;we&#39;d&quot; &quot;they&#39;d&quot; &quot;i&#39;ll&quot; &quot;you&#39;ll&quot; &quot;he&#39;ll&quot; &quot;she&#39;ll&quot; &quot;we&#39;ll&quot; &quot;they&#39;ll&quot; &quot;isn&#39;t&quot; &quot;aren&#39;t&quot; &quot;wasn&#39;t&quot; &quot;weren&#39;t&quot; &quot;hasn&#39;t&quot; &quot;haven&#39;t&quot; &quot;hadn&#39;t&quot; &quot;doesn&#39;t&quot; ## [89] &quot;don&#39;t&quot; &quot;didn&#39;t&quot; &quot;won&#39;t&quot; &quot;wouldn&#39;t&quot; &quot;shan&#39;t&quot; &quot;shouldn&#39;t&quot; &quot;can&#39;t&quot; &quot;cannot&quot; &quot;couldn&#39;t&quot; &quot;mustn&#39;t&quot; &quot;let&#39;s&quot; &quot;that&#39;s&quot; &quot;who&#39;s&quot; &quot;what&#39;s&quot; &quot;here&#39;s&quot; &quot;there&#39;s&quot; &quot;when&#39;s&quot; &quot;where&#39;s&quot; &quot;why&#39;s&quot; &quot;how&#39;s&quot; &quot;a&quot; &quot;an&quot; ## [111] &quot;the&quot; &quot;and&quot; &quot;but&quot; &quot;if&quot; &quot;or&quot; &quot;because&quot; &quot;as&quot; &quot;until&quot; &quot;while&quot; &quot;of&quot; &quot;at&quot; &quot;by&quot; &quot;for&quot; &quot;with&quot; &quot;about&quot; &quot;against&quot; &quot;between&quot; &quot;into&quot; &quot;through&quot; &quot;during&quot; &quot;before&quot; &quot;after&quot; ## [133] &quot;above&quot; &quot;below&quot; &quot;to&quot; &quot;from&quot; &quot;up&quot; &quot;down&quot; &quot;in&quot; &quot;out&quot; &quot;on&quot; &quot;off&quot; &quot;over&quot; &quot;under&quot; &quot;again&quot; &quot;further&quot; &quot;then&quot; &quot;once&quot; &quot;here&quot; &quot;there&quot; &quot;when&quot; &quot;where&quot; &quot;why&quot; &quot;how&quot; ## [155] &quot;all&quot; &quot;any&quot; &quot;both&quot; &quot;each&quot; &quot;few&quot; &quot;more&quot; &quot;most&quot; &quot;other&quot; &quot;some&quot; &quot;such&quot; &quot;no&quot; &quot;nor&quot; &quot;not&quot; &quot;only&quot; &quot;own&quot; &quot;same&quot; &quot;so&quot; &quot;than&quot; &quot;too&quot; &quot;very&quot; Here we remove any word in that list from each abstract using the removeWords() function. abstracts_corp &lt;- tm_map(abstracts_corp, removeWords, stopwords(&quot;english&quot;)) Now, let's add a few more words to our stopword list. Again, we want to remove words that are not differentiating for the corpus at hand (here abstracts from sociology dissertations). We will add the following words that were not in the default stop list. myStopwords &lt;- c(&quot;dissertation&quot;, &quot;chapter&quot;, &quot;chapters&quot;, &quot;research&quot;, &quot;researcher&quot; ,&quot;researchers&quot; ,&quot;study&quot;, &quot;studies&quot;, &quot;studied&quot;, &quot;studys&quot;, &quot;studying&quot;, &quot;one&quot;, &quot;two&quot;, &quot;three&quot;) We tried to include words that are not informative for this corpus, as they are generic and widely used. For example, many abstracts may mention a \"chapter one\" but that does not make it substantively important that chapter and one are used together frequently. We could, of course, imagine a slightly different list, and we must be aware that are our results will differ if we exclude different words. We now go ahead and remove those words from the corpus. abstracts_corp &lt;- tm_map(abstracts_corp, removeWords, myStopwords) Now, let's take out any redundant whitespace between words using the stripWhitespace() function. abstracts_corp &lt;- tm_map(abstracts_corp, stripWhitespace) Finally, we will stem the document using a stemDocument() function. This reduces similar words to a single stem word. For example, test and testing would be reduced to test. The idea is that they convey the same basic meaning and should be treated as the same. abstracts_corp &lt;- tm_map(abstracts_corp, stemDocument) Let's again take a look at the first abstract: abstracts_corp[[1]]$content ## [1] &quot;ontogen variabl experi parent abus child current depress substanc abus expect greater impact risk child abus microsystem exosystem variabl famili function domest violenc incom communiti safeti social support expect social support greatest impact broader system variabl believ attach style mediat relationship ontogen system level microsystem exosystem level variabl secondari data use examin system impact dataset obtain nation data archiv child abus neglect sampl consist women major africanamerican high school educ ged major women employ other school train program time interview percent sampl never marri multipl regress analysi conduct examin impact factor system level risk child abus variabl enter multipl regress equat demonstr signific bivari analys bonferroni correct mother age enter first equat order control potenti effect variabl shown signific relat risk abus enter second step ontogen variabl found signific depress locus control third final step microsystem exosystem variabl found signific impact risk abus enter number social support avail mother mother assess famili function frequenc domest violenc perpetr locus control mother depress demonstr import predictor equat domest violenc social support next import predictor famili function becom nonsignific equat support hypothesi ontogen variabl greater impact predict risk physic abus path analysi conduct order examin possibl time order causal natur variabl variabl includ analysi variabl found impact risk physic abus depress locus control rout experienc childhood sexual abus risk commit child physic abus level depress current experienc mother mother locus control impact risk abus direct well impact depress risk abus influenc direct experi sexual abus&quot; After all of this you end up with a string of word stems with many common words removed. This is done for each input (here abstract) in the corpus. Note that the cleaning process is fraught with difficulties. For example, stemming is far from perfect and can lead to some unexpected results. For example, experiment and experience have the same stem (experi) while child and children do not. This could be what you want, but it may not. More generally, synonyms will not be treated as the same word (i.e., stemming will not capture the idea that kid and child are often used to capture similar ideas). Similarly, we have to be careful about the choice of words to include in the stop list. We want to remove words that are not differentiating, but how long of a list we should construct and which words should be included are difficult (context-specific) questions. Assuming we are satisfied with the cleaning process, we are now in a position to create a document-term matrix. We need to create a document-term matrix as this will serve as input to our LDA model, where the words and abstracts are placed into latent topics. The document-term matrix is a complex object, capturing how many times each document (the rows) used a particular term (the columns). We will use a DocumentTermMatrix() function, with the corpus as input. abstracts_dtm &lt;- DocumentTermMatrix(abstracts_corp) abstracts_dtm ## &lt;&lt;DocumentTermMatrix (documents: 199, terms: 4298)&gt;&gt; ## Non-/sparse entries: 20228/835074 ## Sparsity : 98% ## Maximal term length: 25 ## Weighting : term frequency (tf) We can see that there are 199 documents and 4298 unique terms in our document-term matrix. The values in the matrix correspond to the number of times each document (i.e., abstract) used a given term (i.e., word). 20228 of the values in the matrix are non-sparse, or greater than 1, and 835074 are sparse, equal to 0; where a 0 means that the document did not use the term. Thus, about 98% of the possible 'ties' between documents and terms do not actually exist (835074 / (20228 + 835074)), suggesting that many words are not used widely across abstracts. We also see that the longest word in the corpus is length 25 (i.e., 25 letters). We can use the inspect() function to take a look at particular documents or terms. Here we look at the first abstract: inspect(abstracts_dtm[1, ]) ## &lt;&lt;DocumentTermMatrix (documents: 1, terms: 4298)&gt;&gt; ## Non-/sparse entries: 114/4184 ## Sparsity : 97% ## Maximal term length: 25 ## Weighting : term frequency (tf) ## Sample : ## Terms ## Docs abus child control depress enter impact mother risk support variabl ## 1 14 5 5 6 4 9 6 9 5 12 By default, inspect will print the top ten terms used. We can see here that the first document used the word 'abus' 14 times, 'child' 5 times, and so on. We can also use inspect to look at the columns. Here we look at the top ten documents that use the term risk: inspect(abstracts_dtm[, &quot;risk&quot;]) ## &lt;&lt;DocumentTermMatrix (documents: 199, terms: 1)&gt;&gt; ## Non-/sparse entries: 11/188 ## Sparsity : 94% ## Maximal term length: 4 ## Weighting : term frequency (tf) ## Sample : ## Terms ## Docs risk ## 1 9 ## 116 1 ## 117 2 ## 118 1 ## 143 1 ## 173 1 ## 19 1 ## 53 2 ## 63 1 ## 75 1 We can see that the term 'risk' is used 9 times in the first document, 1 time in the 116th document and so on. We can also use the Terms() function to get all of the terms (i.e., words) used and the Docs() function to get all of the document ids (here abstracts). Here we look at the first 6 terms. head(Terms(abstracts_dtm)) ## [1] &quot;abus&quot; &quot;africanamerican&quot; &quot;age&quot; &quot;analys&quot; &quot;analysi&quot; &quot;archiv&quot; Finally, it may be of use to extract the actual matrix from the document-term matrix. Here we apply as.matrix() on the document-term matrix: mat_abstract_words &lt;- as.matrix(abstracts_dtm) dim(mat_abstract_words) ## [1] 199 4298 We can see there are 199 rows (documents) and 4298 columns (terms). And let's take a look at the first five rows and columns: mat_abstract_words[1:5, 1:5] ## Terms ## Docs abus africanamerican age analys analysi ## 1 14 1 1 1 3 ## 2 0 0 0 0 3 ## 3 1 0 0 0 3 ## 4 0 0 0 0 0 ## 5 0 0 0 1 0 We can see, for example, that the first document used 'abus' 14 times (same as we saw above). We can take this matrix and calculate summary measures. For example, we can calculate how many different terms the first document used (by asking how many times the first row of the matrix is greater than 0): sum(mat_abstract_words[1, ] &gt; 0) ## [1] 114 Note that the document-term matrix is analogous to the affiliation matrices we saw in the previous tutorial, where students were on the rows and clubs were on the columns. Here, with textual data, we have documents on the rows and terms on the columns. 12.2 Topic Modeling Now, we want to analyze our document-term matrix, applying topic models to the text-based data. We will utilize LDA, latent Dirichlet allocation. LDA attempts to uncover the underlying, or latent, topics in the corpus of interest. Different (latent) topics create different word use and we can use the co-occurrence of words in a document to uncover which words hang together under a given topic. A topic will have a high probability of yielding a set of words when those words are used together at high rates. In a similar way, we can ask which abstracts are likely to fall into which topic, based on their distribution of word choice. In many ways, this is conceptually similar to the positional analysis from Chapter 10, where nodes were placed in the same position if they had the same pattern of ties to other nodes. Here, two abstracts are likely to be in the same topic if they use the same set of words. 12.2.1 LDA: Initial Model We need to set a few parameters before we can run the model. LDA utilizes Gibbs sampling, a randomized algorithm for obtaining a sequence of observations from a multivariate probability distribution. This sequence will be used to approximate the joint distribution of topics and words. Here we set the key inputs to the algorithm: burnin &lt;- 200 # number of omitted Gibbs iterations at beginning iter &lt;- 3000 # number of iterations thin &lt;- 2000 # number of omitted iterations between each kept iteration seed &lt;- list(2003, 5,63, 100001, 765) #seeds to enable reproducibility nstart &lt;- 5 # number of repeated random starts best &lt;- TRUE # only continue model on the best model The model also requires that a researcher set the number of topics prior to estimation (similar to setting the number of clusters in Chapter 10). Here we will set the number at 5, noting that this is a pretty arbitrary choice. We will consider more principled ways of setting the number of topics below. k &lt;- 5 Now we are ready to run LDA using Gibbs sampling. The function is LDA(). The main arguments are: x = document term matrix k = number of topics method = either VEM or Gibbs control = list of control input ldaOut &lt;- LDA(x = abstracts_dtm, k = k, method = &quot;Gibbs&quot;, control=list(nstart = nstart, seed = seed, best = best, burnin = burnin, iter = iter, thin = thin)) Now let's explore the results. First, we can use a topics() function to extract the most likely topic for each abstract. ldaOut_topics &lt;- topics(ldaOut) head(ldaOut_topics) ## 1 2 3 4 5 6 ## 1 2 5 1 2 5 This suggests that the first abstract is most likely to fall into topic 1, the second is likely to fall into topic 2 and so on. We can also extract more nuanced information, looking at the probabilities of each abstract going into each latent topic. Thus, rather than just looking at the most likely topic, we can see the relative probability of a given abstract belonging to a given topic. The probabilities can be extracted using @gamma on the model object: topicProbabilities &lt;- ldaOut@gamma head(topicProbabilities) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.59589041 0.09246575 0.1404110 0.1061644 0.06506849 ## [2,] 0.11111111 0.32659933 0.2053872 0.2424242 0.11447811 ## [3,] 0.09623431 0.14225941 0.1589958 0.1715481 0.43096234 ## [4,] 0.22147651 0.20805369 0.1610738 0.1946309 0.21476510 ## [5,] 0.11034483 0.61379310 0.1000000 0.1000000 0.07586207 ## [6,] 0.14644351 0.17991632 0.1799163 0.1422594 0.35146444 We can see that the first abstract has a 0.596 probability of being in topic 1 and much lower probabilities for the other topics. Of course, at this point we do not know anything about what the latent topics correspond to. So, let's go ahead and take a look at the topics. Here we will look at the most likely words associated with each latent topic. This amounts to finding the probability that a given topic will yield a given word and selecting those words with the highest probability. We use the terms() function with the lda object as input, as well as the number of terms to consider (here set to 10). ldaOut_terms &lt;- terms(ldaOut, 10) ldaOut_terms ## Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 ## [1,] &quot;parent&quot; &quot;state&quot; &quot;social&quot; &quot;women&quot; &quot;cultur&quot; ## [2,] &quot;famili&quot; &quot;polit&quot; &quot;model&quot; &quot;student&quot; &quot;ident&quot; ## [3,] &quot;children&quot; &quot;organ&quot; &quot;work&quot; &quot;educ&quot; &quot;social&quot; ## [4,] &quot;relationship&quot; &quot;communiti&quot; &quot;use&quot; &quot;particip&quot; &quot;ethnic&quot; ## [5,] &quot;behavior&quot; &quot;develop&quot; &quot;interact&quot; &quot;american&quot; &quot;examin&quot; ## [6,] &quot;mother&quot; &quot;servic&quot; &quot;theori&quot; &quot;role&quot; &quot;nation&quot; ## [7,] &quot;support&quot; &quot;system&quot; &quot;relat&quot; &quot;school&quot; &quot;within&quot; ## [8,] &quot;signific&quot; &quot;econom&quot; &quot;develop&quot; &quot;black&quot; &quot;case&quot; ## [9,] &quot;child&quot; &quot;polici&quot; &quot;effect&quot; &quot;group&quot; &quot;practic&quot; ## [10,] &quot;result&quot; &quot;local&quot; &quot;individu&quot; &quot;experi&quot; &quot;movement&quot; We can see that topic 4 (for example) yields words like women, student and education, while topic 1 yields words like parent, family, and children. Note that we can get the actual probabilities for the terms using the posterior() function. More generally, it is important to see that the topics are simultaneously constituted by the set of words that are used together and the abstracts that tend to use those words, analogous to the ideas explored in the tutorial on two-mode, affiliation networks. 12.2.2 LDA: Picking the Number of Topics Our initial analysis set the number of latent topics to 5. But we could just as easily have used a different number, yielding a different set of results. The issue of how to determine the appropriate numbers of topics is a difficult one, in part, because there is no single, universally accepted way of doing this. One approach is to redo the analysis using different numbers of topics, interpreting the latent topics at different scales, or resolutions. A quick overview of a corpus may require 10 topics, while something more refined may require 25. We may need 100 topics for a really detailed topical inspection. This strategy is fairly robust (as one does not have to pick a single number of topics) but is also somewhat unsatisfying. It would be nice to have a simple set of results that we can point to as our best estimate of what the true latent topics are. For this, we need a systematic way of finding the optimal number of topics for the corpus. Here we can make use of the functions in the ldatuning package. The basic idea is to rerun the model using increasingly higher number of topics. As k increases, we calculate different fit statistics as a means of comparison, eventually picking the one with the best fit. There are a number of possible fit statistics we could use. Here we utilize two, one suggested by Arun et al. (2010) and the other by Juan et al. (2009). For example, Arun et al. (2010) suggest selecting the model (i.e., number of latent topics) that minimizes the cosine distance between topics. The main function is FindTopicNumber(). The main arguments are: dtm = document-term matrix topics = vector of number of topics to consider metrics = which metrics to use to evaluate different topic numbers method = estimation routine, same as with LDA control = list of controls, same as with LDA mc.cores = number of cores to utilize Here, we rerun the model, varying the number of topics from 4 to 40 (doing it every other number to save time). We set the metrics to \"CaoJuan2009\" and \"Arun2010\". Note that this can take a bit to run. fitmodel &lt;- FindTopicsNumber(dtm = abstracts_dtm, topics = seq(from = 4, to = 40, by = 2), metrics = c(&quot;CaoJuan2009&quot;, &quot;Arun2010&quot;), method = &quot;Gibbs&quot;, control = list(nstart = 1, seed = c(30), best = best, burnin = burnin, iter = iter, thin = thin), mc.cores = 2, verbose = TRUE) fitmodel ## topics CaoJuan2009 Arun2010 ## 1 40 0.06359749 322.2180 ## 2 38 0.06015966 325.9417 ## 3 36 0.06441433 329.1180 ## 4 34 0.06263168 333.9887 ## 5 32 0.06309096 340.8725 ## 6 30 0.05739855 342.1717 ## 7 28 0.06420134 352.2448 ## 8 26 0.06329599 361.7429 ## 9 24 0.06020666 365.6039 ## 10 22 0.06228409 372.4103 ## 11 20 0.07153718 387.0902 ## 12 18 0.06673595 398.3489 ## 13 16 0.06531436 410.1995 ## 14 14 0.07709557 428.3069 ## 15 12 0.07686266 445.5500 ## 16 10 0.06734270 462.3836 ## 17 8 0.08508528 493.6755 ## 18 6 0.08656703 528.9791 ## 19 4 0.11489348 584.0326 Once we fit the model under different numbers of topics, we can go ahead and plot the fit statistics. The function is FindTopicsNumber_plot(). FindTopicsNumber_plot(fitmodel) Based on our two fit statistics of interest, we might choose a number of topics around 30. The Arun statistic is technically minimized at 40, but given that the CaoJaun fit statistic gets worse after 30 (and the Arun statistic has basically plateaued at this point), we may be in better shape using k at 30. If we were interested in examining a less disaggregated solution, setting k to 10 or 16 would be appropriate. We will now go ahead and rerun the same model as before (with the original control parameters) but set k to 30. k &lt;- 30 ldaOut2 &lt;- LDA(x = abstracts_dtm, k = k, method = &quot;Gibbs&quot;, control = list(nstart = nstart, seed = seed, best = best, burnin = burnin, iter = iter, thin = thin)) And now let's look at the results. We will focus on the terms, showing the most likely terms for each topic (i.e., the terms that each topic is most likely to yield). ldaOut_terms2 &lt;- terms(ldaOut2, 10) Let's look at a few example columns (out of the 30 total): ldaOut_terms2[, c(2, 14, 20, 23, 24)] ## Topic 2 Topic 14 Topic 20 Topic 23 Topic 24 ## [1,] &quot;involv&quot; &quot;japanes&quot; &quot;parent&quot; &quot;women&quot; &quot;program&quot; ## [2,] &quot;school&quot; &quot;technolog&quot; &quot;children&quot; &quot;sexual&quot; &quot;use&quot; ## [3,] &quot;educ&quot; &quot;resourc&quot; &quot;mother&quot; &quot;men&quot; &quot;data&quot; ## [4,] &quot;parent&quot; &quot;right&quot; &quot;child&quot; &quot;gender&quot; &quot;effect&quot; ## [5,] &quot;particip&quot; &quot;firm&quot; &quot;famili&quot; &quot;style&quot; &quot;measur&quot; ## [6,] &quot;interview&quot; &quot;japan&quot; &quot;relationship&quot; &quot;experi&quot; &quot;influenc&quot; ## [7,] &quot;teacher&quot; &quot;arrow&quot; &quot;adjust&quot; &quot;race&quot; &quot;find&quot; ## [8,] &quot;scienc&quot; &quot;foreign&quot; &quot;support&quot; &quot;also&quot; &quot;perform&quot; ## [9,] &quot;percept&quot; &quot;learn&quot; &quot;age&quot; &quot;famili&quot; &quot;high&quot; ## [10,] &quot;learn&quot; &quot;invest&quot; &quot;matern&quot; &quot;factor&quot; &quot;increas&quot; What can we conclude substantively from our analysis? First, we can see that some of these topics are easier to interpret than others. Topic 2, for example, clearly centers on school outcomes (school, education, involve, parent). Other topics are a bit harder to parse, and may be an indication that we need to take another look at our model (or text cleaning decisions). For example, Topic 24 is defined by pretty broad, general terms (program, use, data, effect) and is not easy to label. In such cases, it may be useful to examine the abstracts directly to understand why they had been put together. Second, we can see that many of the topics do seem to center on recognizable research areas in the discipline. Topic 23, for example, is about women, gender and sexuality, while Topic 20 focuses on the family. Other topics cover more narrowly defined areas, like Topic 14 that is centered on Japan and investment firms. As a researcher, we would want to walk through the latent topics, interpreting each one and drawing out the larger implications for the substantive case in question. Given these kinds of results, a researcher could also try to predict which researchers fall into which latent topic; for example, based on type of university they attended, gender, etc. Similarly, we could ask how being in a given topic is associated with later career success (i.e., finding a job after graduation). 12.3 A Network Representation A researcher may also be interested in producing a network representation of the textual data. This makes it possible to apply network measures, models and visualizations to the corpus. So far we have cleaned the data, modeled the data using LDA but we have not explicitly constructed a network object. Here we go ahead and do that, creating a two-mode network of abstracts by words. Let’s load the igraph package. library(igraph) We will make use of the abstracts by words matrix extracted earlier. Let’s take a look at the first five rows and columns. The values in the matrix show the number of times that abstract used that word. mat_abstract_words[1:5, 1:5] ## Terms ## Docs abus africanamerican age analys analysi ## 1 14 1 1 1 3 ## 2 0 0 0 0 3 ## 3 1 0 0 0 3 ## 4 0 0 0 0 0 ## 5 0 0 0 1 0 In this case, let’s focus on just a subset of the full matrix. We will look at the network of abstracts and words associated with topic 20 (\"family\") and topic 23 (\"gender\"). Thus, we are using the LDA results above (specifically the latent topics) to inform our analysis here. Let’s grab the vector showing which topic each abstract is most likely to be in. ldaOut_topics2 &lt;- topics(ldaOut2) Now we subset the full abstract-word matrix to just include those abstracts that are most likely to be in topic in 20 or 23. in_20_23 &lt;- ldaOut_topics2 %in% c(20, 23) mat_abstract_words_subset &lt;- mat_abstract_words[in_20_23, ] Let’s also reduce the words a bit as well. We will only consider words that are used frequently in topics 20 and 23. This will simplify the picture, eliminating words that are not so relevant for these two topics. Here we calculate the number of times each word was used. worduse &lt;- colSums(mat_abstract_words_subset) Let’s only keep those words that were used more than 5 times. mat_abstract_words_subset &lt;- mat_abstract_words_subset [, worduse &gt; 5] dim(mat_abstract_words_subset) ## [1] 24 159 We have now reduced our matrix to 24 abstracts and 159 words. We are now in a position to construct the abstract-word network. We will construct a two-mode network, where there are two types of nodes (abstracts and words) and abstracts are connected to words (and vice versa) but there are no direct ties between nodes of the same type. An edge exists if abstract i used word j. We will use the graph_from_incidence() function, setting mode to all (creating mutual connections between abstract and words) and keeping the weights (based on the number of times that abstract i used word j). abstract_word_net &lt;- graph_from_incidence_matrix(mat_abstract_words_subset, mode = &quot;all&quot;, weighted = T) We will now grab the vertex attribute type from the two-mode graph object: type &lt;- vertex_attr(abstract_word_net, &quot;type&quot;) table(type) ## type ## FALSE TRUE ## 24 159 We see that there are 24 abstracts (FALSE) and 159 words (TRUE). Now, let’s go ahead and plot the network, to see what we can learn from turning the textual data into a network. We first set some plotting parameters. Let’s first set the words green: V(abstract_word_net)$color[type == TRUE] &lt;- rgb(red = 0, green = 1, blue = 0, alpha = .2) Note that we just change the color for those nodes where type is equal to TRUE (the words). Now let’s set the color for the abstracts. Here we want to differentiate the two topics. Let’s color topic 20 (family) blue and topic 23 (gender) red. In order to do this we need to identify which of the abstracts are in topic 20 and which are in topic 23. We must also remember that many of the nodes are words (type equal to TRUE). First, let’s get the names of the abstracts in each topic: in20 &lt;- names(which(ldaOut_topics2 == 20)) in23 &lt;- names(which(ldaOut_topics2 == 23)) Now we set all those in topic 20 to blue: which_topic20 &lt;- V(abstract_word_net)$name %in% in20 V(abstract_word_net)$color[which_topic20] &lt;- rgb(red = 0, green = 0, blue = 1, alpha = .2) Note that we grabbed the vertex names from the network and then asked which matched those in topic 20. And here we do the same thing with topic 23, setting it to red. which_topic23 &lt;- V(abstract_word_net)$name %in% in23 V(abstract_word_net)$color[which_topic23] &lt;- rgb(red = 1, green = 0, blue = 0, alpha = .2) Now we'll set some other plotting arguments. V(abstract_word_net)$label &lt;- V(abstract_word_net)$name V(abstract_word_net)$label.color &lt;- rgb(0, 0, .2, .85) V(abstract_word_net)$label.cex &lt;- .75 V(abstract_word_net)$size &lt;- 3 V(abstract_word_net)$frame.color &lt;- V(abstract_word_net)$color Here we set the color of the edges. E(abstract_word_net)$color &lt;- rgb(.5, .5, .5, .04) And now we plot the network. set.seed(106) plot(abstract_word_net, layout = layout_with_fr) What can we learn from the graph? First, we can see that the blue nodes (topic 20) tend to cluster together and the red nodes (topic 23) tend to cluster together. This suggests that the topics found in the LDA do appear to capture meaningful distinctions amongst the abstracts. Second, we can see that there is a set of core words associated with each topic. We see words like family, children, relationship, parent, and mother cluster together close to the blue nodes, those in topic 20. We see words like women, gender, and sexuality cluster together close to the red nodes, those in topic 23. This is in line with our previous analysis but here we get the results arranged in a spatial layout. Third, we see that some the abstracts are on the periphery of the network. For example, abstract 109 (in topic 23) does not use the words at the core of the network very heavily. Instead, abstract 109 uses words like legal and speech that are rarely used by any of the abstracts in the gender (or family) topic. Fourth, the plot makes clear that there is at least some overlap in usage of words across these topics. Most of the blue nodes (the abstracts in topic 20) and the red nodes (abstracts in topic 23) are reasonably close together in the plot and the plot as a whole is quite dense. This suggests that while the usage of the core words is different across topics, there is much in common as well. A family paper is likely to mention gender (or women) even if its main concern is about family, children and the like. On the other hand, the periphery words (i.e. those words that are rarely used) are unlikely to be shared between the two topics. In this way, a topic (or subfield) carves out a niche by emphasizing certain key words that are of general interest, while also employing topic specific language that is unlikely to be shared more widely. Given this kind of initial picture, a researcher could go on to do additional analyses on the network, asking about things like centrality, group structure and the like. For example, we may be interested in examining which words serve to bridge our two topics. Overall, this tutorial has offered an initial glimpse of how to analyze textual data in R, while offering an example of how duality could be applied to a very different type of data source. We will draw on many of these ideas in the next tutorial for Chapter 12, where we cover cultural spaces (using correspondence analysis). "],["ch12-Networks-Cultural-Spaces-CA-R.html", "12, Part 2. Cultural Spaces 12.4 Correspondence Analysis 12.5 Number of Dimensions 12.6 Importance of Rows/Columns 12.7 Hierarchical Clustering", " 12, Part 2. Cultural Spaces This is the second tutorial for Chapter 12, covering the application of network ideas to cultural data. In this tutorial we cover spatial approaches to analyzing cultural data, which offer a useful, 'birds-eye' view of the system in question. Methods like multi-dimensional scaling and correspondence analysis produce a map of the data, showing which objects cluster together and which do not. We will mostly cover correspondence analysis, which is a method for placing multiple types of objects into the same latent space. Correspondence analysis is thus ideally suited for two-mode data and for capturing ideas of duality. This tutorial thus builds directly on the material covered in Chapter 11. Our substantive case is based on the dissertation topics of PhD students in sociology and related disciplines. See Chapter 12, Part 1 for an analysis of the abstracts themselves. Here, we have data on the topic and department of students who submitted a dissertation between 1990 and 2013 (limited to topics typically studied by sociologists). Our main question is how students in different departments fall into different topics and what structures this relationship. We also want to know which topics are 'close' to which other topics (where topics are close if they are covered by the same kinds of departments) and which departments are close to which other departments (where departments are close if they cover the same kinds of topics). We want to understand the department/topic structure in a holistic manner, capturing the department-department, topic-topic and department-topic relationships simultaneously. It is important to recognize that the data are not network data per se (just counts of people in departments and topics) but we will analyze such data from a relational, network perspective, applying ideas of duality, position, distance and the like. 12.4 Correspondence Analysis Let's start by loading some useful packages. library(FactoMineR) library(factoextra) library(CAinterprTools) library(gplots) library(plot3D) library(NbClust) Now we will read in the data. url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/department_topic_data.txt&quot; department_topic &lt;- read.table(file = url1, header = T) Let's take a look at the data (here just for the first 4 rows and columns). department_topic[1:4, 1:4] ## LIFE_COURSE SOCIOLOGY_OF_DEVIANCE SOCIOLOGY_OF_RACE MEDICAL_SOCIOLOGY ## SBS__Sociology 742 715 445 525 ## H__Languages__Societies_and_Cultures 80 103 136 51 ## SBS__Social_and_Developmental_Psychology 75 165 63 33 ## H__History 49 7 47 12 The rows correspond to the department of the student (with the prefix corresponding to department type: SBS is social behavioral sciences; H is humanities; Ed is the education school; B is business; HS is health sciences). The columns correspond to the main topic of the dissertation. The values are the number of people in that department and topic. So 742 students submitted a dissertation on the life course and were in a sociology department. dim(department_topic) ## [1] 25 33 We have 25 departments and 33 topics. Let’s grab the department and topic names, as this will be useful later on. department_names &lt;- rownames(department_topic) topic_names &lt;- colnames(department_topic) We will start with a basic correspondence analysis (CA) to get a sense of what these analyses yield. For a quick primer on correspondence analysis, see (Sourial et al. 2009). Correspondence analysis will place both the rows (departments) and the columns (topics) into the same space. Departments, for example, will be close together if students from those departments cover the same kinds of topics; i.e., they are structurally equivalent (see Chapter 10). [More formally, correspondence analysis works by taking chi-square distances between rows/columns (i.e., a weighted distance between rows based on the probabilities of falling into each column) and decomposing them into a smaller number of dimensions to create a plot where the distances between points approximates those raw chi-square distances.] The main function is CA(). The main argument is X, which sets the data frame of interest (here department_topic). We also set graph to FALSE as we do not want to plot anything here. ca_mod1 &lt;- CA(X = department_topic, graph = FALSE) ca_mod1 ## **Results of the Correspondence Analysis (CA)** ## The row variable has 25 categories; the column variable has 33 categories ## The chi square of independence between the two variables is equal to 20620.07 (p-value = 0 ). ## *The results are available in the following objects: ## ## name description ## 1 &quot;$eig&quot; &quot;eigenvalues&quot; ## 2 &quot;$col&quot; &quot;results for the columns&quot; ## 3 &quot;$col$coord&quot; &quot;coord. for the columns&quot; ## 4 &quot;$col$cos2&quot; &quot;cos2 for the columns&quot; ## 5 &quot;$col$contrib&quot; &quot;contributions of the columns&quot; ## 6 &quot;$row&quot; &quot;results for the rows&quot; ## 7 &quot;$row$coord&quot; &quot;coord. for the rows&quot; ## 8 &quot;$row$cos2&quot; &quot;cos2 for the rows&quot; ## 9 &quot;$row$contrib&quot; &quot;contributions of the rows&quot; ## 10 &quot;$call&quot; &quot;summary called parameters&quot; ## 11 &quot;$call$marge.col&quot; &quot;weights of the columns&quot; ## 12 &quot;$call$marge.row&quot; &quot;weights of the rows&quot; The output is a CA object containing the overall fit of the model (eig), the results for the columns (col) and the results for the rows (row). Note that by default up to 5 dimensions will be kept in the results. This can be changed using the ncp argument. Let's use a summary() function to take a look at the results. Here we will tell R to print 2 decimal places (using nb.dec), print the first five rows/columns (setting nbelements to 5) and print 3 dimensions (setting ncp to 3). summary(ca_mod1, nb.dec = 2, nbelements = 5, ncp = 3) ## ## Call: ## CA(X = department_topic, graph = FALSE) ## ## The chi square of independence between the two variables is equal to 20620.07 (p-value = 0 ). ## ## Eigenvalues ## Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 Dim.7 Dim.8 Dim.9 Dim.10 Dim.11 Dim.12 Dim.13 Dim.14 Dim.15 Dim.16 Dim.17 Dim.18 Dim.19 Dim.20 Dim.21 Dim.22 Dim.23 Dim.24 ## Variance 0.13 0.07 0.05 0.03 0.03 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## % of var. 33.21 16.65 13.48 7.94 6.29 4.65 3.41 2.73 2.58 1.85 1.67 1.31 1.11 0.86 0.79 0.42 0.35 0.25 0.17 0.15 0.07 0.04 0.02 0.01 ## Cumulative % of var. 33.21 49.87 63.35 71.29 77.58 82.23 85.64 88.37 90.95 92.80 94.47 95.77 96.89 97.74 98.53 98.95 99.30 99.55 99.72 99.87 99.94 99.97 99.99 100.00 ## ## Rows (the 5 first) ## Iner*1000 Dim.1 ctr cos2 Dim.2 ctr cos2 Dim.3 ctr cos2 ## SBS__Sociology | 3.39 | -0.01 0.03 0.01 | 0.05 2.26 0.45 | -0.01 0.04 0.01 | ## H__Languages__Societies_and_Cultures | 12.86 | 0.09 0.43 0.04 | -0.17 2.85 0.15 | 0.14 2.43 0.10 | ## SBS__Social_and_Developmental_Psychology | 27.21 | -0.55 12.96 0.63 | -0.02 0.03 0.00 | 0.25 6.36 0.13 | ## H__History | 46.28 | 0.99 25.41 0.73 | -0.08 0.34 0.00 | 0.18 2.11 0.02 | ## SBS__Clinical_Psychology | 28.00 | -0.74 12.66 0.60 | 0.09 0.35 0.01 | 0.40 9.07 0.18 | ## ## Columns (the 5 first) ## Iner*1000 Dim.1 ctr cos2 Dim.2 ctr cos2 Dim.3 ctr cos2 ## LIFE_COURSE | 0.55 | 0.03 0.01 0.03 | 0.10 0.43 0.53 | -0.04 0.08 0.08 | ## SOCIOLOGY_OF_DEVIANCE | 18.69 | -0.57 6.89 0.49 | 0.13 0.71 0.03 | 0.40 8.13 0.24 | ## SOCIOLOGY_OF_RACE | 2.97 | 0.06 0.05 0.02 | -0.23 1.46 0.33 | 0.04 0.05 0.01 | ## MEDICAL_SOCIOLOGY | 21.03 | -0.32 1.55 0.10 | 0.46 6.49 0.21 | 0.14 0.73 0.02 | ## CHILD_DEVELOPMENT | 15.13 | -0.52 7.06 0.62 | -0.11 0.64 0.03 | 0.09 0.51 0.02 | These results are a bit hard to interpret but there is lots of useful information here. For example, the eigenvalue output tells us how much of the variance in the data is explained by 1, 2, 3, etc. dimensions. If we use two dimensions (for example) we could explain about 50% of the total variance. We will return to this later on. The row and column output includes (amongst other things) the locations of each department and topic, showing which departments/topics are close together. This is easier to see in a plot. We can plot the results using fviz_ca_biplot(). fviz_ca_biplot(ca_mod1) We can change some of the inputs to make it easier to interpret, here changing the size and color of the labels. We make departments orange and topics blue. fviz_ca_biplot(ca_mod1, col.row = &quot;orange&quot;, col.col = &quot;steelblue&quot;, labelsize = 3, repel = T) + theme_minimal() Note that the plot only includes the first two dimensions, and that both topics and departments are included, with different colors as a means of differentiating the type of object. Departments are close to topics when those departments have a high proportion of their students doing those topics. For example, students in public health, nursing and medicine tend to take on topics related to medical sociology. Substantively, what have we learned from the plot? First, we may be interested in the overall structure of the space, or the latent dimensions in our two dimensional plot. Looking at the x-axis, we see the far left is constituted by topics like child development and deviance and departments like clinical psychology. On the far right, we see topics like social movements and historical sociology and departments like political science and history. The left-right continuum thus runs from micro-level studies of adolescence to macro-level studies of politics and history. Looking at the y-axis, the bottom includes topics like culture and education and includes education and humanities departments. The top is constituted by topics like medical sociology and stratification and departments like economics and public policy. The y-axis thus runs (more or less) from culture to stratification/health. We thus have a two-dimensional space of micro to macro and culture to stratification. Second, we may be interested in which topics/departments tend to cluster together. For example, the top left quadrant includes medical sociology topics and public health departments. We see sociology of aging close by, as well as topics related to stratification. We also see that some departments are closer together than others. Public health is closer to nursing and public policy than it is to English or history. This means that the topics that public health students tend to take on are very different than those in the humanities. It is worth noting that the college that these departments are affiliated with only sometimes predicts which department are close together. For example, the departments in the social behavioral sciences (SBS) are pretty spread out, all the way from the far left (social and developmental psychology) to the far right (political science). Thus, political science has more in common, topic wise, with history (in the humanities) than fellow social sciences like clinical psychology. We can also clean the plot up a bit to focus on just the rows or just the columns. If we wanted to just have the rows on the plot we would use fviz_ca_row() (or fviz_ca_col() for just the columns): fviz_ca_row(ca_mod1, col.row = &quot;orange&quot;, labelsize = 3, repel = T) + theme_minimal() 12.5 Number of Dimensions We have so far run an initial correspondence analysis and interpreted the results. Here we want to take a closer look at the analysis, to make sure we have the proper set of results. There are a number of issues we need to consider. First, we need to decide how many dimensions are sufficient to represent the space. This means we need to decide on the appropriate number of dimensions to use when interpreting the results. We need to balance fit (more dimensions) with ease of interpretation and presentation (fewer dimensions). Remember that the eigenvalues tell us how much each dimension contributes to explaining the total variance in the data. So let's start by looking at the eigenvalues for each dimension more closely. eig &lt;- get_eigenvalue(ca_mod1) eig ## eigenvalue variance.percent cumulative.variance.percent ## Dim.1 1.332917e-01 33.212926405 33.21293 ## Dim.2 6.683516e-02 16.653627519 49.86655 ## Dim.3 5.410294e-02 13.481083156 63.34764 ## Dim.4 3.185661e-02 7.937859321 71.28550 ## Dim.5 2.524612e-02 6.290692990 77.57619 ## Dim.6 1.867087e-02 4.652307510 82.22850 ## Dim.7 1.368130e-02 3.409034054 85.63753 ## Dim.8 1.095477e-02 2.729651666 88.36718 ## Dim.9 1.036783e-02 2.583399490 90.95058 ## Dim.10 7.408050e-03 1.845898466 92.79648 ## Dim.11 6.710398e-03 1.672061276 94.46854 ## Dim.12 5.237564e-03 1.305068324 95.77361 ## Dim.13 4.471813e-03 1.114262479 96.88787 ## Dim.14 3.433716e-03 0.855595118 97.74347 ## Dim.15 3.153639e-03 0.785806937 98.52927 ## Dim.16 1.680187e-03 0.418660133 98.94793 ## Dim.17 1.422663e-03 0.354491571 99.30243 ## Dim.18 9.880741e-04 0.246203022 99.54863 ## Dim.19 6.808799e-04 0.169658023 99.71829 ## Dim.20 6.083445e-04 0.151584047 99.86987 ## Dim.21 2.616822e-04 0.065204580 99.93508 ## Dim.22 1.465193e-04 0.036508898 99.97158 ## Dim.23 8.747367e-05 0.021796222 99.99338 ## Dim.24 2.656286e-05 0.006618791 100.00000 By first inspection, the model would appear to perform okay with a small number of dimensions, although we may be concerned that the first 2 dimensions only explains about 50% of the variance (looking at column 3, cumulative.variance.percent). Let's produce a scree plot to make the comparisons clearer. fviz_screeplot(ca_mod1) The y-axis shows the percent of total variance explained as we add a dimension and the x-axis shows the number of dimensions. We can see that additional variance explained seems to drop off after the third dimension, so there would appear to be diminishing returns to adding a fourth, fifth, etc. dimension. We may opt for a 2 or 3 dimensional solution based on the plot. Another potentially useful approach is to apply the average rule heuristic, which chooses the number of dimensions by selecting all dimensions that explain more than the average dimension. The function is aver.rule() and the input is the raw data (not the CA object). aver.rule(department_topic) The dashed line indicates the average variance explained and the bars show the percent variance explained for each dimension. The average rule would suggest a solution of 6 dimensions. But if we take a closer look we can see that dimensions 5 and 6 only add around 5% to the explained variance, a fairly low return on adding an entire dimension to the plot. So, we may think that the average rule tends to yield solutions with too many dimensions, at least in this case. How about Malinvaud's test? This sequential test checks the significance of the remaining dimensions once the first k ones have been selected. The function is malinvaud() and the input is the raw data. malinvaud(department_topic) ## K Dimension Eigenvalue Chi-square df p-value p-class ## 1 0 dim. 1 1.332917e-01 20620.073926 768 0.000000e+00 p &lt; 0.001 ## 2 1 dim. 2 6.683516e-02 13771.543949 713 0.000000e+00 p &lt; 0.001 ## 3 2 dim. 3 5.410294e-02 10337.553643 660 0.000000e+00 p &lt; 0.001 ## 4 3 dim. 4 3.185661e-02 7557.744330 609 0.000000e+00 p &lt; 0.001 ## 5 4 dim. 5 2.524612e-02 5920.951870 560 0.000000e+00 p &lt; 0.001 ## 6 5 dim. 6 1.867087e-02 4623.806325 513 0.000000e+00 p &lt; 0.001 ## 7 6 dim. 7 1.368130e-02 3664.497077 468 0.000000e+00 p &lt; 0.001 ## 8 7 dim. 8 1.095477e-02 2961.551735 425 0.000000e+00 p &lt; 0.001 ## 9 8 dim. 9 1.036783e-02 2398.695543 384 1.039277e-287 p &lt; 0.001 ## 10 9 dim. 10 7.408050e-03 1865.996659 345 1.036095e-206 p &lt; 0.001 ## 11 10 dim. 11 6.710398e-03 1485.371030 308 3.065673e-153 p &lt; 0.001 ## 12 11 dim. 12 5.237564e-03 1140.590759 273 2.492875e-106 p &lt; 0.001 ## 13 12 dim. 13 4.471813e-03 871.484706 240 1.658228e-72 p &lt; 0.001 ## 14 13 dim. 14 3.433716e-03 641.722959 209 1.659357e-45 p &lt; 0.001 ## 15 14 dim. 15 3.153639e-03 465.298613 180 3.872782e-27 p &lt; 0.001 ## 16 15 dim. 16 1.680187e-03 303.264642 153 5.705095e-12 p &lt; 0.001 ## 17 16 dim. 17 1.422663e-03 216.936613 128 1.532274e-06 p &lt; 0.001 ## 18 17 dim. 18 9.880741e-04 143.840189 105 7.114028e-03 p &lt; 0.01 ## 19 18 dim. 19 6.808799e-04 93.072944 84 2.335258e-01 p &gt; 0.05 ## 20 19 dim. 20 6.083445e-04 58.089334 65 7.156535e-01 p &gt; 0.05 ## 21 20 dim. 21 2.616822e-04 26.832591 48 9.942444e-01 p &gt; 0.05 ## 22 21 dim. 22 1.465193e-04 13.387359 33 9.990331e-01 p &gt; 0.05 ## 23 22 dim. 23 8.747367e-05 5.859197 20 9.990747e-01 p &gt; 0.05 ## 24 23 dim. 24 2.656286e-05 1.364800 9 9.980310e-01 p &gt; 0.05 The last column of the table prints the significance level associated with that dimension. Lower p-values suggest the (scaled) eigenvalues are significantly different from 0, an indication that the dimension is worth including. The results don't appear to be that informative in this case, however, as all dimensions up to (but not including) 19 are significant (assuming a traditional .05 threshold). Given our results, how many dimensions should we use? In this case the scree plot probably offers the most useful information, where a 2 or 3 dimensional solution is the most appropriate. Let’s go ahead and take a look at that third dimension, to see what it adds to the story. Let's create a 3-d plot, focusing just on the departments. Let's first get the locations of our departments. We can find the locations under row and then coord of our CA object: locs_department &lt;- ca_mod1$row$coord Now, we will use a text3D() function to plot the departments in three dimensions. Note that there are a number of packages that will allow for 3d plotting, but here we utilize the functions in the plot3D package. The text3D() function has a number of possible arguments. The main arguments are x, y z, showing the locations of each label to be plotted. The labels argument shows what should be printed in each location. Here we set the locations based on the first three dimensions in our CA object and set the labels to the names of the departments, defined above. The rest of the arguments control the look and rotation of the plot. text3D(x = locs_department[, &quot;Dim 1&quot;], y = locs_department[, &quot;Dim 2&quot;], z = locs_department[, &quot;Dim 3&quot;], labels = department_names, col = &quot;black&quot;, theta = 120, phi = 20, xlab = &quot;dim1&quot;, ylab = &quot;dim2&quot;, zlab = &quot;dim3&quot;, axes = T, ticktype = &quot;detailed&quot;, main = &quot;Departments in 3 Dimensions&quot;, cex = 0.75, bty = &quot;g&quot;, d = 2, adj = 1, font = 1) The plot is somewhat hard to read in three dimensions, but if we look at the third (z) dimension, we can see that on one extreme we have higher education and administration and on the other end we have English and performing arts. Thus, dimension 3 basically separates some of the departments that are close together on dimension 2 (English and higher education) but unless we really think that distinction is crucial we may be able to get away with the simpler 2-d plot, especially as those departments are already pretty different on dimension 1. 12.6 Importance of Rows/Columns So far we have been examining the quality of the whole model. It will also be useful to assess the specific objects (departments and topics) in terms of their contribution to the fit for each dimension. This can help us in interpreting the dimensions of the space, by showing which rows/columns are most important in fitting a given dimension. Higher values mean that the row is more important in defining that dimension (i.e. the locations of other departments are based on the extreme location of the given department). We can get a handy plot that will sort from high to low on each dimension. The function is fviv_contrib(). The arguments are X (the CA object), choice (either \"row\" or \"column\") and axes (dimension of interest). Here we plot the contribution of each row for the first dimension. We also change the size and angle of the axis labels to make them easier to read (using ggplot functions). fviz_contrib(ca_mod1, choice = &quot;row&quot;, axes = 1) + theme(axis.text.x = element_text(size = 8.0, angle = 75)) The red line represents the expected value if all rows contributed equally to the dimension. We can see that the first dimension is defined primarily by history, political science and the psychology departments. Rows that contribute a lot to the fit tend to be far from the center on the dimension of interest; in this case history and political science on the far right and psychology on the far left. This is consistent with our interpretation from above. Let's look at dimension 2: fviz_contrib(ca_mod1, choice = &quot;row&quot;, axes = 2) + theme(axis.text.x = element_text(size = 8.0, angle = 75)) And the same thing for the columns: fviz_contrib(ca_mod1, choice = &quot;col&quot;, axes = 1) + theme(axis.text.x = element_text(size = 8.0, angle = 75)) fviz_contrib(ca_mod1, choice = &quot;col&quot;, axes = 2) + theme(axis.text.x = element_text(size = 8.0, angle = 75)) Let’s plot the 2-dimensional solution again, but shade the labels by the contribution to the overall fit (summed over the first two dimensions). This is accomplished by setting col.row and col.col to “contrib”. We also include a scale_color_gradient2() function to set how the color scale should look. fviz_ca_biplot(ca_mod1, col.row = &quot;contrib&quot;, col.col =&quot;contrib&quot;, labelsize = 3, repel = T) + scale_color_gradient2(low = &quot;white&quot;, mid = &quot;steelblue&quot;, high = &quot;darkblue&quot;, midpoint = 8) + theme_minimal() Looking at our figure, it is worth thinking about what it means to be in the center of the plot compared to the extremes on one of the dimensions. For example, sociology (department) and gender_sexuality (topic) are in the center, while political science and social movements are not. This would suggest that sociology PhDs tend to take on dissertations that span many of the topics in the field. Similarly, this means that individuals from a wide variety of disciplines study gender. On the other hand, social movements is not studied by a wide variety of departments while those in political science tend not to study a large number of (sociological) topics, focused mainly on social movements. We thus have distinctions between central, generalist departments/topics and those that occupy more peripheral, niche locations. 12.7 Hierarchical Clustering To aid in interpretation, it may be useful to cluster the departments and topics into distinct, structurally equivalent sets, where departments (or topics) that are close together in the space are placed in the same cluster (they are structurally equivalent in the sense of having the same pattern of topics). This can make it easier to talk through the results, as we can start to identify particular regions of the space, and not just the dimensions making up the space. Here we will make use of hierarchical clustering (see also Chapter 10). We begin by calculating the distance matrix between objects, both topics and departments, based on the locations from the two dimensional solution. Let's first put locations for the departments and topics together into one data frame. We already have the locations for the departments, so let’s just grab the locations for the topics: locs_topic &lt;- ca_mod1$col$coord And now we stack them with the locations of the departments using a rbind() function: locs &lt;- rbind(locs_department, locs_topic) We just want the locations for the first two dimensions, so let’s subset the data. locs &lt;- locs[, 1:2] And now we create the distance matrix using the dist() function, setting the method to Euclidean. We will then fit a hierarchical clustering model to the distance matrix using hclust(): d &lt;- dist(locs, method = &quot;euclidean&quot;) fit &lt;- hclust(d, method = &quot;ward.D2&quot;) We need to pick an optimal number of clusters before we can interpret the results. As in Chapter 10, we can make use of the functions in the NbClust package to help us make a choice. The function is NbClust(). In this case we just include the distance matrix as the main inputs (set using diss). We set distance to NULL and tell R to use the silhouette index as the criterion (there are a number of other choices we could have made). clusters &lt;- NbClust(data = NULL, diss = d, distance = NULL, method = &quot;ward.D2&quot;, index = c(&quot;silhouette&quot;)) clusters$Best.nc ## Number_clusters Value_Index ## 8.0000 0.4649 Under this criterion the optimal solution has 8 clusters. The specific clusters can be found under: clusters$Best.partition. Note that both the topics and the departments are included in the same cluster, so it is a simultaneous clustering of rows/columns. Let's name the clusters in a useful way for the plot. Here we add the word cluster before the cluster number using a paste() function. cluster_names &lt;- paste(&quot;cluster&quot;, clusters$Best.partition, sep = &quot;&quot;) And let's also put the department/topic names on the vector of clusters so we know what each value corresponds to. names(cluster_names) &lt;- names(clusters$Best.partition) And finally, let's split those out by rows and columns. row_clusters &lt;- cluster_names[department_names] col_clusters &lt;- cluster_names[topic_names] Now, let's go back and redo our 2-d plot but color the departments and topics by cluster. fviz_ca_biplot(ca_mod1, col.row = row_clusters, col.col = col_clusters, labelsize = 3, repel = T) + theme_minimal() Our first impression is that the clusters align pretty well with the spatial layout. There are a few exceptions, where the department or topic is hard to fit (e.g., social theory or special education) but, on the whole, the clusters capture distinct regions of the larger field. We see an education cluster, a health cluster, a culture cluster, a psychology cluster and so on. In this way, we have two main dimensions (micro to macro; culture to stratification) but distinct regions (or niches) carved out within the larger space. These niches are constituted by both topic and department, so that a health cluster (cluster 4) is not merely the topic studied, but institutionally, the departments that tend to study it. Most illuminating from the cluster analysis, perhaps, is that the center of the figure splits into 2 clusters, divided between sets of 'core' sociology topics: first, we have a cluster including topics like gender and race/ethnicity (cluster 1); and second, we have a cluster containing topics like stratification and organizations (cluster 5). Note that the clustering solution above is based on clustering objects in a joint space, both row and column objects. It might, however, be preferred to cluster the rows and columns separately (e.g., because we do not want to try and interpret the distance between row and column objects). Here we can rely on the functions from the FactoMineR package. We will start by rerunning the correspondence analysis but set ncp to 2 (indicating that only the first 2 dimensions will be retained in the output). We do this to be consistent with the hierarchical clustering analysis done above. ca_mod2 &lt;- CA(department_topic, ncp = 2, graph = FALSE) We will now use a HCPC() function to cluster the departments and topics. In this analysis we will do this separately. We include the CA object as the main input (set using res). The nb.clust argument sets the number of clusters. Here we will set it at 8 to match the analysis above. We could also allow R to find the optimal number of clusters (by setting nb.clust = -1). We also tell R not to plot the results (the default). We set cluster.CA to \"rows\" to do the rows only. clust_rows &lt;- HCPC(res = ca_mod2, nb.clust = 8, graph = FALSE, cluster.CA = &quot;rows&quot;) And now for the columns: clust_cols &lt;- HCPC(res = ca_mod2, nb.clust = 8, graph = FALSE, cluster.CA = &quot;columns&quot;) Now we plot the correspondence analysis and color the departments by the found cluster. We set choice to \"map\" to get the factor map (based on the correspondence analysis). We set draw.tree to FALSE as we do not want to plot the hierarchical tree structure. plot(clust_rows, choice = &quot;map&quot;, draw.tree = FALSE) It looks pretty similar to what we had before. We can check this by doing a table on the clusters found above (row_clusters) and the clusters found here. We need to make sure that the departments are in the same order as above, so the table is meaningful. Here we grab the clust variable and sort by department_names, before calculating our table. row_clusters2 &lt;- clust_rows$data[department_names, &quot;clust&quot;] table(row_clusters, row_clusters2) ## row_clusters2 ## row_clusters 1 2 3 4 5 6 7 8 ## cluster1 0 0 0 4 0 0 0 0 ## cluster2 3 0 0 0 0 0 0 0 ## cluster3 0 0 0 0 0 0 0 2 ## cluster4 0 3 0 0 0 0 0 0 ## cluster5 0 0 0 0 3 0 0 0 ## cluster6 0 0 0 0 0 3 0 0 ## cluster7 0 0 5 0 0 0 0 0 ## cluster8 0 0 0 0 0 0 2 0 We can see that while the labels are different, the departments are placed together in the same way (e.g., cluster 1 is now cluster 4 but the departments are the same). Overall, the results are identical between the two clustering solutions, so we can be confident in our results. We can also plot the column results. plot(clust_cols, choice = &quot;map&quot;, draw.tree = FALSE) In the end, we have a holistic representation of an academic field, here based on topics and departments, that can serve as a useful way of analyzing non-network data in a way that is consistent with network ideas. These kinds of analyses push us beyond what we may traditionally think of as the purview of network analysis (actors and the relationships between them) but the ideas of duality, niches, and position are widely applicable, offering important alternatives to traditional regression frameworks. "],["ch13-Statistical-Models-Networks-R.html", "13 Statistical Models for Networks", " 13 Statistical Models for Networks Chapter 13 covers statistical network models in R. The first tutorial focuses on cross-sectional network models, focusing on exponential random graph models (ERGM), for both binary and valued networks. We consider dynamic extensions to the ERGM framework (STERGM) in the second tutorial for Chapter 13. We will cover models for two-mode networks in the third tutorial. The final tutorial for Chapter 13 will cover relational event models, appropriate for continuous-time network data. "],["ch13-Cross-Sectional-Network-Models-ERGM-R.html", "13, Part 1. Cross-Sectional Network Models: ERGM 13.1 Setting up the Session 13.2 Descriptive Statistics 13.3 CUG Tests 13.4 Fitting an ERGM 13.5 Checking Model Convergence 13.6 Goodness of Fit 13.7 Adding GWESP to the Model 13.8 Simulation 13.9 Example on a Large Network 13.10 ERGM on a Valued Network", " 13, Part 1. Cross-Sectional Network Models: ERGM This tutorial offers an extended example in R demonstrating how to analyze networks using statistical models. We will focus on exponential-family random graph models (ERGMs). ERGMs are useful as they make it possible to uncover what micro tendencies are important in network formation, comparing rates of reciprocity, homophily (for example) net of other network processes. The tutorial will draw on many of the previous tutorials, most directly the tutorials on dyads/triads (Chapter 7) and centrality/hierarchy (Chapter 9). The goals of the tutorial are to get acquainted with: model fitting, interpretation, diagnostics, model comparison, and simulating from a known model. We will offer two main examples, one based on a small school-based network, and one based on a much larger coauthorship network. Both examples are based on cross-sectional network data (so one time point). 13.1 Setting up the Session We will utilize a small school-based network for the first part of the ERGM tutorial. Our main question is what micro-level processes are important for the formation of the network. In particular, we want to explore some of the processes around gender and grade. For example, do boys tend to have more friends than girls? Is gender homophily a strong driver of tie formation? Is gender homophily stronger than grade homophily? How does controlling for other micro tendencies, like reciprocity and transitive closure, change our conclusions? First, let's load the necessary packages. library(ergm) library(sna) library(car) Note that the ergm package (Handcock et al. 2023) uses network objects consistent with the network package. It is also useful to have the latticeExtra and Rglpk packages installed. Now, let's read in our example network data representing a high school friendship network. The network data are saved as an adjacency matrix: url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/example_schoolmat.csv&quot; school_mat &lt;- read.csv(file = url1, row.names = 1) We have added row.names = 1 to tell R that the first column should be used to set the row names. Let’s turn that data frame into a matrix. school_mat &lt;- as.matrix(school_mat) And now let’s take look at the matrix (just the first five rows and columns). A 1 indicates that person i named person j as a friend. school_mat[1:5, 1:5] ## id1 id2 id3 id4 id5 ## id1 0 0 0 0 0 ## id2 0 0 0 0 0 ## id3 0 0 0 0 0 ## id4 0 0 0 0 0 ## id5 0 0 0 0 0 Now, we will read in the attribute file, containing nodal characteristics, including gender, grade and ses. url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/example_schooldata.csv&quot; school_attributes &lt;- read.csv(file = url2) head(school_attributes) ## ids gender grade ses ## 1 1 1 12 7 ## 2 2 1 10 7 ## 3 3 0 8 10 ## 4 4 0 11 9 ## 5 5 1 7 10 ## 6 6 1 10 9 Let's make it easier to interpret the gender variable. Let's recode the 0s and 1s into male and female using a recode() function. school_attributes$gender_recode &lt;- recode(school_attributes$gender, as.factor = F, &quot;0 = &#39;male&#39;; 1 = &#39;female&#39;&quot;) Let's construct a network object and put the attributes onto the network. First, we need to create a list of attributes based on our attribute data frame. attribute_list &lt;- do.call(list, school_attributes) Now we create the network based on the matrix and attribute list. We set the network as directed. school_net &lt;- network(x = school_mat, directed = T, vertex.attr = attribute_list) school_net ## Network attributes: ## vertices = 32 ## directed = TRUE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 124 ## missing edges= 0 ## non-missing edges= 124 ## ## Vertex attribute names: ## gender gender_recode grade ids ses vertex.names ## ## No edge attributes 13.2 Descriptive Statistics Before we go ahead and try to fit an exponential random graph model, it will be useful to know a bit about the network in question. Here we will plot the network and calculate basic network statistics. plot(school_net) One main question is how gender and grade shape the formation of ties in the network. So, let's look at the network but color the nodes by gender and then by grade. Let's color the nodes by gender, blue for boys and pink for girls: cols &lt;- recode(school_attributes$gender, as.factor = F, &quot;0 = &#39;blue&#39;; 1 = &#39;pink&#39;&quot;) We will now use the cols vector in our plot statement (using the default plot() function in the network package). plot(school_net, vertex.col = cols) It looks like there is moderate homophily along gender lines. What about grade? plot(school_net, vertex.col = school_attributes$grade) We see pretty clear social divisions by grade. It is also useful to know a bit about indegree and outdegree, seeing which groups (if any) are particularly likely to have high degree. Let's calculate indegree and outdegree for the network: outdegree &lt;- degree(school_net, cmode = &quot;outdegree&quot;) indegree &lt;- degree(school_net, cmode = &quot;indegree&quot;) Let's see if boys or girls have higher degree. We will check this by calculating mean indegree by gender. We can use tapply(), with indegree as the first input, gender as the second, and the function (here mean) as the third. tapply(indegree, school_attributes$gender_recode, mean) ## female male ## 3.25 4.50 Here we calculate the same thing for outdegree. tapply(outdegree, school_attributes$gender_recode, mean) ## female male ## 3.125 4.625 It looks like boys receive and send out more ties than girls. 13.3 CUG Tests We now turn to some simple statistical tests about the features of the network. This will give us useful information about the network, setting our base expectations before running the full model below. Here we will focus on Conditional Uniform Graph Tests (CUG test). See also Chapter 7. We begin by calculating the features of interest, like reciprocity or transitivity. We will then compare the observed statistic with the values that one would see in a random network of the same size (and possibly other constraints). As a researcher, you specify the baseline model of interest, specifying how to construct the random network. This sets the comparison of interest, effectively holding certain desired features constant and allowing others to vary randomly. The question is whether the observed statistic is above/below (or similar to) what one would expect by chance. Let's walk through a few examples to get a sense of the features of our network and how this compares to a random network. First, let's calculate density on the network. gden(school_net) ## [1] 0.125 We can see that .125 of all possible ties exist. Now, let's compare the observed density to what we are likely to see in a random network of the same size (i.e., if we took a network of the same size and just started randomly forming ties with probability .5, what would the density look like?). Here, we will run the CUG test with the function of interest set to \"gden\", calculating density. The conditioning is set to \"size\", holding size constant at the true network size. Everything else is allowed to vary randomly. We set dat to our network of interest. cug.test(dat = school_net, FUN = &quot;gden&quot;, cmode = &quot;size&quot;) ## ## Univariate Conditional Uniform Graph Test ## ## Conditioning Method: size ## Graph Type: digraph ## Diagonal Used: FALSE ## Replications: 1000 ## ## Observed Value: 0.125 ## Pr(X&gt;=Obs): 1 ## Pr(X&lt;=Obs): 0 The results clearly show that the density of .125 is well below what we would expect in a random network of size 32 (as the probability of finding simulated values above the observed value is effectively 1). This shouldn't be surprising however, as people have a limited capacity to form ties and the number of possible ties increases non-linearly as size goes up. Density thus tends to decrease non-linearly as system size increases. Now, let's do something a little more interesting and look at the rate of reciprocity. Reciprocity captures how often i is tied to j when j is tied to i. Here, we will condition the randomly generated networks on density. The question is thus: is the observed rate of reciprocity higher/lower than what we would expect in a network of the same size and number of edges, but where the edges are formed randomly? First, let's calculate reciprocity on the observed network, not including the null dyads in the calculation. grecip(school_net, measure = &quot;dyadic.nonnull&quot;) ## Mut ## 0.4090909 This says that about .409 of the non-null dyads are reciprocated (where i-&gt;j and j-&gt;i). And now we will run a CUG test. Here the function of interest is grecip, calculating reciprocity; the argument to the function is measure, set to \"dyadic.nonnull\". This is passed to grecip via FUN.args. cmode is set to \"edges\" to condition the random graphs on density. cug.test(dat = school_net, FUN = &quot;grecip&quot;, FUN.args = list(measure = &quot;dyadic.nonnull&quot;), cmode = &quot;edges&quot;) ## ## Univariate Conditional Uniform Graph Test ## ## Conditioning Method: edges ## Graph Type: digraph ## Diagonal Used: FALSE ## Replications: 1000 ## ## Observed Value: 0.4090909 ## Pr(X&gt;=Obs): 0 ## Pr(X&lt;=Obs): 1 The results suggest that the reciprocity rate, .409, is above what we would expect in a network of the same size and number of edges but otherwise random ties. We can see this as the probability of finding a randomly generated value higher than the observed value is 0. This would suggest that reciprocity is an important micro process shaping the formation of ties. We can do similar analyses with transitivity (or other statistics), constraining the randomly formed networks to make useful comparisons. For example, it may be useful to ask if the rate of transitivity is higher than what we would expect in a network with the same dyad census, showing if there are triadic effects (if i is friends with j and j is friends with k, is i friends with k?), above what can be induced by dyadic processes (i.e., reciprocity). 13.4 Fitting an ERGM We have so far learned a bit about our network. We know there is strong grade homophily and weaker gender homophily. We know that boys tend to give and receive more ties than girls. We also know that there are likely norms around reciprocating friendship. We now want to consider these micro-processes together, to see (in a systematic way) how important each factor is in predicting the existence of a tie, net of the other network processes. We saw a very simple version of this above with the CUG test (looking at the rate of reciprocity net of density) but we want to consider a number of factors together. We also want to see how these factors work together to generate the larger features of the network, like distance, thus linking the micro-processes to the emergent, global features of the network. An exponential random graph model allows us to take the network as the item of interest. The model predicts the presence/absence of a tie between all i-j pairs as a function of different network features, like reciprocity and homophily (on gender for example). It is useful to think of the terms added to the model in terms of dependencies. By including a term for reciprocity (for example), one is claiming that the existence of the i-&gt;j tie is dependent on whether or not j-&gt;i exists. We can apply similar logic to other terms, where different terms correspond to different arguments about what the formation of the i-&gt;j tie is (or is not) dependent on. Before we actually fit any ERGMs, it will be useful to look at some of the help files for the main functions: ?ergm, ?'ergm-terms', ?mcmc.diagnostics, ?gof. The function to actually run the model is ergm(). There are a large number of possible arguments to ergm. We focus on the main ones here: formula = a formula object with network on the left followed by a ~ and then the set of terms we want to include in the model. control = list of options used in the algorithm, like sample size and burnin. The inputs are set via a control.ergm() function. constraints = a formula dictating if there should be any constraints on the networks generated in the MCMC sample (used to estimate the parameters). This can be useful when certain networks are impossible (i.e., because the survey constrained the number of friends the respondents could name). For example: constraints = ~ bd(maxout = 10) would only consider networks where the max outdegree is 10 or less. 13.4.1 Edges Only We will start simple and run an ERGM with only edges as a predictor. We thus predict the school network as a function of the number of edges in the network and nothing else. mod_rand &lt;- ergm(formula = school_net ~ edges) Let’s take a look at the model results. summary(mod_rand) ## Call: ## ergm(formula = school_net ~ edges) ## ## Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -1.946 0.096 0 -20.27 &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 1375.2 on 992 degrees of freedom ## Residual Deviance: 747.5 on 991 degrees of freedom ## ## AIC: 749.5 BIC: 754.4 (Smaller is better. MC Std. Err. = 0) The interpretation of the model coefficients is straightforward in this case. The log-odds of any tie existing is -1.946. The probability of any tie existing is: exp(-1.946) / (1 + exp(-1.946)) = .125, which is just the density. Note that if we do a summary on the ergm formula, we get the counts for each network statistic in the observed network. summary(school_net~edges) ## edges ## 124 We see here that there are 124 edges in the observed network. 13.4.2 Edges and Homophily We know that homophily matters from the plots, so let's add nodematch terms for gender and grade. The nodematch terms count the number of times that an edge exists where i and j have the same attribute (here gender or grade). Note that the ergm package does not currently handle missing data on nodal attributes; the simplest solution to such issues is to impute missing attribute data prior to model estimation. mod_homoph1 &lt;- ergm(school_net ~ edges + nodematch(&quot;gender_recode&quot;) + nodematch(&quot;grade&quot;)) summary(mod_homoph1) ## Call: ## ergm(formula = school_net ~ edges + nodematch(&quot;gender_recode&quot;) + ## nodematch(&quot;grade&quot;)) ## ## Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -2.6140 0.1703 0 -15.351 &lt;1e-04 *** ## nodematch.gender_recode 0.1137 0.2086 0 0.545 0.586 ## nodematch.grade 2.3088 0.2144 0 10.769 &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 1375.2 on 992 degrees of freedom ## Residual Deviance: 635.7 on 989 degrees of freedom ## ## AIC: 641.7 BIC: 656.4 (Smaller is better. MC Std. Err. = 0) The results suggest that there is strong homophily on grade, well above that expected by chance (conditioned on density and homophily on gender). Grade thus appears to be an important factor in the formation of ties in the network. The nodematch term on gender is not significant, suggesting that the number of edges that match on gender is not clearly above what we would expect in a random network of the same size (conditioned on the other terms in the model, including density). Let's interpret our grade nodematch coefficient. For a tie between two nodes that match on grade and mismatch on gender the conditional log-odds is: -2.6140 + .1137 * 0 + 2.3088 * 1 = -.3052. The probability of a tie is: exp(-.3052) / (1 + exp(-.3052)) = .424. For a tie between two nodes that mismatch on grade and mismatch on gender the conditional log-odds is: -2.6140 + .1137 * 0 + 2.3088 * 0 = -2.6140. The probability of a tie is: exp(-2.6140) / (1 + exp(-2.6140)) = .068. We can see that the probability of a tie goes down substantially when the two nodes are not in the same grade. In terms of odds ratios, we can say that the odds of a tie existing is exp(2.3088) = 10.062 times higher if the nodes match on grade than if they differ on grade, net of the other terms in the model. Note that we would want to use an absdiff term if the attribute of interest is continuous and can take on many values, such as with gpa or household income. absdiff takes the absolute difference between the two actors in the dyad on the attribute of interest. The basic idea is that actors who are tied together may have similar values but not match exactly. absdiff is coded in the opposite manner as nodematch, so that a negative coefficient is evidence of homphily. 13.4.3 Edges, Homophily, and Node Covariates We can ask related questions about gender and grade by adding terms to the model that show if boys (compared to girls) send out and/or receive more ties. We will add nodeifactor terms and nodeofactor terms for gender. nodeifactor captures the ties coming in and nodeofactor captures the ties going out. The terms capture if the mean number of ties coming in or going out is higher for boys than girls. We can add analogous terms for grade using nodeicov and nodeocov. Nodecov terms are appropriate for quantitative variables, rather than factors. mod_homoph2 &lt;- ergm(school_net ~ edges + nodematch(&quot;gender_recode&quot;) + nodematch(&quot;grade&quot;) + nodeifactor(&quot;gender_recode&quot;) + nodeofactor(&quot;gender_recode&quot;) + nodeicov(&quot;grade&quot;) + nodeocov(&quot;grade&quot;)) summary(mod_homoph2) ## Call: ## ergm(formula = school_net ~ edges + nodematch(&quot;gender_recode&quot;) + ## nodematch(&quot;grade&quot;) + nodeifactor(&quot;gender_recode&quot;) + nodeofactor(&quot;gender_recode&quot;) + ## nodeicov(&quot;grade&quot;) + nodeocov(&quot;grade&quot;)) ## ## Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -7.54896 0.82637 0 -9.135 &lt;1e-04 *** ## nodematch.gender_recode 0.03797 0.22294 0 0.170 0.8648 ## nodematch.grade 2.49128 0.23497 0 10.603 &lt;1e-04 *** ## nodeifactor.gender_recode.male 0.45209 0.22428 0 2.016 0.0438 * ## nodeofactor.gender_recode.male 0.55980 0.22467 0 2.492 0.0127 * ## nodeicov.grade 0.26617 0.06388 0 4.167 &lt;1e-04 *** ## nodeocov.grade 0.18685 0.06275 0 2.978 0.0029 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 1375.2 on 992 degrees of freedom ## Residual Deviance: 586.6 on 985 degrees of freedom ## ## AIC: 600.6 BIC: 634.9 (Smaller is better. MC Std. Err. = 0) It looks like individuals who are male are more likely to send and receive friendship nominations, net of the other terms in the model. The results also suggest that individuals in higher grades tend to receive and send more ties while forming strong boundaries around grade (looking at the nodematch term on grade). 13.4.4 Edges, Homophily, Node Covariates, and Reciprocity Now, let's go ahead and make a more complicated model, one that incorporates reciprocity. We will do this by adding a mutual term to the model. The mutual term counts the number of pairs where i-&gt;j and j-&gt;i exist. Note that by adding mutual to the model the model will now be fit using MCMC methods. With MCMC estimation, it is often useful to set the input parameters to the estimation routine. This can be done using control.ergm(). Let's see how to do this. Here we will set the sample size using a MCMC.samplesize argument and burnin using a MCMC.burnin argument. set.seed(1012) mod_homoph_mutual1 &lt;- ergm(school_net ~ edges + nodematch(&quot;gender_recode&quot;) + nodematch(&quot;grade&quot;) + nodeifactor(&quot;gender_recode&quot;) + nodeofactor(&quot;gender_recode&quot;) + nodeicov(&quot;grade&quot;) + nodeocov(&quot;grade&quot;) + mutual, control = control.ergm(MCMC.burnin = 50000, MCMC.samplesize = 6000)) Note that in the R console there we will be added output telling you what the algorithm is doing, although we suppress this here. 13.5 Checking Model Convergence Our model is now being estimated via MCMC methods. It is important to make sure that the algorithm converged, so that the sampled networks (which the estimates are based on) offer reasonable, consistent statistics (or counts) sample to sample. Let's see how the model looks using a mcmc.diagnostics() function: mcmc.diagnostics(mod_homoph_mutual1, vars.per.page = 4) The function plots the MCMC sample of network statistics for each term included in the model. The plots show how far the statistics calculated on the sampled networks are from the true value, calculated on the observed network. The plots on the left hand show the statistics taken from each individual sample (plotted in order), while the right hand side shows the distribution of sample statistics. A 'nice' looking plot would have the sample statistics centered around 0, with some (but not extreme) variance sample to sample, suggesting that the specified model is well-behaved and can produce networks consistent with the observed data. In this case everything looks okay and the model seems to have converged. The networks statistics haven't gone off dramatically in one direction and are not erratic, changing dramatically sample to sample (but there is still variation). We will consider examples later on where the models do not converge. Note that if we had found problems one possible option is to tweak the input parameters further, generally allowing for a longer burnin, more iterations between samples and so on. Let's go ahead and look at the results: summary(mod_homoph_mutual1) ## Call: ## ergm(formula = school_net ~ edges + nodematch(&quot;gender_recode&quot;) + ## nodematch(&quot;grade&quot;) + nodeifactor(&quot;gender_recode&quot;) + nodeofactor(&quot;gender_recode&quot;) + ## nodeicov(&quot;grade&quot;) + nodeocov(&quot;grade&quot;) + mutual, control = control.ergm(MCMC.burnin = 50000, ## MCMC.samplesize = 6000)) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -6.48488 0.71672 0 -9.048 &lt; 1e-04 *** ## nodematch.gender_recode 0.02139 0.18794 0 0.114 0.90938 ## nodematch.grade 1.74426 0.21750 0 8.020 &lt; 1e-04 *** ## nodeifactor.gender_recode.male 0.29171 0.23868 0 1.222 0.22164 ## nodeofactor.gender_recode.male 0.45784 0.23841 0 1.920 0.05481 . ## nodeicov.grade 0.21605 0.06836 0 3.161 0.00157 ** ## nodeocov.grade 0.10241 0.06877 0 1.489 0.13643 ## mutual 2.35476 0.36848 0 6.390 &lt; 1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 1375.2 on 992 degrees of freedom ## Residual Deviance: 546.5 on 984 degrees of freedom ## ## AIC: 562.5 BIC: 601.7 (Smaller is better. MC Std. Err. = 0.1587) The results suggest, first, that if i nominates j then j is much more likely to nominate i. We can see this as the number of mutual pairs is well above what we would expect based on density and homophily alone. More formally, the odds of a tie increase by exp(2.35476) = 10.536 times if adding i-&gt;j adds a mutual pair to the network. This suggests that there are norms of reciprocity in the network guiding tie formation. We also see that the gender differences in indegree and outdegree are no longer significant. This suggests that while boys do receive more ties than girls, this is largely explained by expectations of reciprocity. So that boys get more nominations only because they send out more friendship ties and part of friendship is to reciprocate. Or, more substantively, boys have a larger number of reciprocated ties, but there are not necessarily status differences between girls and boys. We can push this intuition a little further by allowing the effect of different relational terms, like mutual, to vary by nodal attributes. The idea is that certain interactional tendencies may be stronger/weaker for different sets of actors; for example, based on gender. This amounts to including an interaction in the model, between the term of interest and a nodal attribute. Here, we want to test if boys have more reciprocated ties than girls, and thus are involved in more mutual dyads. We will specify this by including a mutual term where we set by to the attribute of interest (gender_recode). See ?'ergm-terms' for more options on the mutual term. The model will include two mutual terms, one for boys and one for girls, counting the number of mutual dyads that include at least one boy (or girl). For this initial model, we will not include nodefactor terms for gender. We add those controls below as a means of comparison. mod_homoph_mutual1b &lt;- ergm(school_net ~ edges + nodematch(&quot;gender_recode&quot;) + nodematch(&quot;grade&quot;) + nodeicov(&quot;grade&quot;) + nodeocov(&quot;grade&quot;) + mutual(by = &quot;gender_recode&quot;), control = control.ergm(MCMC.burnin = 50000, MCMC.samplesize = 6000)) summary(mod_homoph_mutual1b) ## Call: ## ergm(formula = school_net ~ edges + nodematch(&quot;gender_recode&quot;) + ## nodematch(&quot;grade&quot;) + nodeicov(&quot;grade&quot;) + nodeocov(&quot;grade&quot;) + ## mutual(by = &quot;gender_recode&quot;), control = control.ergm(MCMC.burnin = 50000, ## MCMC.samplesize = 6000)) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -6.09449 0.67938 0 -8.971 &lt; 1e-04 *** ## nodematch.gender_recode 0.05100 0.19080 0 0.267 0.78926 ## nodematch.grade 1.69064 0.21051 0 8.031 &lt; 1e-04 *** ## nodeicov.grade 0.21603 0.06928 0 3.118 0.00182 ** ## nodeocov.grade 0.10247 0.06855 0 1.495 0.13497 ## mutual.by.gender_recode.female 0.91528 0.24995 0 3.662 0.00025 *** ## mutual.by.gender_recode.male 1.45561 0.22282 0 6.533 &lt; 1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 1375.2 on 992 degrees of freedom ## Residual Deviance: 551.3 on 985 degrees of freedom ## ## AIC: 565.3 BIC: 599.6 (Smaller is better. MC Std. Err. = 0.1797) We can see that boys are, in fact, more likely to be involved in mutual dyads than girls (comparing the coefficient on mutual for male to female). The results are thus consistent with our previous conclusions. Now, let's go ahead and include a control for degree differences (by gender) in the model. This will allow us to see if boys are more likely to be involved in mutual dyads because they send out more ties or because expectations of reciprocity are higher among boys. We will include the nodeofactor term for gender, controlling for the tendency to send out ties. mod_homoph_mutual1c &lt;- ergm(school_net ~ edges + nodematch(&quot;gender_recode&quot;) + nodematch(&quot;grade&quot;) + nodeofactor(&quot;gender_recode&quot;) + nodeicov(&quot;grade&quot;) + nodeocov(&quot;grade&quot;) + mutual(by = &quot;gender_recode&quot;), control = control.ergm(MCMC.burnin = 50000, MCMC.samplesize = 6000)) summary(mod_homoph_mutual1c) ## Call: ## ergm(formula = school_net ~ edges + nodematch(&quot;gender_recode&quot;) + ## nodematch(&quot;grade&quot;) + nodeofactor(&quot;gender_recode&quot;) + nodeicov(&quot;grade&quot;) + ## nodeocov(&quot;grade&quot;) + mutual(by = &quot;gender_recode&quot;), control = control.ergm(MCMC.burnin = 50000, ## MCMC.samplesize = 6000)) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -6.35418 0.72070 0 -8.817 &lt; 1e-04 *** ## nodematch.gender_recode 0.05256 0.18961 0 0.277 0.781610 ## nodematch.grade 1.71578 0.20940 0 8.194 &lt; 1e-04 *** ## nodeofactor.gender_recode.male 0.50212 0.29245 0 1.717 0.085994 . ## nodeicov.grade 0.21850 0.06925 0 3.155 0.001603 ** ## nodeocov.grade 0.09788 0.06795 0 1.440 0.149736 ## mutual.by.gender_recode.female 1.14996 0.29852 0 3.852 0.000117 *** ## mutual.by.gender_recode.male 1.24696 0.24280 0 5.136 &lt; 1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 1375 on 992 degrees of freedom ## Residual Deviance: 548 on 984 degrees of freedom ## ## AIC: 564 BIC: 603.2 (Smaller is better. MC Std. Err. = 0.2244) It looks like boys and girls have similar tendencies of forming mutual dyads, controlling for the rate of sending out ties. Thus, the differences between boys and girls are not in the expectations of reciprocity, but in the sheer volume of relationships that boys tend to be involved in. Note that it is difficult to do formal statistical tests comparing one coefficient (mutual) across group (boys v. girls) in logistic regression. A more formal test would require using average marginal effects in the scale of the predicted probabilities. See the ergMargins package. 13.6 Goodness of Fit It is generally a good idea to see if the specified model is fitting the network well. The basic test is whether the micro-level tie formation processes assumed in the model are sufficient to reproduce the features of the network as a whole (e.g., if individuals form ties based on the specified micro-processes, does that explain why the network looks the way it does?). Note that this is a goodness of fit test and is different than the MCMC diagnostics viewed above. The MCMC diagnostics only show if the model estimates can be trusted and thus focus on the terms included in the model. Goodness of fit statistics, in contrast, test whether the model can reproduce features of the network not included in the model. Here, we will ask if our model can reproduce distance and the shared partner distribution from the true network. Distance captures the distribution of shortest paths between all ij pairs. The shared partner distribution shows for each ij pair how many other nodes both i and j are friends with. This captures clustering in the network, showing if i and j tend to be friends with the same people. Let's take a quick look at the edgewise shared partner distribution in the observed network: summary(school_net ~ esp(0:10)) ## esp.OTP0 esp.OTP1 esp.OTP2 esp.OTP3 esp.OTP4 esp.OTP5 esp.OTP6 esp.OTP7 esp.OTP8 esp.OTP9 esp.OTP10 ## 24 37 29 19 12 2 1 0 0 0 0 This says that 24 students who are friends have 0 friends in common, 37 have 1 friend in common, and so on. The function to run the goodness of fit test is gof. The inputs are the fitted model followed by the statistics of interest you want to test against, set using the GOF argument (as a formula). The gof() function will simulate a set of networks based on the estimated model; it will then take the generated networks, calculate the macro features of interest (here distance and the shared partner distribution) and compare that to the true value, based on the empirical network. We will also include a model term in the formula. This asks if the model is reproducing the terms included in the model itself (like mutual and homophily on grade). This is a useful check, although not a good test of model fit. We will use the model with the simple specification for mutual (i.e., not allowing the effect to vary by gender). gof_mod_homoph_mutual1 &lt;- gof(mod_homoph_mutual1, GOF = ~ distance + espartners + model, control = control.gof.ergm(seed = 110)) We first set up the plot to have three columns and then plot the goodness of fit statistics. par(mfrow = c(1, 3)) plot(gof_mod_homoph_mutual1) The boxplots capture the values from the simulated network and the black line represent the values from the observed network. A good model will have the values from the simulated networks close to the true values. Our model is clearly missing something with the shared partners. We can see that the simulated networks greatly overestimate the number of (tied) pairs with 0 common partners. Or, more substantively, it is clear that the model underestimates local clustering (or the tendency for a friend of a friend to be a friend). We can also look at the goodness of fit statistics directly. gof_mod_homoph_mutual1 ## ## Goodness-of-fit for minimum geodesic distance ## ## obs min mean max MC p-value ## 1 124 93 124.18 154 1.00 ## 2 235 188 299.40 416 0.12 ## 3 252 193 298.97 375 0.16 ## 4 122 52 129.61 185 0.82 ## 5 23 1 37.87 106 0.54 ## 6 1 0 9.21 40 0.56 ## 7 0 0 2.26 34 1.00 ## 8 0 0 0.51 18 1.00 ## 9 0 0 0.16 8 1.00 ## 10 0 0 0.03 2 1.00 ## Inf 235 0 89.80 328 0.04 ## ## Goodness-of-fit for edgewise shared partner ## ## obs min mean max MC p-value ## esp.OTP0 24 38 54.65 72 0.00 ## esp.OTP1 37 17 36.05 56 0.94 ## esp.OTP2 29 6 19.86 33 0.18 ## esp.OTP3 19 0 9.72 22 0.16 ## esp.OTP4 12 0 3.32 16 0.08 ## esp.OTP5 2 0 0.54 8 0.20 ## esp.OTP6 1 0 0.04 1 0.08 ## ## Goodness-of-fit for model statistics ## ## obs min mean max MC p-value ## edges 124 93 124.18 154 1.00 ## nodematch.gender_recode 62 42 61.37 80 0.98 ## nodematch.grade 63 42 62.24 76 0.92 ## nodeifactor.gender_recode.male 72 50 72.52 91 1.00 ## nodeofactor.gender_recode.male 74 50 73.55 90 1.00 ## nodeicov.grade 1263 959 1264.07 1545 1.00 ## nodeocov.grade 1247 951 1247.68 1542 0.98 ## mutual 36 23 36.11 48 1.00 A significant p-value tells us that the observed value (from the true network) is significantly different from the values from the simulated networks, a clear sign that the model is not fitting well. For example, there is clear difference for the esp0 (0 shared partner) count. Note also that when doing the goodness of fit test we can include the same kinds of control and constraint inputs as with the actual model fitting. Let's see if we can complicate our model a bit to account for local clustering. First, let's make a small tweak to the model, here changing the way we estimate homophily on grade. By adding diff = T to the grade nodematch term we add 6 terms to the model, one for each grade. The idea is that in-group bias may be stronger/weaker for certain grades. mod_homoph_mutual2 &lt;- ergm(school_net ~ edges + nodematch(&quot;gender_recode&quot;) + nodematch(&quot;grade&quot;, diff = T) + nodeifactor(&quot;gender_recode&quot;) + nodeofactor(&quot;gender_recode&quot;) + nodeicov(&quot;grade&quot;) + nodeocov(&quot;grade&quot;) + mutual) summary(mod_homoph_mutual2) ## Call: ## ergm(formula = school_net ~ edges + nodematch(&quot;gender_recode&quot;) + ## nodematch(&quot;grade&quot;, diff = T) + nodeifactor(&quot;gender_recode&quot;) + ## nodeofactor(&quot;gender_recode&quot;) + nodeicov(&quot;grade&quot;) + nodeocov(&quot;grade&quot;) + ## mutual) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -6.33143 1.12564 0 -5.625 &lt; 1e-04 *** ## nodematch.gender_recode 0.06463 0.19358 0 0.334 0.738470 ## nodematch.grade.7 2.01431 0.46736 0 4.310 &lt; 1e-04 *** ## nodematch.grade.8 0.47033 0.94843 0 0.496 0.619963 ## nodematch.grade.9 1.72554 0.44383 0 3.888 0.000101 *** ## nodematch.grade.10 1.39172 0.41999 0 3.314 0.000921 *** ## nodematch.grade.11 1.78042 0.44102 0 4.037 &lt; 1e-04 *** ## nodematch.grade.12 2.09320 0.48451 0 4.320 &lt; 1e-04 *** ## nodeifactor.gender_recode.male 0.29469 0.25297 0 1.165 0.244046 ## nodeofactor.gender_recode.male 0.46405 0.24185 0 1.919 0.055015 . ## nodeicov.grade 0.20711 0.08175 0 2.534 0.011289 * ## nodeocov.grade 0.09470 0.08087 0 1.171 0.241586 ## mutual 2.30384 0.37413 0 6.158 &lt; 1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 1375.2 on 992 degrees of freedom ## Residual Deviance: 541.7 on 979 degrees of freedom ## ## AIC: 567.7 BIC: 631.4 (Smaller is better. MC Std. Err. = 0.3625) Here, we see that grade 12 has the highest in-group bias (where they disproportionately have friends within the same grade) while grade 8 has the lowest in-group bias. Let’s again look at goodness of fit. gof_mod_homoph_mutual2 &lt;- gof(mod_homoph_mutual2, GOF = ~ distance + espartners + model, control = control.gof.ergm(seed = 113)) par(mfrow = c(1, 3)) plot(gof_mod_homoph_mutual2) It doesn't look much better in terms of fit. Of course, it still may be of interest to know if there are different levels of in-group bias across grades. 13.7 Adding GWESP to the Model We have so far fit a number of models, learned about reciprocity and homophily but have not captured the local clustering in the network. So, let's go ahead and add a term to the model that will capture the tendency for nodes who are tied to have the same friends. For these models we will use our simple specification on grade homophily. There are a number of ways of specifying triadic effects like clustering and dominance. For example, one could include terms for different triad types (i.e., including different terms of the triad census). By including different combinations of triad counts, one could test different hypotheses about the formation of the network, comparing the fit of a 'clustering' model compared to a 'ranked clustering model'. This would be akin to a traditional tau statistic (see Chapter 9). Similarly, we could capture clustering using a triangle term. A triangle is defined as all cases where i-j and j-k exist and then either k-&gt;i or i-&gt;k exist. Based on past work this is unlikely to be a good choice. In fact, it is very difficult to get a model with a triangle term to converge in the case of our school network, and we will not run this here. Instead, let's include a gwesp term in the model, or a geometrically weighted edge-wise shared partner term. GWESP is a weighted count of the distribution of shared partners discussed above. If there is local clustering in the network, we would expect nodes who are tied to have many friends in common (and more than what we would expect based on chance expectations). Unlike the triangle term, GWESP is not focused solely on what happens in a given triad, as it counts all shared partners for a given ij pair. Note that we must specify a decay parameter, dictating the weighting on how much the first shared partners counts (in terms of increasing the probability of a tie) compared to the second, third, fourth, etc. Lower values put more weight on the initial partners and less weight on adding additional partners. Here we will set decay to 1. And to save time let’s limit the algorithm run time to three iterations. mod_homoph_mutual_gwesp1 &lt;- ergm(school_net ~ edges + nodematch(&quot;gender_recode&quot;) + nodematch(&quot;grade&quot;) + nodeifactor(&quot;gender_recode&quot;) + nodeofactor(&quot;gender_recode&quot;) + nodeicov(&quot;grade&quot;) + nodeocov(&quot;grade&quot;) + mutual + gwesp(decay = 1, fixed = T), control = control.ergm(MCMLE.maxit = 3)) Looking at the diagnostics: mcmc.diagnostics(mod_homoph_mutual_gwesp1) This is a clear example of a degenerate model. We can see that the simulated networks have extreme characteristics, far from the observed values in the network. Another quick way of judging if the sample of MCMC statistics are okay is plotting the last network from the MCMC sample. plot(mod_homoph_mutual_gwesp1$newnetwork) A complete or empty network is not what you want to see. In general, there are two reasons we could have a degenerate model. 1. A poorly specified model. In a poorly specified model, the terms in the model do not reflect the actual processes that generated the network. When this happens, the simulated networks do not have realistic features, making it difficult to estimate the parameters very well. 2. The inputs that control the algorithm (like burnin) are not sufficient for the model to converge. Maybe we should try a different specification of the model. For example, we can change the input decay value. Here we will set the decay value lower. When decay is high, the probability of a tie is greatly increased by adding another shared partner, even if the pair already have many shared partners. When decay is low, adding more shared partners does not greatly increase the probability of a tie if the pair already have a few shared partners. For this next model we will decrease the decay parameter to .5. We will also change some of the control parameters (like burnin, number of iterations and sample size) to let the algorithm run longer. This can take a bit to run, but we can try and speed that up using parallel processing (here utilizing 2 cores): mod_homoph_mutual_gwesp2 &lt;- ergm(school_net ~ edges + nodematch(&quot;gender_recode&quot;) + nodematch(&quot;grade&quot;) + nodeifactor(&quot;gender_recode&quot;) + nodeofactor(&quot;gender_recode&quot;) + nodeicov(&quot;grade&quot;) + nodeocov(&quot;grade&quot;) + mutual + gwesp(decay = .5, fixed = T), control = control.ergm(MCMC.burnin = 50000, MCMC.samplesize = 7000, parallel = 2, parallel.type = &quot;PSOCK&quot;)) mcmc.diagnostics(mod_homoph_mutual_gwesp2, vars.per.page = 3) Convergence looks okay. Now let’s look at model fit. gof_mod_homoph_mutual_gwesp2 &lt;- gof(mod_homoph_mutual_gwesp2, GOF = ~ distance + espartners + model, control = control.gof.ergm(seed = 108)) par(mfrow = c(1, 3)) plot(gof_mod_homoph_mutual_gwesp2) As we can see, the fit is good (and improved) for the shared partner distribution. This suggests that our previous model (mod_homoph_mutual2) was, in fact, missing the local clustering in the network; without a gwesp term we overestimate the number of people who are tied together but have no common friends. This is now accurately captured in our new model (mod_homoph_mutual_gwesp2). We can also see that the fit, in terms of AIC and BIC, is better in the full model. summary(mod_homoph_mutual_gwesp2) ## Call: ## ergm(formula = school_net ~ edges + nodematch(&quot;gender_recode&quot;) + ## nodematch(&quot;grade&quot;) + nodeifactor(&quot;gender_recode&quot;) + nodeofactor(&quot;gender_recode&quot;) + ## nodeicov(&quot;grade&quot;) + nodeocov(&quot;grade&quot;) + mutual + gwesp(decay = 0.5, ## fixed = T), control = control.ergm(MCMC.burnin = 50000, MCMC.samplesize = 7000, ## parallel = 2, parallel.type = &quot;PSOCK&quot;)) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -5.117856 0.459526 0 -11.137 &lt;1e-04 *** ## nodematch.gender_recode 0.021423 0.200843 0 0.107 0.9151 ## nodematch.grade 1.403702 0.176879 0 7.936 &lt;1e-04 *** ## nodeifactor.gender_recode.male 0.120178 0.217153 0 0.553 0.5800 ## nodeofactor.gender_recode.male 0.272596 0.217352 0 1.254 0.2098 ## nodeicov.grade 0.100776 0.058257 0 1.730 0.0837 . ## nodeocov.grade 0.009869 0.057974 0 0.170 0.8648 ## mutual 1.830650 0.392218 0 4.667 &lt;1e-04 *** ## gwesp.OTP.fixed.0.5 0.799865 0.136124 0 5.876 &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 1375.2 on 992 degrees of freedom ## Residual Deviance: 511.2 on 983 degrees of freedom ## ## AIC: 529.2 BIC: 573.3 (Smaller is better. MC Std. Err. = 0.2277) Substantively, we can see that after controlling for local clustering (gwesp) there is still a significant effect for grade homophily and reciprocity. The gwesp term is significant and clearly improved the fit of the model. A positive coefficient suggests that students tend to have more shared partners than we would expect based on density, homophily and reciprocity alone. More generally, the main micro drivers of tie formation in this network (sufficient to reproduce the macro features of the network) are grade homophily, reciprocity and local clustering. Moreover, net of other micro-processes, there would appear to be very little gender effects, as gender homophily is relatively weak and the nodefactor terms for gender are not significant (showing that boys don't have more ties net of other factors). Let's simulate a network to see how it looks compared to the true network. This offers another useful check to see if the model is working as expected. The function is simulate(). The main arguments are: object = the estimated ergm nsim = number of simulated networks Note that we can include constraints and control as with ergm, although the inputs must be set by control.simulate.ergm(): sim_schoolnet &lt;- simulate(mod_homoph_mutual_gwesp2, nsim = 1, seed = 1012, control = control.simulate.ergm(MCMC.burnin = 100000)) And now let's create a plot to compare the true network to the simulated one. par(mfrow = c(1, 2)) plot(school_net, vertex.col = &quot;grade&quot;, main = &quot;True Network&quot;) plot(sim_schoolnet, vertex.col = &quot;grade&quot;, main = &quot;Simulated Network&quot;) Looks reasonably close to the true network, but if we are unsatisfied, we may want to consider adding other terms to the model. 13.8 Simulation Simulation is often used, as above, as a means of checking model fit. Simulation can also be used as a useful tool in itself. In this case, we are not trying to estimate an ERGM. Instead, we will use the ERGM framework to simulate networks with desired properties. We can use the generated networks to ask theoretical questions. We can also use the generated network as inputs in other analyses (e.g., as inputs into a diffusion model-see Chapter 14). Here, we ask an important theoretical question about homophily. What would happen to the network structure if we altered the strength of homophily? For example, if homophily on grade was weaker or stronger, how would transitivity in the network change? Our goal is to generate two networks, a 'strong' homophily network and a 'weak' homophily network, comparing the two cases to see how homophily affects network structure. We will continue to make use of the school network for this example. Both networks will be of the same size (n = 32) and density as the school network but they will differ on grade homophily. We could consider other features, like reciprocity, but we will keep this simple and just look at networks conditioned on density and grade homophily. The simulation has three main steps: first, set the basic features of the network; second, estimate the ERGM coefficients; third, use the coefficients to generate networks. Note that the ergm package can handle quite complicated simulation inputs, allowing for realistic, nuanced networks to be generated with desired properties. In the first step, we will create a network object, setting the size and type of network to be used in the simulation. Here, we initialize a network with the same size as our school network (32) and set it as directed. net &lt;- network.initialize(n = 32, directed = T) We will also seed the network with attributes, based on the distribution of grade from the observed data. This is necessary as we want to set the strength of grade homophily in the simulation (and thus nodes in the network must have a grade attribute). net %v% &quot;grade&quot; &lt;- get.vertex.attribute(school_net, &quot;grade&quot;) In the second step, we will estimate an ERGM, which will serve as input into the simulation. The main inputs to the ERGM are the network of interest (constructed above), an ERGM formula and the target statistics, governing the features of the simulated networks. Here, the formula and target statistics are based on the number of edges and the number of edges that match on grade. We will need to set those target statistics for each case of interest, strong and weak homophily. Let's first take a look at the values in the observed network: summary(school_net ~ edges + nodematch(&quot;grade&quot;)) ## edges nodematch.grade ## 124 63 We can see that there are 124 edges in the original network and that 63 of the edges match on grade. For our analysis, we want to generate two networks, both with 32 nodes (the size of the original network) and 124 edges, but with different values for nodematch on grade. We will define strong homophily as a network with 75% of edges matching on grade, and weak homophily as a network with 25% of edges matching on grade. This means that nodematch on grade should be set to 93 edges for the strong homophily network (.75 * 124) and 31 for the weak homophily network (.25 * 124). We will now estimate the ERGM for the strong homophily case. This will get us the coefficients to use in the simulation below. The call is similar to what we saw above but here we include a target.stats argument. target.stats corresponds to the desired counts for the terms included in the model (edges and nodematch on grade). Here we set edges to 124 and nodematch(\"grade\") to 93. The target.stats input must be in the same order as the terms in the formula (so edges first and then nodematch on grade). Note that in this case one input is based on the empirical data (edges = 124) and one is based on a value set by the researcher (nodematch on grade = 93). The network of interest is net, constructed in step 1. mod_stronghomophily &lt;- ergm(net ~ edges + nodematch(&quot;grade&quot;), target.stats = c(124, 93)) mod_stronghomophily ## ## Call: ## ergm(formula = net ~ edges + nodematch(&quot;grade&quot;), target.stats = c(124, ## 93)) ## ## Maximum Likelihood Coefficients: ## edges nodematch.grade ## -3.272 3.872 Now, let’s do the same thing for the weak homophily case. Here the number of edges is the same but only 31 edges go within grade. mod_weakhomophily &lt;- ergm(net ~ edges + nodematch(&quot;grade&quot;), target.stats = c(124, 31)) As a third step, we will simulate networks based on the estimated models above. Let's start with the strong homophily model. We will generate a network of the right size (32), with approximately 124 edges and 93 of those going to people of the same grade. The main input is the estimated model. By default, the simulate() function will generate one network, but this can be altered using the nsim argument. We set the seed argument to make it easier to replicate. sim_strong_homophily &lt;- simulate(mod_stronghomophily, seed = 1006) Let's check the network statistics of the simulated network: summary(sim_strong_homophily ~ edges + nodematch(&quot;grade&quot;)) ## edges nodematch.grade ## 122 89 It looks okay. The generated network has around 124 edges, with close to 75% matching on grade. The simulation is stochastic, so every network generated will have slightly different features. Note that we could also have constrained the simulation on the number of edges strictly, ensuring that the simulated networks have exactly 124 edges: sim_strong_homophily &lt;- simulate(mod_stronghomophily, constraints = ~ edges). And now we simulate a network based on the weak homophily model. sim_weak_homophily &lt;- simulate(mod_weakhomophily, seed = 1009) Let's check the network statistics of the weak homophily network: summary(sim_weak_homophily ~ edges + nodematch(&quot;grade&quot;)) ## edges nodematch.grade ## 114 30 Here we see that the generated network has around 25% edges matching on grade (again, this will vary from simulation to simulation). Let's plot the two simulated networks. par(mfrow = c(1, 2)) plot(sim_weak_homophily, vertex.col = &quot;grade&quot;, main = &quot;Weak Homophily&quot;) plot(sim_strong_homophily, vertex.col = &quot;grade&quot;, main = &quot;Strong Homophily&quot;) We can see that the network with higher homophily is separated more clearly into social groups (based on grade). Let's also calculate transitivity in each of the generated networks. gtrans(sim_weak_homophily) ## [1] 0.1116279 gtrans(sim_strong_homophily) ## [1] 0.3037037 We can see that the network with stronger homophily has higher transitivity, so that a friend of a friend is more likely to be a friend. In this case, homophily on grade created groups in the network. Three people in the same grade are all likely to be friends, raising the potential for a friend of a friend to also be a friend. More generally, the results demonstrate how shifting patterns of homophily can affect network structure. We can imagine exploring a number of other questions using this kind of simulation platform; for example, we could ask how network cohesion changes as we alter the way actors form ties, based on mechanisms like homophily, hierarchy, or balance. 13.9 Example on a Large Network We now turn to an example on a much larger network than the school network used above. In this case, we model a coauthorship network with 60098 nodes. The actors (or nodes) are sociologists who published at least one paper. Two actors are tied together if they coauthored a paper together. The network is undirected. A network of this size creates practical issues for statistical models and we will consider how a researcher can navigate such difficulties. First, let's read in the edgelist. url3 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/coauthorship_edgelist.txt&quot; coauthor_edgelist &lt;- read.table(file = url3, header = T) head(coauthor_edgelist) ## sender receiver ## 1 14 24 ## 2 134 151 ## 3 6 170 ## 4 15 212 ## 5 19 219 ## 6 267 283 Now, let's read in the attribute file. url4 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/coauthorship_attributes.txt&quot; coauthor_attributes &lt;- read.table(file = url4, header = T) head(coauthor_attributes) ## ids gender prestige research_type subject ## 1 1 NA low prestige quantitative 1 ## 2 2 1 low prestige qualitative 24 ## 3 3 0 low prestige quantitative 20 ## 4 4 0 low prestige quantitative 7 ## 5 5 1 low prestige quantitative 19 ## 6 6 NA low prestige quantitative 3 We will focus on two key attributes: prestige and research type. For prestige, 0 = never published in high prestige journals; 1 = has published in high prestige journal. For research_type, 0 = primarily a qualitative scholar; 1 = primarily a quantitative scholar; 2 = primarily uses mixed methods. Research type thus captures the kinds of methods and data employed in their research. Let’s recode things to make it a little easier to interpret. coauthor_attributes$prestige &lt;- recode(coauthor_attributes$prestige, as.factor = F, &quot;0 = &#39;low prestige&#39;; 1 = &#39;high prestige&#39;&quot;) coauthor_attributes$research_type &lt;- recode(coauthor_attributes$research_type, as.factor = F, &quot;0 = &#39;qualitative&#39;; 1 = &#39;quantitative&#39;; 2 = &#39;mixed&#39;&quot;) And now let's construct our network using the edgelist and attribute data frame as inputs. coauthorship_net &lt;- network(x = coauthor_edgelist, directed = F, vertices = coauthor_attributes) coauthorship_net ## Network attributes: ## vertices = 60098 ## directed = FALSE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 94338 ## missing edges= 0 ## non-missing edges= 94338 ## ## Vertex attribute names: ## gender prestige research_type subject vertex.names ## ## Edge attribute names not shown We can see there are 60098 nodes and 94338 edges. Now, let's try to run some simple ERGMs on the coauthorship network. Given the size of the network, it will be difficult to run certain models using the ergm package. In particular, dyadic dependent models (i.e., models that include terms like gwesp, where the ij tie is dependent on the presence/absence of other ties) pose difficult computational challenges. This estimation problem is an important topic for current research. For example, see the work of Byshkin et al. (2018) for new algorithms to make the estimation of such models on large networks faster and more plausible. Here, we will focus on the simpler case, where we only try to estimate a dyadic independent model, focusing on homophily and node-level factors. Substantively, we will focus on the social divisions that exist in coauthorship in terms of prestige and type of research (quantitative/qualitative/mixed). For example, do 'high prestige' scholars tend to coauthor with other 'high prestige’ scholars? We will now go ahead and run our models. We start with a model that looks at homophily for prestige and research type. Note that even this simple model can take a bit to run (e.g., around 5 minutes on a reasonable personal computer). In general, running ERGMs on a large network is computationally taxing, and it is even possible that R will crash before the estimation is complete; this is especially likely in the Windows environment. We consider an alternative approach below, based on sampling, that is computationally less burdensome. mod1 &lt;- ergm(coauthorship_net ~ edges + nodematch(&quot;research_type&quot;) + nodematch(&quot;prestige&quot;)) summary(mod1) ## Call: ## ergm(formula = coauthorship_net ~ edges + nodematch(&quot;research_type&quot;) + ## nodematch(&quot;prestige&quot;)) ## ## Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -11.023537 0.015556 0 -708.63 &lt;1e-04 *** ## nodematch.research_type 1.379181 0.008049 0 171.34 &lt;1e-04 *** ## nodematch.prestige 0.278784 0.014511 0 19.21 &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 2.503e+09 on 1.806e+09 degrees of freedom ## Residual Deviance: 2.012e+06 on 1.806e+09 degrees of freedom ## ## AIC: 2012156 BIC: 2012214 (Smaller is better. MC Std. Err. = 0) Looking at the results, we won't worry too much with the statistical tests and standard errors, given the size of the network (i.e., the number of dyads). We will focus more on the magnitude of the coefficient for each term. Substantively, the results suggest that homophily does strongly shape the probability of a tie forming. For example, the odds of a tie forming when two scholars match on research type is exp(1.37918) = 3.972 times higher than the odds of a tie forming when the scholars differ on research type (controlling for matching on prestige). We now run the same basic model but add nodefactor terms for each attribute of interest. This will make it possible to look at homophily net of the differences in degree across categories (e.g., quantitative scholars may coauthor more than qualitative scholars). Note that the added terms are nodefactor terms (rather than nodeifactor or nodeofactor terms) as the network is undirected. mod2 &lt;- ergm(coauthorship_net ~ edges + nodematch(&quot;research_type&quot;) + nodematch(&quot;prestige&quot;) + nodefactor(&quot;research_type&quot;) + nodefactor(&quot;prestige&quot;)) summary(mod2) ## Call: ## ergm(formula = coauthorship_net ~ edges + nodematch(&quot;research_type&quot;) + ## nodematch(&quot;prestige&quot;) + nodefactor(&quot;research_type&quot;) + nodefactor(&quot;prestige&quot;)) ## ## Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -8.619221 0.024507 0 -351.70 &lt;1e-04 *** ## nodematch.research_type 1.550310 0.008790 0 176.37 &lt;1e-04 *** ## nodematch.prestige 1.564844 0.018795 0 83.26 &lt;1e-04 *** ## nodefactor.research_type.qualitative -0.638367 0.009082 0 -70.29 &lt;1e-04 *** ## nodefactor.research_type.quantitative -0.639767 0.008732 0 -73.27 &lt;1e-04 *** ## nodefactor.prestige.low prestige -1.312055 0.012411 0 -105.71 &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 2.503e+09 on 1.806e+09 degrees of freedom ## Residual Deviance: 2.002e+06 on 1.806e+09 degrees of freedom ## ## AIC: 2002310 BIC: 2002426 (Smaller is better. MC Std. Err. = 0) The results suggest, as before, that high prestige researcher tend to coauthor with high prestige researchers, while researchers tend to coauthor with others doing similar kinds of research. This time, however, we see that the coefficient on nodematch for prestige is higher than seen in model 1, where we did not control for the degree differences across categories. Model 2 makes clear that those with high prestige tend to coauthor more than those with lower prestige (see negative coefficient on nodefactor.prestige.low prestige). Controlling for these differences in degree, the tendency for researchers to coauthor with researchers of similar prestige appears to be amplified, although we would want to explore this more formally (i.e., by using probabilities to look at the marginal effect of homophily on prestige across the two models). The models estimated above are computationally burdensome. As an alternative approach, we can rely on work done on ERGMs on sampled data (Krivitsky and Morris 2017). The basic idea is to take samples from the complete network and estimate the models on the subsetted data. This offers consistent estimates but is easier to compute than the models using the complete network. Let's start by loading the ergm.ego package. Note that ergm.ego is designed to estimate models on sampled ego network data (and thus is appropriate for cases where a researcher does not have the full network to work with, as we do here). library(ergm.ego) As a next step, we need to transform our network into a format that the ergm.ego package (Krivitsky 2023) can use. We basically need to turn the true, complete network into an egor object, like that covered in Chapter 6 (ego network analysis). To save time, we will read in the egor object, constructed previously, using: coauthorship_egodat &lt;- as.egor(coauthorship_net). We use a url() function as we are directly loading a .Rdata file (as opposed to reading in a CSV file). url5 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/coauthorship_egodat.Rdata&quot; load(url(description = url5)) coauthorship_egodat ## # EGO data (active): 60,098 × 6 ## .egoID gender prestige research_type subject vertex.names ## * &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 1 NA low prestige quantitative 1 1 ## 2 2 1 low prestige qualitative 24 2 ## 3 3 0 low prestige quantitative 20 3 ## 4 4 0 low prestige quantitative 7 4 ## 5 5 1 low prestige quantitative 19 5 ## # ℹ 60,093 more rows ## # ALTER data: 188,676 × 7 ## .altID .egoID gender prestige research_type subject vertex.names ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 58346 1 1 low prestige quantitative 20 58346 ## 2 45514 1 NA low prestige quantitative 20 45514 ## 3 44671 1 NA low prestige quantitative 20 44671 ## # ℹ 188,673 more rows ## # AATIE data: 580,092 × 3 ## .egoID .srcID .tgtID ## * &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 58346 28678 ## 2 1 58346 44671 ## 3 1 58346 45514 ## # ℹ 580,089 more rows And now we take a sample from the complete network. Here we will sample 25000 people from the network, with replacement. The sampled data will be in the form of ego network information, where we know, for each sampled person, how many ties they have, their attributes and the attributes of the people they are tied to. This information is sufficient to estimate the models specified above. Note that it is also possible to create a list of ego networks (from the complete network), sample from that list, and construct the egor object from the selected egos, although we will not do this here. set.seed(200) coauthorship_samp &lt;- sample(coauthorship_egodat, 25000, replace = T) Now we can estimate our models. We will focus on model 1 (edges and the nodematch terms). The syntax is similar as before, but here we use the ergm.ego() function and the sampled version of the network as input. We also include a bit more information to help adjust the estimates for the fact that we are sampling from the full network. We will set popsize to 60098, indicating the size of the true population. We also set ppopsize (within control.ergm.ego()), to determine the size of the pseudo population used in the estimation routine. Here we set it at the size of the sample. mod1_samp &lt;- ergm.ego(coauthorship_samp ~ edges + nodematch(&quot;research_type&quot;) + nodematch(&quot;prestige&quot;), popsize = 60098, control = control.ergm.ego(ppopsize = 25000, ergm = control.ergm(parallel = 2))) summary(mod1_samp) ## Call: ## ergm.ego(formula = coauthorship_samp ~ edges + nodematch(&quot;research_type&quot;) + ## nodematch(&quot;prestige&quot;), control = control.ergm.ego(ppopsize = 25000, ## ergm = control.ergm(parallel = 2)), popsize = 60098) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## offset(netsize.adj) 0.87710 0.00000 0 Inf &lt;1e-04 *** ## edges -10.98920 0.03327 0 -330.29 &lt;1e-04 *** ## nodematch.research_type 1.35814 0.01366 0 99.42 &lt;1e-04 *** ## nodematch.prestige 0.25482 0.03063 0 8.32 &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## The following terms are fixed by offset and are not estimated: ## offset(netsize.adj) Focusing on the nodematch terms, we see that the coefficients are very similar to what was estimated above, on the complete network. The standard errors are, as one would expect, higher, as we are utilizing less information than before. Our conclusions are the same, however, and the inflated standard errors may be a reasonable trade off given the reduction in computational burden. Note that a researcher could repeat this process a number of times, taking new samples, estimating the model each time, and then summarizing the results over all samples. 13.10 ERGM on a Valued Network As a final example, we turn to estimating ERGMs on a valued network. The basic idea is the same as what we have seen so far in this tutorial, but now the model is extended to handle weighted, or valued, edges. Much of the syntax and interpretation is analogous to the non-weighted (binary) case. We will thus run through this fairly quickly, highlighting how the terms in the model have been adapted to the case of valued networks. Our example is based on data collected by Daniel McFarland. The data capture the social interactions happening in one classroom during a single class period. A researcher recorded each time a student talked socially with another student. For this analysis, we have aggregated the interactional data, counting the number of times that two students talked during the class period. Let's go ahead and read in the weighted edgelist. url6 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/social_interaction_edgelist.txt&quot; social_interactions &lt;- read.table(file = url6, header = T) head(social_interactions) ## sender receiver count ## 1 1 7 22 ## 2 3 5 2 ## 3 3 11 19 ## 4 3 12 5 ## 5 3 17 4 ## 6 4 12 1 The first column shows the sender of the tie and the second column shows the receiver (although the network is undirected and each i-j pair with a tie is only represented once in the edgelist). The third column (count) captures the number of social interactions between sender and receiver. We can see that actor 1 and actor 7 talked 22 times during the class, 3 and 5 talked 2 times, and so on. We also need to read in the attribute file. We have information on gender, race and grade. url7 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/social_interaction_attributes.txt&quot; attributes &lt;- read.table(file = url7, header = T, stringsAsFactors = F) We are now in a position to construct the network. The network is set to be undirected and we include the attribute data frame to define our vertex attributes. social_net &lt;- network(x = social_interactions, directed = F, vertices = attributes) social_net ## Network attributes: ## vertices = 18 ## directed = FALSE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 25 ## missing edges= 0 ## non-missing edges= 25 ## ## Vertex attribute names: ## gender grade race vertex.names ## ## Edge attribute names: ## count We can see that we have our basic network object. Note that an edge attribute, called count, has been added to the network object. count is our edge weight of interest, showing how many times actor i and actor j talked to each other. We will now go ahead and plot the network, using functions from the GGally package. We will color the nodes based on gender and set the size of the edges to be proportional to the edge weights (count), capturing the number of interactions between actors. library(ggplot2) library(GGally) ggnet2(social_net, node.color = &quot;gender&quot;, node.size = 7, palette = c(&quot;male&quot; = &quot;navy&quot;, &quot;female&quot; = &quot;lightskyblue&quot;), edge.size = get.edge.attribute(social_net, &quot;count&quot;) / 2.75, edge.color = &quot;grey80&quot;) + guides(size = &quot;none&quot;) We can see that interactions tend to split along gender lines. We also see variation in how much students talk to each other (i.e., some of the edges are thick and some are thin), while some students do not talk to anyone during the class. In general, the network is fairly sparse, with students only talking to a handful of people during the class. 13.10.1 Baseline ERGM on Valued Network Now, let's see if we can fit an ERGM to our network. We could, of course, always binarize the network (0 if below some threshold; 1 if above) but this throws away a lot of useful information. We would do better by fitting an ERGM on the weighted version of the network, keeping all of the information on the edge weights intact. In this case, our edge weights are based on counts (number of interactions). To fit ERGMs based on count data, we will need to load the ergm.count package (Krivitsky 2022). library(ergm.count) We will now go ahead and run models predicting our weighted network. The basic form of the ergm() function is the same as with binary networks, but there are important differences. First, we need to include a response argument, indicating the edge attribute on the network that we want to model (in this case, count). Second, we need to include a reference argument. This is a one sided formula indicating how the edge attribute is distributed. There are a number of options (see ?'ergm-references' for details), but we will use the Poisson option. Third, we need to include terms specifically designed for weighted networks. For our first model, we will keep things simple and just include a term capturing the baseline rate of interaction in the network. Instead of using an edge term (as with binary data), we will include a sum term, summing up the edge weights over all ij pairs. count_mod1 &lt;- ergm(social_net ~ sum, response = &quot;count&quot;, reference = ~ Poisson) summary(count_mod1) ## Call: ## ergm(formula = social_net ~ sum, response = &quot;count&quot;, reference = ~Poisson) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## sum -0.05446 0.07603 0 -0.716 0.474 ## ## Null Deviance: 0.0000 on 153 degrees of freedom ## Residual Deviance: -0.2079 on 152 degrees of freedom ## ## Note that the null model likelihood and deviance are defined to be 0. This means that all likelihood-based inference (LRT, Analysis of Deviance, AIC, BIC, etc.) is only valid between models with the same reference distribution and constraints. ## ## AIC: 1.792 BIC: 4.823 (Smaller is better. MC Std. Err. = 0.08565) Note that the model was fit with MCMC estimation (even though it was very simple). We can check if the model converged using mcmc.diagnostics(count_mod1), although we do not present this here. The interpretation of the coefficients is different than above, as it follows the language of count models. We begin by exponentiating the coefficient on our sum term:exp(-0.05446) = 0.947. This tells us that the expected number of interactions between any two students is 0.947 (i.e., about 1 interaction during the class). 13.10.2 Adding Nodematch and Nodefactor Terms Now, let's add terms capturing homophily for gender and race. The term is still nodematch, but in this case we add a form argument, setting it to \"sum\". The term will sum up the edge weights over all edges that match on gender (or race). We will also add nodefactor terms for gender and race. count_mod2 &lt;- ergm(social_net ~ sum + nodematch(&quot;gender&quot;, form = &quot;sum&quot;) + nodematch(&quot;race&quot;, form = &quot;sum&quot;) + nodefactor(&quot;gender&quot;, form = &quot;sum&quot;) + nodefactor(&quot;race&quot;, form = &quot;sum&quot;), response = &quot;count&quot;, reference = ~ Poisson, control = control.ergm(MCMC.samplesize = 5000)) summary(count_mod2) ## Call: ## ergm(formula = social_net ~ sum + nodematch(&quot;gender&quot;, form = &quot;sum&quot;) + ## nodematch(&quot;race&quot;, form = &quot;sum&quot;) + nodefactor(&quot;gender&quot;, form = &quot;sum&quot;) + ## nodefactor(&quot;race&quot;, form = &quot;sum&quot;), response = &quot;count&quot;, reference = ~Poisson, ## control = control.ergm(MCMC.samplesize = 5000)) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## sum -1.39125 0.25386 0 -5.480 &lt;1e-04 *** ## nodematch.sum.gender 1.29417 0.20143 0 6.425 &lt;1e-04 *** ## nodematch.sum.race 0.15512 0.16932 0 0.916 0.360 ## nodefactor.sum.gender.male 0.46068 0.10027 0 4.594 &lt;1e-04 *** ## nodefactor.sum.race.white 0.09558 0.12131 0 0.788 0.431 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 0.00 on 153 degrees of freedom ## Residual Deviance: -55.62 on 148 degrees of freedom ## ## Note that the null model likelihood and deviance are defined to be 0. This means that all likelihood-based inference (LRT, Analysis of Deviance, AIC, BIC, etc.) is only valid between models with the same reference distribution and constraints. ## ## AIC: -45.62 BIC: -30.47 (Smaller is better. MC Std. Err. = 0.3071) Let's interpret the results for gender. We see that there is homophily on gender (the nodematch term for gender), while boys in the class tend to have higher levels of interaction (the nodefactor term for gender). More formally, the expected number of interactions between two boys (i.e., same gender) is 5.783 times higher than the number between one boy and one girl (i.e., different gender). The calculation is: exp(-1.3912 + 1.2942 + 2 * 0.4607 ) / exp(-1.3912+ 0 * 1.2942 + 1 * 0.4607). Note that we multiply the coefficient on nodefactor.sum.gender.male (0.4607) by 2 in the numerator (as there are 2 boys) and 1 in the denominator (as there is 1 boy). Similarly, the expected number of interactions between two girls is 2.301 times higher than between one boy and one girl: exp(-1.3912 + 1.2942 + 0 * 0.4607 ) / exp(-1.3912+ 0 * 1.2942 + 1 * 0.4607). As with binary data, we can use simulation to check if the model is fitting well. Here, we will simulate one network from the underlying model. count_sim_mod2 &lt;- simulate(count_mod2, nsim = 1) And now we plot the simulated network. ggnet2(count_sim_mod2, node.color = &quot;gender&quot;, node.size = 7, palette = c(&quot;male&quot; = &quot;navy&quot;, &quot;female&quot; = &quot;lightskyblue&quot;), edge.size = get.edge.attribute(count_sim_mod2, &quot;count&quot;) / 2.75, edge.color = &quot;grey80&quot;) + guides(size = &quot;none&quot;) The simulated network looks denser than the observed network. We can check this more formally by redoing our simulation, this time including a monitor argument to output statistics on the simulated networks. We will monitor a nonzero term, counting the number of dyads where the edge weight is greater than 0 (so at least some interaction between i and j). We will simulate 100 networks. count_sim_mod2_stats &lt;- simulate(count_mod2, nsim = 100, monitor = ~ nonzero, output = c(&quot;stats&quot;)) And now we summarize the results, comparing the count of nonzeros from the simulated networks (using the mean over the 100 networks) to the count from the empirical network. We calculate the empirical value using a summary() function (setting response to \"count\" to use the edge weights in the calculation). data.frame(sim = mean(count_sim_mod2_stats[, &quot;nonzero&quot;]), empirical = summary(social_net ~ nonzero, response = &quot;count&quot;)) ## sim empirical ## nonzero 84.69 25 As we saw in the plot, the simulated networks greatly overestimate how many of the students talk to each other during the class. The actual network is sparser, with students talking intensely with a only small number of other students. Thus, there are fewer nonzeros (or more 0s) in the network that can be accounted for by our model. Let's go ahead and add nonzero to the model. count_mod3 &lt;- ergm(social_net ~ sum + nodematch(&quot;gender&quot;, form = &quot;sum&quot;) + nodematch(&quot;race&quot;, form = &quot;sum&quot;) + nodefactor(&quot;gender&quot;, form = &quot;sum&quot;) + nodefactor(&quot;race&quot;, form = &quot;sum&quot;) + nonzero, response = &quot;count&quot;, reference = ~ Poisson, control = control.ergm(MCMC.samplesize = 5000)) summary(count_mod3) ## Call: ## ergm(formula = social_net ~ sum + nodematch(&quot;gender&quot;, form = &quot;sum&quot;) + ## nodematch(&quot;race&quot;, form = &quot;sum&quot;) + nodefactor(&quot;gender&quot;, form = &quot;sum&quot;) + ## nodefactor(&quot;race&quot;, form = &quot;sum&quot;) + nonzero, response = &quot;count&quot;, ## reference = ~Poisson, control = control.ergm(MCMC.samplesize = 5000)) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## sum 1.47958 0.14234 0 10.395 &lt; 1e-04 *** ## nodematch.sum.gender 0.23064 0.08700 0 2.651 0.00803 ** ## nodematch.sum.race 0.03305 0.07265 0 0.455 0.64914 ## nodefactor.sum.gender.male 0.08220 0.04262 0 1.929 0.05376 . ## nodefactor.sum.race.white 0.01969 0.04809 0 0.409 0.68220 ## nonzero -7.19845 0.54072 0 -13.313 &lt; 1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 0 on 153 degrees of freedom ## Residual Deviance: -400 on 147 degrees of freedom ## ## Note that the null model likelihood and deviance are defined to be 0. This means that all likelihood-based inference (LRT, Analysis of Deviance, AIC, BIC, etc.) is only valid between models with the same reference distribution and constraints. ## ## AIC: -388 BIC: -369.8 (Smaller is better. MC Std. Err. = 0.8216) The strongly negative coefficient on nonzero suggests that the network does, in fact, have an inflated number of zeros (i.e., dyads where no edge exists). Adding the nonzero term clearly helped the fit (see AIC and BIC), and also seems to have accounted for some of the gender differences in social interactions. 13.10.3 Transitivity in Valued Networks As a final model, we will include a term to capture local clustering, or transitivity, in the network. With count data, the term to capture transitive relations is transitiveweights (see Krivitsky (2012)). In general, we want to know about the tendency for a friend of a friend to also be a friend. This is fairly straightforward in the binary case; if i is tied with j and j is tied with k, then a transitive triad would mean i should be tied to k. The weighted case is harder, as it is not immediately clear how to count transitive triads when there are weighted edges linking the three actors. For example, if i-j has weight 5, j-k has weight 3, and i-k has weight 1, how transitive is that triad? With such questions in mind, the transitiveweights term takes three arguments, allowing the researcher to decide how the transitive triads should be summed up. The three arguments are twopath, combine and affect. twopath The twopath argument controls how the weights over a given two path should be treated. Assume a triad of i, j, and k where the focal dyad is i, k. Setting twopath to \"min\" takes the minimum edge weight between i-j and j-k when determining the strength of the twopath connecting i to k (so 3 in our example above); setting twopath to \"geomean\" takes the geometric mean between the edge weights for i-j and j-k (3.873 in our example). The key difference is that the \"min\" option puts all of the weight on the weakest edge while \"geomean\" does not. combine The combine argument controls how all two paths surrounding a given dyad should be combined (i.e., i-j, j-k; i-n n-k, etc.). Setting combine to \"max\" uses the maximum twopath value while \"sum\" takes the summation over all twopaths. The \"sum\" option thus takes into account all of the twopaths between i and k, while \"max\" only considers the strongest twopath. affect Finally, affect controls how the combined twopaths should affect the focal dyad (i, k). Setting affect to \"min\" takes the minimum value between the combined two path score and the edge weight on the focal dyad; setting affect to \"geomean\" takes the geometric mean of those two values. The defaults are \"min\", \"max\", and \"min\", as these offer more stable models (although the default options also tend to offer more conservative estimates). Substantively, we might have good reason to deviate from the default settings. For example, if we thought that students were more likely to interact when connected by many twopaths (i.e., suggesting they are in the same local group), we might want to use the \"sum\" option for the combine argument. Similarly, we would want the \"geomean\" option for twopath if we thought that a strong relationship within a triad could induce transitive closure (e.g., if i is weakly tied to j, and j is strongly tied to k, should we expect i to be strongly tied to k?). We could also run the model under different specifications and examine differences in model fit. Here we will use the default options. count_mod4 &lt;- ergm(social_net ~ sum + nodematch(&quot;gender&quot;, form = &quot;sum&quot;) + nodematch(&quot;race&quot;, form = &quot;sum&quot;) + nodefactor(&quot;gender&quot;, form = &quot;sum&quot;) + nodefactor(&quot;race&quot;, form = &quot;sum&quot;) + nonzero + transitiveweights(&quot;min&quot;, &quot;max&quot;, &quot;min&quot;), response = &quot;count&quot;, reference = ~ Poisson, control = control.ergm(MCMC.samplesize = 5000)) summary(count_mod4) ## Call: ## ergm(formula = social_net ~ sum + nodematch(&quot;gender&quot;, form = &quot;sum&quot;) + ## nodematch(&quot;race&quot;, form = &quot;sum&quot;) + nodefactor(&quot;gender&quot;, form = &quot;sum&quot;) + ## nodefactor(&quot;race&quot;, form = &quot;sum&quot;) + nonzero + transitiveweights(&quot;min&quot;, ## &quot;max&quot;, &quot;min&quot;), response = &quot;count&quot;, reference = ~Poisson, ## control = control.ergm(MCMC.samplesize = 5000)) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## sum 1.46183 0.13662 0 10.700 &lt; 1e-04 *** ## nodematch.sum.gender 0.20931 0.07600 0 2.754 0.00588 ** ## nodematch.sum.race 0.03187 0.07392 0 0.431 0.66639 ## nodefactor.sum.gender.male 0.06722 0.03701 0 1.816 0.06935 . ## nodefactor.sum.race.white 0.01523 0.04561 0 0.334 0.73842 ## nonzero -7.36105 0.53147 0 -13.850 &lt; 1e-04 *** ## transitiveweights.min.max.min 0.09804 0.06107 0 1.605 0.10839 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 0.0 on 153 degrees of freedom ## Residual Deviance: -399.5 on 146 degrees of freedom ## ## Note that the null model likelihood and deviance are defined to be 0. This means that all likelihood-based inference (LRT, Analysis of Deviance, AIC, BIC, etc.) is only valid between models with the same reference distribution and constraints. ## ## AIC: -385.5 BIC: -364.3 (Smaller is better. MC Std. Err. = 0.8194) We see a positive (but not significant) coefficient on transitiveweights, while the model fit has not really improved from the previous model. This suggests that there is only a weak tendency toward transitive relations (above what can be explained by the other terms in the model). Of course, we might find stronger results if we use different inputs to the transitiveweights term. Overall, this tutorial has covered exponential random graph models for cross-sectional data. See http://statnet.org/Workshops/ergm_tutorial.html for additional ERGM examples in R. The second tutorial in Chapter 13 considers models that extend ERGM to handle longitudinal (or dynamic) data. In Chapter 15, we consider statistical models that deal with the co-evolution of networks and attributes (like behaviors). "],["ch13-Longitudinal-Network-Models-STERGM-R.html", "13, Part 2. Longitudinal Network Models: STERGM 13.11 Getting the Data Ready 13.12 STERGMs 13.13 Checking Model Fit", " 13, Part 2. Longitudinal Network Models: STERGM This is the second tutorial for Chapter 13 on statistical network models. The first tutorial covered the case of cross-sectional network data. Here, we assume that a researcher has data on at least two time points and is interested in modeling change in the network over time. In this tutorial, we will walk through the estimation and interpretation of separable temporal exponential random graph models (STERGM). We can think of STERGM as a longitudinal extension to ERGM (see previous tutorial). With STERG models, we are interested in predicting the formation and dissolution of edges from time T to T+1 (however defined). The model is appropriate for cases where the edges can be defined discretely, for the periods defined in the study. For example, we could predict the formation/dissolution of edges from data collected on relationships amongst students at two different time points. We will consider relational event models, appropriate for continuous-time, streaming data in the fourth tutorial (Chapter 13, Part 4). For this tutorial, we will use network data collected by Daniel McFarland on adolescents in a classroom. We have four discrete networks, capturing changes in network ties across different segments of a single class period. An edge exists in the first network if i and j talked in the first 10 minutes of the class (0-10 minutes); there is an edge in the second network if i and j talked in the second 10 minutes (10-20 minutes); there is an edge in the third network if i and j talked in the third 10 minutes (20-30 minutes); and there is an edge in the fourth network if i and j talked j in the last segment of the class (30-40 minutes). The networks are treated as undirected. Substantively, our main question is how interaction partners in the classroom shift over a single class period. What mechanisms predict the adding of a tie that did not exist earlier in the class? What mechanisms predict the dropping of a tie? And are the mechanisms that predict the adding of a tie the same as dropping one? For example, students are likely to initiate conversations (i.e., add ties) with students who are sitting physically nearby. On the other hand, once two students are talking, sitting adjacent might not have a large effect on maintaining the tie over time; as they have already overcome the initial hurdle of being physically distant. 13.11 Getting the Data Ready We will make use of the network and sna packages in this tutorial (rather than igraph). library(sna) library(network) library(networkDynamic) As a first step, we will load our four networks, already constructed as network objects. url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/talk_nets_undirected.RData&quot; load(url(description = url1)) Let's take a look at the four networks, talk_time1, talk_time2, talk_time3 and talk_time4: talk_time1 ## Network attributes: ## vertices = 18 ## directed = FALSE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 14 ## missing edges= 0 ## non-missing edges= 14 ## ## Vertex attribute names: ## gender race vertex.names ## ## No edge attributes talk_time2 ## Network attributes: ## vertices = 18 ## directed = FALSE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 16 ## missing edges= 0 ## non-missing edges= 16 ## ## Vertex attribute names: ## gender race vertex.names ## ## No edge attributes talk_time3 ## Network attributes: ## vertices = 18 ## directed = FALSE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 13 ## missing edges= 0 ## non-missing edges= 13 ## ## Vertex attribute names: ## gender race vertex.names ## ## No edge attributes talk_time4 ## Network attributes: ## vertices = 18 ## directed = FALSE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 12 ## missing edges= 0 ## non-missing edges= 12 ## ## Vertex attribute names: ## gender race vertex.names ## ## No edge attributes talk_time1 corresponds to interactions taking place in the first 'period' (0-10 minutes), talk_time2 corresponds to interactions taking place in the second 'period' (10-20 minutes) and so on. Note that each of these networks already has attributes mapped onto them; specifically gender and race. In this case, all nodes are present in each period of observation. Each network (period 1 through 4) thus has the same set of nodes. In many cases, however, nodes may enter or exit through time (i.e., be present in period 1 but not period 2). We consider problems of entry/exit in the next lab, on two-mode networks. Let's plot the networks for the first two periods. We will set the color of the nodes by gender (navy blue for male and light blue for female). We start by extracting gender from the first network. gender &lt;- get.vertex.attribute(talk_time1, &quot;gender&quot;) head(gender) ## [1] &quot;female&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; &quot;male&quot; We now set color based on gender. cols &lt;- ifelse(gender == &quot;male&quot;, &quot;navy blue&quot;, &quot;light blue&quot;) We will also set the layout for the plot to be the same across the two periods. This makes it easier to see how edges are dropped/added from period to period. We accomplish this by defining the locations (locs) to place the nodes and then using the coord argument in the plot statement. We define the locations of the nodes based on the period 1 network. locs &lt;- network.layout.fruchtermanreingold(talk_time1, layout.par = NULL) par(mfrow = c(1, 2)) plot(talk_time1, main = &quot;Talk to Network, 0 to 10 Minutes&quot;, vertex.col = cols, coord = locs, vertex.cex = 2) plot(talk_time2, main = &quot;Talk to Network, 10 to 20 Minutes&quot;, vertex.col = cols, coord = locs, vertex.cex = 2) We see that the basic structure of the network is pretty similar period to period, but specific edges do change. For example, in a number of instances students who were isolates (did not talk to anyone in the first 10 minutes) become active, socially, making connections to existing groups of talking students. We also see that one social group is almost entirely female, but otherwise boys and girls seem to mix pretty freely in this classroom. STERG models require that our discrete, longitudinal networks be put together as a networkDynamic object. The function is networkDynamic(). In this case, the only input we need is a list of networks, where the networks are placed into a list in sequential order (see Chapter 3 for more complicated examples; e.g., where there are time changing vertex attributes or nodes becoming inactive/active over time). The resulting networkDynamic object will serve as input into our STERG model. net_dynamic_4periods &lt;- networkDynamic(network.list = list(talk_time1, talk_time2, talk_time3, talk_time4)) net_dynamic_4periods ## NetworkDynamic properties: ## distinct change times: 5 ## maximal time range: 0 until 4 ## ## Includes optional net.obs.period attribute: ## Network observation period info: ## Number of observation spells: 1 ## Maximal time range observed: 0 until 4 ## Temporal mode: discrete ## Time unit: step ## Suggested time increment: 1 ## ## Network attributes: ## vertices = 18 ## directed = FALSE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## net.obs.period: (not shown) ## total edges= 25 ## missing edges= 0 ## non-missing edges= 25 ## ## Vertex attribute names: ## active gender race vertex.names ## ## Edge attribute names: ## active Note that the default here is to set the start time (or onset) at 0 and the terminus at 4, defined as the period where no further change is recorded. We thus have a starting point of period 0 and changes occurring in period 1 , 2 and 3. Let's look at the object as a data frame. net_dynamic_4periods_dat &lt;- as.data.frame(net_dynamic_4periods) head(net_dynamic_4periods_dat) ## onset terminus tail head onset.censored terminus.censored duration edge.id ## 1 0 4 2 11 FALSE FALSE 4 1 ## 2 0 3 8 9 FALSE FALSE 3 2 ## 3 0 4 1 7 FALSE FALSE 4 3 ## 4 0 1 7 8 FALSE FALSE 1 4 ## 5 0 1 7 9 FALSE FALSE 1 5 ## 6 0 1 2 3 FALSE FALSE 1 6 We can see that there is an edge between node 2 and node 11 in period 0 (onset) and this lasts to the end of the observation period. As another example, the edge between node 7 and 8 is present in period 0 (duration = 1), but is dropped during period 1. We will also add an edge covariate to the networkDynamic object. Here we will include the seating arrangement in the classroom, as we might expect that students who sit close together are more likely to talk to one another. Let's read in the edgelist. url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/discrete_talk_nets_seating.txt&quot; seating_edgelist &lt;- read.table(file = url2, header = T) head(seating_edgelist) ## ego_id alter_id ## 1 1 7 ## 2 1 9 ## 3 1 15 ## 4 2 3 ## 5 2 5 ## 6 2 11 The data is stored as an edgelist, where i-&gt;j exists if node i is sitting near (i.e., adjacent) to node j. Let's turn the seating edgelist into a matrix and then add it to the networkDynamic object. We will first take the seating edgelist and turn it into a directed network object. We will then symmetrize it to make it undirected. We will use a 'weak' rule when symmetrizing the matrix, so if i is next to j or j is next to i, the matrix will have a 1 for both ij and ji (we do this to ensure that the matrix is logically consistent, so that if i is next to j, then j must be next to i). The symmetrize() function will output a matrix by default, which is what we want here. We will then attach the matrix to the networkDynamic object using set.network.attribute(). seating_network &lt;- network(x = seating_edgelist, directed = T, vertices = data.frame(ids = 1:18)) seating_matrix &lt;- symmetrize(seating_network, rule = &quot;weak&quot;) set.network.attribute(net_dynamic_4periods, &quot;seating&quot;, seating_matrix) 13.12 STERGMs We are now ready to run an initial STERG model, where the goal is to predict the adding and dropping (or keeping) of edges across discretely defined time periods. This amounts to running separate ERGMs predicting the formation and persistence/dissolution of edges, given the network at time T. Note that the tergm package will allow us to choose if we want to model the keeping of an edge (persistence) or dropping of an edge (dissolution) for the persistence/dissolution portion of the model. For the formation model, we run an ERGM predicting an edge between i-j in T+1, given that the i-j edge does not exist in the current network, time T. For the persistence model, we run an ERGM predicting an edge between i-j in time T+1, given that i-j does exist in time T. For the dissolution model, we run an ERGM predicting the absence of an edge between i-j in time T+1, given that i-j does exist in time T. In this way, STERGM is a direct extension of the exponential random graph models covered in the previous tutorial. The models are simply run with the addition of a time component, and corresponding conditioning to capture the formation and persistence/dissolution of edges. This means that the model specifications that work in the ERGM case will (mostly) be appropriate in the STERGM case. In this case, the network is undirected so we will only include terms appropriate for undirected networks. Let's load the tergm package (Krivitsky and Handcock 2023). Note this depends on networkDynamic and ergm. Let's also load tsna. library(tergm) library(tsna) 13.12.1 Model 1: Just Edges The main function is tergm(). The key arguments are: formula = formula specifying the formation and persistence/dissolution equation estimate = type of estimation to use in fitting model constraints = formula specifying any constraints to put on model control = list of inputs to control estimation algorithm; set using control.tergm() times = periods to include in estimating model Note that the formation and persistence/dissolution formulas are specified separately and do not need to be the same. The formation formula is set using Form(~...). The persistence/dissolution formulas can be set using Diss(~...) or Persist(~...). If we use Persist, then we will predict if the i-j edge remains from one period to the next. If we use Diss we predict the dissolution, or dropping, of edges from one period to next. For our example, we will focus on the persistence model. With the persistence model, positive effects make it more likely for an edge to last from one period to another. Note also that the constraints and control arguments work in very similar ways to what we saw in the ERGM case (see previous tutorial). For our first model, we will do something simple and only include a term for edges, capturing the base rate of tie formation/persistence. We set estimate to CMLE. stergm_mod1 &lt;- tergm(net_dynamic_4periods ~ Form(~ edges) + Persist(~ edges), estimate = &quot;CMLE&quot;, times = 0:3) summary(stergm_mod1) ## Call: ## tergm(formula = net_dynamic_4periods ~ Form(~edges) + Persist(~edges), ## estimate = &quot;CMLE&quot;, times = 0:3) ## ## Conditional Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## Form(1)~edges -3.2859 0.2630 0 -12.495 &lt;1e-04 *** ## Persist(1)~edges 0.4249 0.3119 0 1.362 0.173 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 636.3 on 459 degrees of freedom ## Residual Deviance: 186.8 on 457 degrees of freedom ## ## AIC: 190.8 BIC: 199.1 (Smaller is better. MC Std. Err. = 0) We can see that we have two different equations, one for tie formation and one for tie persistence. The results suggest that ties form at a lower rate than if people were randomly forming ties (based on the negative coefficient on edges for the formation model). To get a sense of what the results mean, we can look at the coefficient on edges for the tie persistence model. If we take the .4249, we can calculate the probability of an edge persisting from one period to the next: exp(.4249) / (1 + exp(.4249)) ## [1] 0.6046552 Thus, an edge that existed in one period has a .605 probability of still existing in the next period. Note that we set times to 0:3, as we want to model the formation and persistence of edges across our four time periods. If we tried to run the following bit of code, without times explicitly set, we would run into problems: wrong_mod1 &lt;- tergm(net_dynamic_4periods ~ Form(~ edges) + Persist(~ edges), estimate = \"CMLE\") If we had run the dissolution model, we would get the same basic results but the .4249 would now be negative (as we are predicting the dropping of a tie): summary(tergm(net_dynamic_4periods ~ Form(~ edges) + Diss(~ edges), estimate = &quot;CMLE&quot;, times = 0:3)) ## Call: ## tergm(formula = net_dynamic_4periods ~ Form(~edges) + Diss(~edges), ## estimate = &quot;CMLE&quot;, times = 0:3) ## ## Conditional Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## Form(1)~edges -3.2859 0.2630 0 -12.495 &lt;1e-04 *** ## Diss(1)~edges -0.4249 0.3119 0 -1.362 0.173 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 636.3 on 459 degrees of freedom ## Residual Deviance: 186.8 on 457 degrees of freedom ## ## AIC: 190.8 BIC: 199.1 (Smaller is better. MC Std. Err. = 0) 13.12.2 Model 2: Edges, Homophily and Nodefactor We will now try a little more interesting model, including nodematch and nodefactor terms for gender and race. The nodefactor terms capture basic differences in degree by the nodal attribute of interest (i.e., do girls talk more than boys in class?), while the nodematch terms capture if there is homophily on the attribute (do girls tend to talk to other girls in class?). Here we will keep the formulas for formation and persistence the same. stergm_mod2 &lt;- tergm(net_dynamic_4periods ~ Form(~ edges + nodematch(&quot;gender&quot;) + nodefactor(&quot;gender&quot;) + nodematch(&quot;race&quot;) + nodefactor(&quot;race&quot;)) + Persist(~ edges + nodematch(&quot;gender&quot;) + nodefactor(&quot;gender&quot;) + nodematch(&quot;race&quot;) + nodefactor(&quot;race&quot;)), estimate = &quot;CMLE&quot;, times = 0:3) summary(stergm_mod2) ## Call: ## tergm(formula = net_dynamic_4periods ~ Form(~edges + nodematch(&quot;gender&quot;) + ## nodefactor(&quot;gender&quot;) + nodematch(&quot;race&quot;) + nodefactor(&quot;race&quot;)) + ## Persist(~edges + nodematch(&quot;gender&quot;) + nodefactor(&quot;gender&quot;) + ## nodematch(&quot;race&quot;) + nodefactor(&quot;race&quot;)), estimate = &quot;CMLE&quot;, ## times = 0:3) ## ## Conditional Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## Form(1)~edges -3.01959 0.82990 0 -3.639 0.000274 *** ## Form(1)~nodematch.gender 0.63323 0.55982 0 1.131 0.258000 ## Form(1)~nodefactor.gender.male 0.40814 0.36992 0 1.103 0.269888 ## Form(1)~nodematch.race -0.08931 0.66430 0 -0.134 0.893056 ## Form(1)~nodefactor.race.white -1.07422 0.54052 0 -1.987 0.046879 * ## Persist(1)~edges 0.85108 1.13717 0 0.748 0.454209 ## Persist(1)~nodematch.gender 0.71104 0.78997 0 0.900 0.368074 ## Persist(1)~nodefactor.gender.male -0.89406 0.54060 0 -1.654 0.098163 . ## Persist(1)~nodematch.race -0.59615 0.75889 0 -0.786 0.432134 ## Persist(1)~nodefactor.race.white -0.08171 0.51961 0 -0.157 0.875045 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 636.3 on 459 degrees of freedom ## Residual Deviance: 172.0 on 449 degrees of freedom ## ## AIC: 192 BIC: 233.3 (Smaller is better. MC Std. Err. = 0) Race and gender would not appear to play a large role in the formation or persistence of edges in this classroom network, although there is some evidence that students who identify as white initiate interactions at lower rates. The fit (based on BIC) is actually worse than in model 1, just including edges. 13.12.3 Model 3: Edges, Nodefactor, and Seating As a third model, we will incorporate seating arrangements into the formation and persistence models. The question is whether students who sit close to one another are more likely to initiate, and then maintain, interaction ties. Here, we will include an edgecov term with the seating matrix as the input. Note that we have already included the seating matrix on the networkDynamic object above (so no further manipulation is necessary here). Given our results above, we will drop most of the gender and race terms from the model, just keeping the nodefactor terms for race in the formation model and gender in the persistence model. stergm_mod3 &lt;- tergm(net_dynamic_4periods ~ Form(~ edges + nodefactor(&quot;race&quot;) + edgecov(&quot;seating&quot;)) + Persist(~ edges + nodefactor(&quot;gender&quot;) + edgecov(&quot;seating&quot;)), estimate = &quot;CMLE&quot;, times = 0:3) summary(stergm_mod3) ## Call: ## tergm(formula = net_dynamic_4periods ~ Form(~edges + nodefactor(&quot;race&quot;) + ## edgecov(&quot;seating&quot;)) + Persist(~edges + nodefactor(&quot;gender&quot;) + ## edgecov(&quot;seating&quot;)), estimate = &quot;CMLE&quot;, times = 0:3) ## ## Conditional Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## Form(1)~edges -3.3301 0.4996 0 -6.666 &lt;1e-04 *** ## Form(1)~nodefactor.race.white -0.8750 0.4161 0 -2.103 0.0355 * ## Form(1)~edgecov.seating 2.3582 0.5574 0 4.231 &lt;1e-04 *** ## Persist(1)~edges 0.6369 1.2705 0 0.501 0.6161 ## Persist(1)~nodefactor.gender.male -0.8748 0.7035 0 -1.244 0.2137 ## Persist(1)~edgecov.seating 0.3550 1.1585 0 0.306 0.7593 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 636.3 on 459 degrees of freedom ## Residual Deviance: 158.3 on 453 degrees of freedom ## ## AIC: 170.3 BIC: 195 (Smaller is better. MC Std. Err. = 0) It looks like being physically close in the classroom increases the probability of forming a tie but less so for the probability of the tie persisting from one period to the next (although the standard errors are high here). The odds of a tie forming between i and j is exp(2.3582) = 10.572 times higher if i and j are sitting close together than if they are not sitting close together. This suggests the importance of opportunity for beginning social ties. The results are similar if we kept the full range of nodefactor and nodematch terms on race and gender. 13.12.4 Model 4: Adding GWESP to the Model For our last model, we will include a gwesp term, capturing if students form (and keep) edges when they share many common interaction partners. Here we will only include gwesp in the formation model, as including it in the persistence model leads to estimation problems. set.seed(107) stergm_mod4 &lt;- tergm(net_dynamic_4periods ~ Form(~ edges + nodefactor(&quot;race&quot;) + edgecov(&quot;seating&quot;) + gwesp(decay = .5, fixed = T)) + Persist(~ edges + nodefactor(&quot;gender&quot;) + edgecov(&quot;seating&quot;)), estimate = &quot;CMLE&quot;, times = 0:3, control = control.tergm(CMLE.ergm = control.ergm(MCMC.burnin = 50000, MCMC.interval = 3000, MCMC.samplesize = 6500))) The model with gwesp is estimated with MCMC estimation so we need to check the diagnostics, to make sure the model is converging. This is directly analogous to the kind of diagnostics we saw in the ERGM case. mcmc.diagnostics(stergm_mod4, vars.per.page = 4) There are two sets of diagnostics; first for the formation model and then for the persistence model. Looks okay on the whole, although we might consider rerunning with different input parameters or a simpler model. Let's take a look at the model results. summary(stergm_mod4) ## Call: ## tergm(formula = net_dynamic_4periods ~ Form(~edges + nodefactor(&quot;race&quot;) + ## edgecov(&quot;seating&quot;) + gwesp(decay = 0.5, fixed = T)) + Persist(~edges + ## nodefactor(&quot;gender&quot;) + edgecov(&quot;seating&quot;)), estimate = &quot;CMLE&quot;, ## control = control.tergm(CMLE.ergm = control.ergm(MCMC.burnin = 50000, ## MCMC.interval = 3000, MCMC.samplesize = 6500)), times = 0:3) ## ## Monte Carlo Conditional Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## Form(1)~edges -4.1382 0.5368 0 -7.708 &lt; 1e-04 *** ## Form(1)~nodefactor.race.white -0.6255 0.3811 0 -1.641 0.100728 ## Form(1)~edgecov.seating 1.4868 0.5749 0 2.586 0.009702 ** ## Form(1)~gwesp.fixed.0.5 0.9417 0.2810 0 3.351 0.000806 *** ## Persist(1)~edges 0.6543 1.2901 0 0.507 0.612008 ## Persist(1)~nodefactor.gender.male -0.8758 0.7157 0 -1.224 0.221065 ## Persist(1)~edgecov.seating 0.3374 1.1704 0 0.288 0.773100 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 636.3 on 459 degrees of freedom ## Residual Deviance: 146.3 on 452 degrees of freedom ## ## AIC: 160.3 BIC: 189.2 (Smaller is better. MC Std. Err. = 0.1137) The results suggest that students tend to form talking relationships when they have common interaction partners (looking at the positive, significant gwesp coefficient). So, if i and j talk with the same other students in Time T, they are likely to start talking with each other in T + 1. 13.13 Checking Model Fit Let's go ahead and check the fit of the model. One way of seeing if the model is fitting well is to simulate networks based on the underlying model and then compare the statistics in the simulated networks to that observed in the true networks (analogous to the gof() function in the ERGM case). Note that the simulations combine the formation and persistence processes and output the generated networks (based on both processes) at a given time slice. We will use a simulate() function. The main arguments are: object = model of interest nsim = number of separate replications; set to 1 in case of networkDynamic object time.slices = number of distinct periods to run dynamic simulation over nw.start = indicator for what network to start simulation at; nw.start=1 to begin with first observed network monitor = formula indicating statistics to calculate on the simulated networks. For our simulation we will set time.slices to 1000, so we simulate change over 1000 different time periods. Of course our actual data only has 4 time periods, but since the model is about the general tendencies of tie formation and persistence, having a larger number of time periods simply adds more information about how the model is fitting (i.e., can it generate realistic networks over many periods that correspond to what we saw in the actual data?). We will start the simulations at the time 1 network. We set monitor to edges and triadcensus (there are four types as the network is undirected). Note that triadcensus was not a term in the model. We use set.seed() to ease reproducibility. set.seed(130) sim_mod4 &lt;- simulate(stergm_mod4, nsim = 1, time.slices = 1000, nw.start = 1, monitor = ~ edges + triadcensus(c(0, 1, 2, 3))) If we take the generated object, sim_mod4, and use the attributes() function, we can grab the statistics for each generated network (specified in the monitor formula). This is housed under the stats part of the object. sim_stats &lt;- attributes(sim_mod4)$stats head(sim_stats) ## Markov Chain Monte Carlo (MCMC) output: ## Start = 1 ## End = 7 ## Thinning interval = 1 ## edges triadcensus.0 triadcensus.1 triadcensus.2 triadcensus.3 ## [1,] 14 619 177 13 7 ## [2,] 13 635 159 17 5 ## [3,] 10 667 140 7 2 ## [4,] 7 711 98 7 0 ## [5,] 12 645 153 15 3 ## [6,] 13 636 156 20 4 ## [7,] 11 665 129 19 3 There are 1000 rows, 1 for each generated network. We see that there is a column for edges and four columns for the triadcensus, showing the count for edges and the four triad types (null triads, triads with 1 tie, triads with 2 ties and triads with 3 ties) for each simulated network (or time period in the larger simulation). This can be compared to what we saw in the observed network. We will use a tErgmStats() function to calculate the statistics on the observed network. The main arguments are: nd = networkDynamic object formula = ergm formula describing terms to calculate statistics on start = period where calculations should start end = period where calculations should end We will set formula to include edges and triadcensus, matching what was calculated on the simulated networks above. We set start to 0 and end to 3, as we have 4 time periods, starting from 0 (onset). true_values &lt;- tErgmStats(nd = net_dynamic_4periods, formula = &#39;~ edges + triadcensus(c(0, 1, 2, 3))&#39;, start = 0, end = 3) true_values ## Time Series: ## Start = 0 ## End = 3 ## Frequency = 1 ## edges triadcensus.0 triadcensus.1 triadcensus.2 triadcensus.3 ## 0 14 613 188 9 6 ## 1 16 584 213 14 5 ## 2 13 625 177 11 3 ## 3 12 640 165 6 5 Here we summarize the statistics of the simulated networks using apply() (summarizing over the columns): sim_values &lt;- apply(sim_stats, 2, summary) And now let’s add some useful rownames to the values calculated above. rownames(sim_values) &lt;- paste(&quot;sim&quot;, rownames(sim_values), sep = &quot;_&quot;) sim_values ## edges triadcensus.0 triadcensus.1 triadcensus.2 triadcensus.3 ## sim_Min. 5.000 409.000 70.000 2.000 0.000 ## sim_1st Qu. 13.000 557.000 161.000 16.000 2.000 ## sim_Median 16.000 597.000 189.000 24.000 4.000 ## sim_Mean 15.979 595.934 189.173 26.188 4.705 ## sim_3rd Qu. 19.000 633.250 218.000 35.000 6.000 ## sim_Max. 34.000 741.000 294.000 89.000 24.000 Here we calculate the true values over the 4 periods as a means of comparison. true_values_mean &lt;- colMeans(true_values) Putting together the true and simulated values: rbind(sim_values, true_mean = true_values_mean) ## edges triadcensus.0 triadcensus.1 triadcensus.2 triadcensus.3 ## sim_Min. 5.000 409.000 70.000 2.000 0.000 ## sim_1st Qu. 13.000 557.000 161.000 16.000 2.000 ## sim_Median 16.000 597.000 189.000 24.000 4.000 ## sim_Mean 15.979 595.934 189.173 26.188 4.705 ## sim_3rd Qu. 19.000 633.250 218.000 35.000 6.000 ## sim_Max. 34.000 741.000 294.000 89.000 24.000 ## true_mean 13.750 615.500 185.750 10.000 4.750 Looks like the model is fitting okay, although the simulated networks have too many intransitive triads (triadcensus.2). We could imagine adjusting the model to account for that. As another way of assessing fit, we can extract networks from particular time periods and plot them against the observed network (we could also run a movie over all of the simulated networks). Here we will take the network from time period 10. We will use the network.extract() function, setting at to 10. net10 &lt;- network.extract(sim_mod4, at = 10) And now let’s plot the simulated network against the observed network, here just from the first 10 minute period. par(mfrow = c(1, 2)) plot(talk_time1, main = &quot;Observed Network from 0-10 Minutes&quot;) plot(net10, main = &quot;Example Simulated Network&quot;) Looks okay on the whole, although we might consider adding additional homophliy terms to try and deal with the over count of intransitive triads. Overall, our results suggest that seating arrangements structure the formation of interaction ties. Individuals sitting close together are likely to initiate interactions. Gender and race have relatively weak effects on the formation of ties. For example, even though there is one group of isolated girls, the overall effect of gender on formation of ties is not particularly strong. We also see that students tend to form new ties to people in their social cluster, with students initiating talking relationships with other students who have the same (current) partners as themselves. The tie persistence results are less clear, with few good predictors in the persistence model. In sum, the results suggest that classroom interactions are largely based on opportunity structure and norms of interaction: one tends to talk to other students in close proximity (socially or physically); once an initial discussion relationship is prompted, however, the interaction between i-j has its own internal dynamics largely independent of other, external factors. This tutorial has covered statistical models for discrete, longitudinal network data. In the next tutorial, we consider the application of ERGMs and STERGMs to the case of two-mode (or bipartite) networks. "],["ch13-Two-mode-Network-Models-ERGM-STERGM-R.html", "13, Part 3. Two-mode Network Models 13.14 Affiliation Data 13.15 ERGM on a Two-mode Network 13.16 Second Mode Terms 13.17 First Mode Terms 13.18 Clustering 13.19 Two-mode Longitudinal Networks 13.20 STERGM on a Two-mode Network", " 13, Part 3. Two-mode Network Models This is the third tutorial for Chapter 13, covering statistical network models in R. The first two tutorials covered network models for one-mode networks. Part 1 covered cross-sectional network models (ERGM), while Part 2 covered longitudinal network models (STERGM). In this tutorial, we will apply ERGMs and STERGMs to two-mode network data. Two-mode, or bipartite, networks are based on two types of actors (e.g., students and clubs). Ties exist between actors of different modes, but there are no ties between actors of the same mode. See Chapter 11 for details. We will draw heavily on the ERGM and STERGM tutorials, demonstrating how the specification and interpretation of the models must be rethought for two-mode networks. For example, even simple terms, like those capturing homophily, are not so simple when there are two kinds of actors in the network. We will cover cross-sectional models (ERGM) first, to get a sense of the model mechanics for two-mode networks. We will then cover longitudinal models (STERGM), appropriate for over time two-mode data. Our substantive case is based on students joining clubs in a high school. We introduced this example in Chapter 11. We will work with data from two years, 1996 and 1997. We have basic information about the students, the clubs, and which students are members of which clubs for both years. Our goal is to tease out how students join, drop and keep their club affiliations through time. For example, are certain kinds of clubs more effective at keeping their members than others? Do students join a mixture of clubs (sports and academic) or do they stick to one kind of club (just sports)? And how strongly do students segregate themselves along racial lines? More generally, are the processes of tie gain (students joining clubs) the same as tie loss (students leaving clubs), and what does that tell us about the social dynamics of the school? In answering these questions, we will build on our results from Chapter 11, where we found, for example, that club memberships were partly segregated along racial lines, while generalist, service clubs served to integrate the school, which was otherwise divided along tracks of sports, academic interest, etc. 13.14 Affiliation Data We will make use of the network package for this tutorial, as well as the ergm and tergm packages. library(network) library(ergm) library(tergm) library(networkDynamic) Let's begin by reading in our data. We will first read in the affiliation matrices for the 1996 and 1997 school years, showing the club memberships for each student in the school. We have separate files for each year. url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/affiliations_1996.txt&quot; affiliations96 &lt;- read.delim(file = url1, check.names = F) url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/affiliations_1997.txt&quot; affiliations97 &lt;- read.delim(file = url2, check.names = F) Let's look at the first six rows and columns for 1996. affiliations96[1:6, 1:6] ## Academic decathalon Art Club Asian Club Band, 8th Band, Jazz Band, Marching (Symphonic) ## 101498 0 0 0 0 0 0 ## 104452 0 0 0 0 0 1 ## 104456 0 0 0 0 0 0 ## 104462 0 0 0 0 0 0 ## 104471 0 0 0 0 0 0 ## 105215 0 0 0 0 0 0 Students are on the rows and clubs are on the columns. A 1 means that student i is part of club j in 1996 (e.g., the second student, 104452, is part of the symphonic marching band). Let's take a look at the dimensions of the affiliation matrices: dim(affiliations96) ## [1] 1295 91 dim(affiliations97) ## [1] 1295 91 We can see that there are 1295 students (on the rows) and 91 clubs (on the columns) for both 1996 and 1997. Note that the 1295 students reflects all students present in at least one year. Thus, students who are in the school in 1996 but leave (i.e., graduate) are in the 1996 and 1997 matrices. Similarly, students who enter in 1997, but are not in the school in 1996, are in both matrices. We will deal with node entry and exit later on. And now we will read in the attribute data. We will read in a combined data frame, containing the attributes of both students and clubs. This was constructed in Chapter 11. url3 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/attributes_students_clubs.txt&quot; attributes_students_clubs &lt;- read.delim(file = url3, stringsAsFactors = F) nrow(attributes_students_clubs) ## [1] 1386 Note that the data frame has 1386 rows, 1295 students + 91 clubs. Note also that the students come first in the data frame (the first 1295 rows), followed by the clubs. Let's look at the first six rows: head(attributes_students_clubs) ## ids type missing96 missing97 race gender grade96 grade97 club_type_detailed club_profile club_feeder club_type_gender club_type_grade ## 1 101498 student 0 0 white female 11 12 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 104452 student 0 0 black male 9 10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 104456 student 0 0 white female 9 10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 104462 student 0 0 black male 9 10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 104471 student 0 0 black female 9 10 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 105215 student 0 0 white female 11 12 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; And here we look at the last six rows (using a tail() function): tail(attributes_students_clubs) ## ids type missing96 missing97 race gender grade96 grade97 club_type_detailed club_profile club_feeder club_type_gender club_type_grade ## 1381 Volleyball, JV club 0 0 &lt;NA&gt; &lt;NA&gt; NA NA Team Sports moderate yes girls ninth_tenth_eleventh ## 1382 Volleyball, V club 0 0 &lt;NA&gt; &lt;NA&gt; NA NA Team Sports high no girls ninth+ ## 1383 Wrestling, 8th club 0 0 &lt;NA&gt; &lt;NA&gt; NA NA Individual Sports low yes boys eighth ## 1384 Wrestling, V club 0 0 &lt;NA&gt; &lt;NA&gt; NA NA Individual Sports moderate no boys ninth+ ## 1385 Yearbook Contributors club 0 0 &lt;NA&gt; &lt;NA&gt; NA NA Media moderate no boys_girls all_grades ## 1386 Yearbook Editors club 0 0 &lt;NA&gt; &lt;NA&gt; NA NA Media high no boys_girls all_grades The data frame includes student attributes, like race (Asian, black, Hispanic, Native American, white), as well as club attributes like club_profile (how much attention does the club get? low, moderate, high, very high) and club_type_detailed (Academic Interest, Academic Competition, Ethnic Interest, Individual Sports, Leadership, Media, Performance Art, Service, Team Sports). For all student attributes, like race and gender, the clubs get NA values, while for all club attributes, like club_profile, the students get NA values. See Chapter 11 for more details. The following variables have meaningful values for both students and clubs: ids (a unique identifier), type (student or club), missing96 (is node missing in 1996?), and missing97 (is node missing in 1997?). Let's do a quick table on type: table(attributes_students_clubs$type) ## ## club student ## 91 1295 Before we construct our bipartite network, we need to create two more variables, which will be essential when running the models below. We will first construct a variable called, studentgender_clubgender. This simply combines the values for gender, for students, with the values for club_type_gender, for clubs. gender shows the gender of the specific student (male or female), while club_type_gender shows if the club is just boys, mixed gender, or just girls. By forming a single variable, it will be easier to adjust the models for the fact that certain types of students (e.g., female) are unlikely to join certain types of clubs (just boy clubs). The values for gender (students) comes first, followed by club_type_gender (clubs). gender &lt;- attributes_students_clubs$gender students &lt;- attributes_students_clubs$type == &quot;student&quot; club_type_gender &lt;- attributes_students_clubs$club_type_gender clubs &lt;- attributes_students_clubs$type == &quot;club&quot; studentgender_clubgender &lt;- c(gender[students], club_type_gender[clubs]) attributes_students_clubs$studentgender_clubgender &lt;- studentgender_clubgender And now we do the same thing, combining student grade and club grade. grade96 shows the grade of the student in the 1996 school year while club_type_grade shows which grades (if any) the club is restricted to. Again, the goal is to make it easier to adjust for the fact that certain students (e.g., those in grade 12) are unlikely to join certain types of clubs (eighth grade football). We do this for grade in both 1996 and 1997. grade96 &lt;- attributes_students_clubs$grade96 grade97 &lt;- attributes_students_clubs$grade97 club_type_grade &lt;- attributes_students_clubs$club_type_grade studentgrade96_clubgrade &lt;- c(grade96[students], club_type_grade[clubs]) studentgrade97_clubgrade &lt;- c(grade97[students], club_type_grade[clubs]) attributes_students_clubs$studentgrade96_clubgrade &lt;- studentgrade96_clubgrade attributes_students_clubs$studentgrade97_clubgrade &lt;- studentgrade97_clubgrade We are now in a position to construct our bipartite (two-mode) networks. Here, we will just construct the network using the 1996 data, as we will use the 1996 network in the cross-sectional models. We will consider network change, between 1996 and 1997, in the STERGM section below. Note that in constructing our network, the attribute data frame must be sorted to be consistent with the affiliation matrix. This means that the students in the attribute data frame should be sorted in the same order as the rows in the affiliation matrix, while the clubs in the attribute data frame should be sorted in the same order as the columns of the affiliation matrix (with the students coming first in the attribute data frame followed by the clubs). This is already done in our case. And now we turn our attribute data frame into a list, to make it easier to construct the network (as the network input is a matrix, meaning we cannot use the vertices argument in the network() function). attribute_list &lt;- do.call(list, attributes_students_clubs) Let's also make sure our affiliation data is treated as a matrix in R: affiliations96 &lt;- as.matrix(affiliations96) And now we construct the network, setting bipartite to T and using the affiliation matrix and the attribute list as inputs. bnet96 &lt;- network(affiliations96, bipartite = T, vertex.attr = attribute_list) We now have a two-mode network based on the 1996 school year. We will, however, need to clean up the network a bit before we can run any models. We need to drop all students who are not in the high school in 1996. In particular, the school includes grades 8-12, so all students in grades 6 or 7 in 1996 need to be removed (as they joined the school in later years). All clubs are present in both years, so we only have to worry about entry/exit for the students. We will utilize the missing96 column in our combined data frame to remove those students not in the network in 1996. We begin by creating a copy of our network (as we do not want to remove nodes from the original network). We then use a delete.vertices() function to remove the nodes of interest, in this case all nodes where missing96 is equal to 1 (note that the delete.vertices() function will directly modify the input network). bnet96_nomiss &lt;- bnet96 missing96 &lt;- which(attributes_students_clubs[, &quot;missing96&quot;] == 1) delete.vertices(bnet96_nomiss, missing96) bnet96_nomiss ## Network attributes: ## vertices = 1029 ## directed = FALSE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = 938 ## total edges= 2641 ## missing edges= 0 ## non-missing edges= 2641 ## ## Vertex attribute names: ## club_feeder club_profile club_type_detailed club_type_gender club_type_grade gender grade96 grade97 ids missing96 missing97 race studentgender_clubgender studentgrade96_clubgrade studentgrade97_clubgrade type vertex.names ## ## Edge attribute names not shown We now have a two-mode network with 938 students and 91 clubs (1029 total). The students constitute the first mode while the clubs constitute the second mode (more generally, the rows are set as the first mode and columns as the second mode). A number of vertex attributes have also been added to the network, which will be useful when running the models. 13.15 ERGM on a Two-mode Network Our goal is to run a series of ERGMs on our two-mode network. The simplest option is to project the two-mode network into two one-mode networks, and analyze in the normal fashion. See Chapter 13, Part 1 for ERGMs on one-mode networks. Projecting the network is not an ideal solution, however, as this throws away important information, and can lead to incorrect inference. We will specify our models while maintaining both modes in the network. The network is defined by the ties between students and clubs, where a tie exists if student i is a member of club j. We are thus trying to predict which students are likely to be members of which clubs. The process of specifying an ERGM is similar to what we have already seen (in Part 1), but the model terms are often a little different in the case of two-mode data. We will walk through the process of specifying two-mode ERGMs step-by-step, building up the model as we go. 13.15.1 Edges We will start with a simple model and just include a term for edges. The edges term is directly analogous to the edges term seen in previous tutorials, except here we must remember that edges can only exist between actors of different modes. As we have specified our network as bipartite, the model is estimated assuming this structure, where edges are only possible between students and clubs. mod1 &lt;- ergm(bnet96_nomiss ~ edges) summary(mod1) ## Call: ## ergm(formula = bnet96_nomiss ~ edges) ## ## Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -3.44427 0.01977 0 -174.3 &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 118331 on 85358 degrees of freedom ## Residual Deviance: 23558 on 85357 degrees of freedom ## ## AIC: 23560 BIC: 23569 (Smaller is better. MC Std. Err. = 0) We can interpret the coefficient in terms of the baseline probability of a tie existing. The probability is: exp(-3.44427) / (1 + exp(-3.44427)) = .0309402. Thus, about 3 percent of all possible edges actually exist. If we want to see where this comes from, we can calculate the probability directly by taking the affiliation matrix, summing up over all cells (to get the number of edges) and dividing by the total number possible (the number of rows times the number of columns). affil_mat96 &lt;- as.matrix(bnet96_nomiss) sum(affil_mat96) / (nrow(affil_mat96) * ncol(affil_mat96)) ## [1] 0.03094028 13.15.2 Adjusting for Structural Issues: Gender and Grade Specific Clubs We now turn to some tricky structural issues in the data. Most pressing is the fact that some student-club combinations cannot occur, or occur at extremely low rates. We need to adjust our model for these cases, or run the risk of biasing the estimates. For example, girls are very unlikely to join boys sports teams (e.g., wrestling or football), while boys are unlikely to join girls sports teams (volleyball). There are similar structural issues with grade, as some clubs are restricted to certain grades. For example, students in grade 9, 10, 11 or 12 do not join eighth grade sports teams (e.g., 8th grade football). To account for these structural conditions, we will include nodemix terms in the model for gender and grade, capturing the student-club combinations that are unlikely to exist. The two attributes of interest are studentgender_clubgender and studentgrade96_clubgrade. By default, nodemix will include terms for all student-club combinations. We, however, only want to include terms for the rare cases of interest (e.g., boys joining girls sports). So, let's create two vectors, defining which terms we want to include for the grade and gender variables. We will start with gender. Remember that student gender is measured as male or female, while club gender is measured as boys, boys_girls, or girls. We want to print out the combinations of student-club genders (e.g., \"male.boys\"). By default, nodemix will print the terms sorted alphabetically, first by the second level (clubs) and then by the first level (students). Let's create our vector of names (student-club combinations) with that ordering in mind. student_gender &lt;- sort(as.character(c(&quot;female&quot;, &quot;male&quot;))) club_gender &lt;- sort(c(&quot;boys&quot;, &quot;boys_girls&quot;, &quot;girls&quot;)) gender_levels &lt;- paste(rep(student_gender, times = length(club_gender)), rep(club_gender, each = length(student_gender)), sep = &quot;.&quot; ) data.frame(gender_levels) ## gender_levels ## 1 female.boys ## 2 male.boys ## 3 female.boys_girls ## 4 male.boys_girls ## 5 female.girls ## 6 male.girls The terms of interest are female.boys and male.girls; as these are the rare events we want to adjust for, girls joining boys clubs and boys joining girls clubs. This corresponds to spots 1 and 6 from the summary statistics, so let's create a vector holding that information. gender_terms &lt;- c(1, 6) And now we look at the grade attribute, studentgrade96_clubgrade. Student grade is measured as 8, 9, 10, 11 or 12. Club grade is measured as: eighth (just eighth graders), ninth (just ninth graders), ninth_tenth_eleventh (just ninth, tenth or eleventh graders), and ninth+ (no eighth graders). The value is all_grades if there are no restrictions on membership, in terms of grade. Let's get all student-club combinations for grade and put them together to be consistent with the nodemix term (sorted by the second level and then by the first level): student_grades &lt;- sort(as.character(c(8:12))) club_grades &lt;- sort(c(&quot;all_grades&quot;, &quot;eight&quot;, &quot;ninth&quot;, &quot;ninth_tenth_eleventh&quot;, &quot;ninth+&quot;)) grade_levels &lt;- paste(rep(student_grades, times = length(club_grades)), rep(club_grades, each = length(student_grades)), sep = &quot;.&quot; ) data.frame(grade_levels) ## grade_levels ## 1 10.all_grades ## 2 11.all_grades ## 3 12.all_grades ## 4 8.all_grades ## 5 9.all_grades ## 6 10.eight ## 7 11.eight ## 8 12.eight ## 9 8.eight ## 10 9.eight ## 11 10.ninth ## 12 11.ninth ## 13 12.ninth ## 14 8.ninth ## 15 9.ninth ## 16 10.ninth_tenth_eleventh ## 17 11.ninth_tenth_eleventh ## 18 12.ninth_tenth_eleventh ## 19 8.ninth_tenth_eleventh ## 20 9.ninth_tenth_eleventh ## 21 10.ninth+ ## 22 11.ninth+ ## 23 12.ninth+ ## 24 8.ninth+ ## 25 9.ninth+ The grade variable is more complicated, but we want to include terms for any student-club combination that should not exist; like 12th graders in a ninth grade club, 12.ninth. Based on the order from the summary statistics, this corresponds to: 6 (10.eighth), 7 (11.eighth), 8 (12.eighth), 10 (9.eighth), 11 (10.ninth), 12 (11.ninth), 13 (12.ninth), 14 (8.ninth), 18 (12.ninth_tenth_eleventh), 19 (8.ninth_tenth_eleventh) and 24 (8.ninth+). grade_terms &lt;- c(6, 7, 8, 10, 11, 12, 13, 14, 18, 19, 24) We can now estimate our model, including nodemix terms for studentgender_clubgender and studentgrade96_clubgrade. We will specify which terms to include using the levels2 argument, setting it to the vectors defined above. To simplify the estimation of the model, we will specify these terms using an Offset() function (although we could have estimated the coefficients within the model). When using Offset(), the coefficients are not estimated and are instead set using the values supplied in the coef argument. This is appropriate in our case as the coefficients are based on logical conditions (e.g., 12th graders do not join 9th grade clubs) and can be set a priori by the researcher. Here, we set the coefficients to -10 for every term. We set the coefficients to -10 as we want to estimate the models conditioned on the fact that these student-club combinations are very rare. offset_coefs_gender &lt;- rep(-10, length(gender_terms)) offset_coefs_grade &lt;- rep(-10, length(grade_terms)) mod2 &lt;- ergm(bnet96_nomiss ~ edges + Offset(~ nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~ nodemix(&quot;studentgrade96_clubgrade&quot;, levels2 = grade_terms), coef = offset_coefs_grade)) summary(mod2) ## Call: ## ergm(formula = bnet96_nomiss ~ edges + Offset(~nodemix(&quot;studentgender_clubgender&quot;, ## levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade96_clubgrade&quot;, ## levels2 = grade_terms), coef = offset_coefs_grade)) ## ## Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -2.99179 0.01994 0 -150 &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 118331 on 85358 degrees of freedom ## Residual Deviance: 21515 on 85357 degrees of freedom ## ## AIC: 21517 BIC: 21526 (Smaller is better. MC Std. Err. = 0) All of the offset terms are set to -10, although they are not printed out here. We see that the edge coefficient is different than with mod1, suggesting the importance of adjusting our model for structural/logical constraints. Note also that the model fit should only be compared to other models with the same set of offset terms. In this case, we used nodemix terms to capture structural conditions in the data, but we could imagine using nodemix terms to answer more substantive questions. We could test if certain types of students (e.g., girls) are more likely to join certain types of clubs (academic, leadership, etc.), although we will not consider this here. 13.16 Second Mode Terms We will now do something a little more interesting, and add nodefactor and homophily terms to our model. With two-mode networks, nodefactor and homophily terms can take two forms, with different terms for the first and second mode. Here we focus on the second mode, the clubs. We consider the first mode, students, below. 13.16.1 Nodefactor Nodefactor terms capture differences in degree across nodes, here clubs, with different attributes. We are interested in the main effect of club type (sports, academic, etc.) and club profile (low, moderate, high, very high) on membership. For example, do high profile clubs have more members than low profile clubs? The term of interest is b2factor (b2 indicating the second mode of a bipartite network). We will include b2factor terms for each club attribute of interest, club_type_detailed and club_profile. We include a levels argument for club_profile to set the order that the results are printed. By default, the results are printed in alphabetical order. In this case, that would correspond to high (1), low (2), moderate (3), very high (4), with high excluded as the reference. But we want the results to run from moderate (3) to high (1) to very high (4), with low (2) excluded (as this is easier to interpret). For club_type_detailed, we use the levels argument to set the second category, academic interest, as the reference. We set levels to -2 to exclude only the second category. mod3a &lt;- ergm(bnet96_nomiss ~ edges + Offset(~nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade96_clubgrade&quot;, levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(&quot;club_type_detailed&quot;, levels = -2) + b2factor(&quot;club_profile&quot;, levels = c(3, 1, 4))) summary(mod3a) ## Call: ## ergm(formula = bnet96_nomiss ~ edges + Offset(~nodemix(&quot;studentgender_clubgender&quot;, ## levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade96_clubgrade&quot;, ## levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(&quot;club_type_detailed&quot;, ## levels = -2) + b2factor(&quot;club_profile&quot;, levels = c(3, 1, ## 4))) ## ## Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -2.82059 0.04035 0 -69.902 &lt;1e-04 *** ## b2factor.club_type_detailed.Academic Competition -0.44051 0.08242 0 -5.345 &lt;1e-04 *** ## b2factor.club_type_detailed.Ethnic Interest -1.03174 0.16677 0 -6.186 &lt;1e-04 *** ## b2factor.club_type_detailed.Individual Sports -0.41835 0.08391 0 -4.986 &lt;1e-04 *** ## b2factor.club_type_detailed.Leadership -0.36924 0.18887 0 -1.955 0.0506 . ## b2factor.club_type_detailed.Media -0.16730 0.14109 0 -1.186 0.2357 ## b2factor.club_type_detailed.Performance Art 0.10725 0.06436 0 1.666 0.0956 . ## b2factor.club_type_detailed.Service 0.31118 0.06262 0 4.970 &lt;1e-04 *** ## b2factor.club_type_detailed.Team Sports 0.45881 0.07798 0 5.883 &lt;1e-04 *** ## b2factor.club_profile.moderate -0.29070 0.05741 0 -5.064 &lt;1e-04 *** ## b2factor.club_profile.high -0.94788 0.09396 0 -10.088 &lt;1e-04 *** ## b2factor.club_profile.very_high -0.48185 0.11402 0 -4.226 &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 118331 on 85358 degrees of freedom ## Residual Deviance: 21116 on 85346 degrees of freedom ## ## AIC: 21140 BIC: 21252 (Smaller is better. MC Std. Err. = 0) We see, for example, that competitive academic clubs and individual sports tend to have fewer members than academic interest clubs (the reference), while service clubs (like National Honors Society) and team sports tend to be large. We also see that high profile clubs have, if anything, fewer members than low profile clubs. This largely reinforces the picture from Chapter 11, where we found that generalist service clubs occupied more central, integrating positions in the network. More formally, we can interpret the coefficient on individual sports (for example) as follows: the odds of a student being part of an individual sports team is exp(-0.41835) times lower than the odds of being part of an academic interest club. It is worth emphasizing that b2factor is based only on club attributes, and is thus different from nodemix (see above), which incorporates attributes from both modes. 13.16.2 Homophily We now turn to adding homophily terms to the model, still focusing on the attributes of the clubs. Homophily terms capture if actors tend to form ties with similar other actors. Homophily is more complicated with two-mode networks than with one-mode networks. This is the case as there are no direct ties from nodes of the same type; in our case, there are no ties from students to students or from clubs to clubs. So, if we are interested in homophily on a club attribute, say club type, we cannot ask if team sports are tied to other team sports, as there are no ties between clubs. Instead, we must ask if similar clubs are linked together through students; e.g., do students in team sports tend to be members of other team sports? More technically, we must shift to counts of two-paths, based on homophily on the attribute of interest. The basic idea is to sum up the number of times that we see A-i-B, where A and B are clubs with the same attribute (e.g., both team sports) and i is a student in both A and B. The count is technically over half the number of two-paths, to avoid double counting (as A-i-B is substantively the same as B-i-A). The term is b2nodematch. A positive coefficient on b2nodematch would indicate that students are members of similar kinds of clubs, above what can be explained by other terms in the model. One complication is that we must decide on how to sum up the homophilous two-paths. In the simplest case, we can sum up all of the two-paths that match on the attribute of interest. This is the default specification. We may, however, have good reason to incorporate some discounting, so that adding one more two-path (for a given edge) only marginally increases the count, once we reach a certain threshold. For example, if student i is a member of the football team, (i-football edge), then if i is also a member of the wrestling team, that would be a strong signal of matching on club type (both team sports). But adding another team sport membership, say i is also a member of the baseball team, may not offer quite as much information; as we already know that i joins team sports. We may then want to count the second two-path less than the first. We can control the discounting using a beta parameter, which raises the count of two-paths (for a given edge) to beta. Setting beta to 1 would yield the number of two-paths (for an edge) where there is a match on the club attribute of interest (so the i-football edge would contribute a count of 2). Setting beta to 0 gives us the other extreme: showing if the given edge is involved in at least one homophilous two-path (so the i-football edge would contribute a count of 1). The count of two-paths, raised to beta, is then summed over all edges and divided by 2. See Bomiriya (2014) for details. For our model, we will include nodematch terms for club type and club profile. We set beta to .25, but we could imagine using a range of values (estimating the model each time), using model fit statistics to evaluate the choice of beta. set.seed(1007) mod3b &lt;- ergm(bnet96_nomiss ~ edges + Offset(~nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade96_clubgrade&quot;, levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(&quot;club_type_detailed&quot;, levels = -2) + b2nodematch(&quot;club_type_detailed&quot;, beta = .25) + b2factor(&quot;club_profile&quot;, levels = c(3, 1, 4)) + b2nodematch(&quot;club_profile&quot;, beta = .25), control = control.ergm(MCMC.burnin = 20000, MCMC.samplesize = 3000)) The first thing to note is that the model is now being fit using MCMC techniques. This means that we should check if the model is fitting well. Let's use the mcmc.diagnostics() function. mcmc.diagnostics(mod3b) Looks okay in general, so let's go ahead and interpret the results. summary(mod3b) ## Call: ## ergm(formula = bnet96_nomiss ~ edges + Offset(~nodemix(&quot;studentgender_clubgender&quot;, ## levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade96_clubgrade&quot;, ## levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(&quot;club_type_detailed&quot;, ## levels = -2) + b2nodematch(&quot;club_type_detailed&quot;, beta = 0.25) + ## b2factor(&quot;club_profile&quot;, levels = c(3, 1, 4)) + b2nodematch(&quot;club_profile&quot;, ## beta = 0.25), control = control.ergm(MCMC.burnin = 20000, ## MCMC.samplesize = 3000)) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -3.596083 0.075080 0 -47.897 &lt; 1e-04 *** ## b2factor.club_type_detailed.Academic Competition -0.323232 0.085331 0 -3.788 0.000152 *** ## b2factor.club_type_detailed.Ethnic Interest -0.844453 0.169750 0 -4.975 &lt; 1e-04 *** ## b2factor.club_type_detailed.Individual Sports -0.296443 0.083610 0 -3.546 0.000392 *** ## b2factor.club_type_detailed.Leadership -0.171296 0.193888 0 -0.883 0.376978 ## b2factor.club_type_detailed.Media 0.028381 0.138718 0 0.205 0.837889 ## b2factor.club_type_detailed.Performance Art 0.129749 0.059862 0 2.167 0.030198 * ## b2factor.club_type_detailed.Service 0.361836 0.059245 0 6.107 &lt; 1e-04 *** ## b2factor.club_type_detailed.Team Sports 0.502725 0.075382 0 6.669 &lt; 1e-04 *** ## b2nodematch.club_type_detailed 0.413830 0.070045 0 5.908 &lt; 1e-04 *** ## b2factor.club_profile.moderate -0.075008 0.059536 0 -1.260 0.207713 ## b2factor.club_profile.high -0.538820 0.098625 0 -5.463 &lt; 1e-04 *** ## b2factor.club_profile.very_high -0.003789 0.124166 0 -0.031 0.975654 ## b2nodematch.club_profile 0.779436 0.088837 0 8.774 &lt; 1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 118331 on 85358 degrees of freedom ## Residual Deviance: 20965 on 85344 degrees of freedom ## ## AIC: 20993 BIC: 21124 (Smaller is better. MC Std. Err. = 0.9958) We can see that students do, in fact, tend to join clubs of the same type (based on the positive, significant coefficient for b2nodematch.club_type_detailed). Students in one academic competition club (debate) tend to be in other academic competition clubs (chess club). We see similar results for nodematch on club profile, as students in very high profile clubs (cheerleading) tend to be in other clubs that are very high profile (student council). This suggests something about the status hierarchy operating in the school, and offers a more explicit test than seen in Chapter 11. Chapter 11 showed that clubs tended to be grouped together along broad divides of sports, academic interest, etc., but here we can see more clearly how clubs of the same specific type and profile are tied at high rates. Note that these homophily coefficients are estimated net of the nodefactor terms. More generally, we see that two-mode ERGMs can differentiate between main effects of attributes (certain types of clubs have more members) and homophily (students join clubs with similar attributes). 13.17 First Mode Terms We now turn to adding the analogous terms (nodefactor and nodematch) for the first mode, students. Here we focus on student attributes, specifically for race. We will add b1factor and b1nodematch terms to the model (b1 indicating the first mode of a bipartite network). For b1factor, we test if there are differences in degree by racial groups (do white students join clubs at lower rates than Asian students?). For b1nodematch, we test if clubs are segregated along racial lines. We will count the number of two paths, i-A-j, where i and j are students of the same race and A is a club that both i and j are members of. Again, we can use the beta argument to set the discounting when summing up the two-paths that match on the attribute of interest (e.g., if student i joins club A and that creates 10 student to student two-paths that match on race, how should that 10 be discounted?). We will set beta to .15. Note, however, that if we set beta to 0 or 1 we would get the same basic calculations seen in the tutorial for Chapter 11, where we went over, step-by-step, how to calculate the number of matching two-paths on race. Section 11.6.1 corresponds to the case where beta is set to 1 (the total number of two-paths where students match on race), while Section 11.6.2 corresponds to setting beta to 0 (the number of student-club edges where students match racially with at least one other student in the club). To help with estimation convergence, we will also set the reference category for the b1factor term to include Asian (1), Hispanic (3) and Native American (4) (basically collapsing some of the smaller categories into a single 'other' category). Finally, we will tweak the control parameters, increasing the burnin and sample size. This can take a little while to run, so we might want to include parallel processing options to speed things up (here we set the number of processors to 4). mod4 &lt;- ergm(bnet96_nomiss ~ edges + Offset(~nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade96_clubgrade&quot;, levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(&quot;club_type_detailed&quot;, levels = -2) + b2nodematch(&quot;club_type_detailed&quot;, beta = .25) + b2factor(&quot;club_profile&quot;, levels = c(3, 1, 4)) + b2nodematch(&quot;club_profile&quot;, beta = .25) + b1factor(&quot;race&quot;, levels = -c(1, 3, 4)) + b1nodematch(&quot;race&quot;, beta = .15), control = control.ergm(MCMC.burnin = 30000, MCMC.samplesize = 5000, parallel = 4, parallel.type = &quot;PSOCK&quot;)) Let's go ahead and look at the results. summary(mod4) ## Call: ## ergm(formula = bnet96_nomiss ~ edges + Offset(~nodemix(&quot;studentgender_clubgender&quot;, ## levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade96_clubgrade&quot;, ## levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(&quot;club_type_detailed&quot;, ## levels = -2) + b2nodematch(&quot;club_type_detailed&quot;, beta = 0.25) + ## b2factor(&quot;club_profile&quot;, levels = c(3, 1, 4)) + b2nodematch(&quot;club_profile&quot;, ## beta = 0.25) + b1factor(&quot;race&quot;, levels = -c(1, 3, 4)) + b1nodematch(&quot;race&quot;, ## beta = 0.15), control = control.ergm(MCMC.burnin = 30000, ## MCMC.samplesize = 5000, parallel = 4, parallel.type = &quot;PSOCK&quot;)) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -6.83027 0.14712 0 -46.426 &lt;1e-04 *** ## b2factor.club_type_detailed.Academic Competition -0.05914 0.05116 0 -1.156 0.2477 ## b2factor.club_type_detailed.Ethnic Interest -0.24224 0.11227 0 -2.158 0.0310 * ## b2factor.club_type_detailed.Individual Sports 0.39357 0.05757 0 6.836 &lt;1e-04 *** ## b2factor.club_type_detailed.Leadership 0.10532 0.13252 0 0.795 0.4268 ## b2factor.club_type_detailed.Media 0.15606 0.09393 0 1.661 0.0966 . ## b2factor.club_type_detailed.Performance Art 0.17197 0.03435 0 5.006 &lt;1e-04 *** ## b2factor.club_type_detailed.Service 0.14154 0.03296 0 4.294 &lt;1e-04 *** ## b2factor.club_type_detailed.Team Sports 0.95282 0.05173 0 18.420 &lt;1e-04 *** ## b2nodematch.club_type_detailed 0.33851 0.06648 0 5.092 &lt;1e-04 *** ## b2factor.club_profile.moderate 0.07439 0.04003 0 1.858 0.0632 . ## b2factor.club_profile.high -0.10308 0.07348 0 -1.403 0.1607 ## b2factor.club_profile.very_high 0.16866 0.09232 0 1.827 0.0677 . ## b2nodematch.club_profile 0.69575 0.08628 0 8.064 &lt;1e-04 *** ## b1factor.race.black -1.11809 0.05122 0 -21.830 &lt;1e-04 *** ## b1factor.race.white -1.51223 0.06228 0 -24.281 &lt;1e-04 *** ## b1nodematch.race 5.04005 0.18534 0 27.194 &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 118331 on 85358 degrees of freedom ## Residual Deviance: 20437 on 85341 degrees of freedom ## ## AIC: 20471 BIC: 20630 (Smaller is better. MC Std. Err. = 1.304) It looks like black and white students are members of fewer clubs than Asian, Native American or Hispanic students (the reference), while there is segregation along racial lines (looking at the b1nodematch term). Students are unlikely to find themselves in clubs where there are few (or even no) students of the same race. For example, by chance we might expect Asian students (a small racial group) to often be in clubs with few other Asian students, but empirically this rarely happens. This offers a more formal test, and confirmation, of the basic results found in Chapter 11; unlike with Chapter 11, here we explicitly control for other factors, like differences in club size (across types of clubs) and differences in degree (number of affiliations) across racial groups. Looking at AIC and BIC, we see that the model fit is improved quite a bit from the previous model. 13.18 Clustering As a final set of models, we consider adding a term to capture clustering in the network. Here, we want to know if there are sets of students who join the same clubs. For example, we might expect a set of students to be joint members of chess, forensics and debate; another set might be part of cheerleading, yearbook, and student council. This opens up important questions about the patterns, or profiles, of club affiliations that students take on (we could also think of this in terms of identities). In ERGM-terms, we are asking if certain pairs of clubs share members at high rates; or, conversely, if certain pairs of students share a high number of clubs. In previous labs, we included a GWESP term to capture clustering. GWESP is based on the shared partner count around edges. With two-mode networks, we must shift to terms that capture the non-edgewise shared partner distribution. We want to count the number of students shared by clubs (or vice versa), but clubs are not tied together; we thus need to consider the shared partners around non-edges. We can specify the count of shared partners separately, based on the two modes of interest. The term is gwb2nsp (geometrically weighted non-edgewise shared partner distribution) when we focus on the second mode. In our case, gwb2nsp captures the number of shared students between pairs of clubs. The term is gwb1nsp for the first mode. In our case, this counts the number of shared clubs between pairs of students. As with GWESP, we can set a decay parameter to discount higher counts of shared partners (i.e., students or clubs). We need to read in the ergm.terms.contrib package to use the gwb1nsp or gwb2nsp terms, as these terms are user contributed (and currently not part of the main ergm package). Unfortunately, to use contributed ergm terms, we must install the package from source. This means that a user must have the required software already installed to compile the package (see https://mac.r-project.org/tools/ for the Mac operating system or https://cran.r-project.org/bin/windows/Rtools/ for Windows). We will also need the remotes package to install from the github location. remotes::install_github(&quot;statnet/ergm.terms.contrib&quot;) library(ergm.terms.contrib) The ergm.terms.contrib package should now be installed and loaded. This means that we can go ahead and run our ERGM with gwb2nsp or gwb1nsp as terms in the model. We will focus on gwb2nsp, looking at the clustering of students between pairs of clubs. For our first model, let's include edges, the offset terms and the gwb2nsp term. We will set the decay parameter to 3 for gwb2nsp. We set the decay parameter fairly high in this case because sharing a small number of students (or even a single student) between clubs may not be a strong sign of having shared clusters of students. We would likely set this lower if we were focusing on the first mode (as sharing 2 or 3 clubs would be a strong signal that two students join similar clubs). mod5a &lt;- ergm(bnet96_nomiss ~ edges + Offset(~nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade96_clubgrade&quot;, levels2 = grade_terms), coef = offset_coefs_grade) + gwb2nsp(3, fixed = T), control = control.ergm(MCMC.burnin = 30000, MCMC.samplesize = 5000, parallel = 4, parallel.type = &quot;PSOCK&quot;)) summary(mod5a) ## Call: ## ergm(formula = bnet96_nomiss ~ edges + Offset(~nodemix(&quot;studentgender_clubgender&quot;, ## levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade96_clubgrade&quot;, ## levels2 = grade_terms), coef = offset_coefs_grade) + gwb2nsp(3, ## fixed = T), control = control.ergm(MCMC.burnin = 30000, MCMC.samplesize = 5000, ## parallel = 4, parallel.type = &quot;PSOCK&quot;)) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -3.15961 0.04525 0 -69.82 &lt;1e-04 *** ## gwb2nsp.fixed.3 0.06358 0.01532 0 4.15 &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 118331 on 85358 degrees of freedom ## Residual Deviance: 21499 on 85356 degrees of freedom ## ## AIC: 21503 BIC: 21522 (Smaller is better. MC Std. Err. = 0.303) It looks like students do tend to cluster in sets of clubs. The positive coefficient for gwb2nsp suggests that clubs share students (or have common members) at a higher rate than if students were randomly part of clubs, given the baseline rate of club membership. Let's add in the nodematch and nodefactor terms for race (for students). mod5b &lt;- ergm(bnet96_nomiss ~ edges + Offset(~nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade96_clubgrade&quot;, levels2 = grade_terms), coef = offset_coefs_grade) + b1factor(&quot;race&quot;, levels = -c(1, 3, 4)) + b1nodematch(&quot;race&quot;, beta = .15) + gwb2nsp(3, fixed = T), control = control.ergm(MCMC.burnin = 30000, MCMC.samplesize = 5000, parallel = 4, parallel.type = &quot;PSOCK&quot;)) summary(mod5b) ## Call: ## ergm(formula = bnet96_nomiss ~ edges + Offset(~nodemix(&quot;studentgender_clubgender&quot;, ## levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade96_clubgrade&quot;, ## levels2 = grade_terms), coef = offset_coefs_grade) + b1factor(&quot;race&quot;, ## levels = -c(1, 3, 4)) + b1nodematch(&quot;race&quot;, beta = 0.15) + ## gwb2nsp(3, fixed = T), control = control.ergm(MCMC.burnin = 30000, ## MCMC.samplesize = 5000, parallel = 4, parallel.type = &quot;PSOCK&quot;)) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -5.79116 0.13374 0 -43.301 &lt;1e-04 *** ## b1factor.race.black -0.99290 0.05735 0 -17.313 &lt;1e-04 *** ## b1factor.race.white -1.32951 0.06515 0 -20.408 &lt;1e-04 *** ## b1nodematch.race 4.29586 0.17038 0 25.213 &lt;1e-04 *** ## gwb2nsp.fixed.3 0.07049 0.01434 0 4.917 &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 118331 on 85358 degrees of freedom ## Residual Deviance: 20977 on 85353 degrees of freedom ## ## AIC: 20987 BIC: 21034 (Smaller is better. MC Std. Err. = 1.595) It looks like clubs still share members at higher rates than expected by chance, controlling for racial segregation in club membership. As a final model, let’s add back in the nodefactor and nodematch terms for club attributes (club type and club profile). Here, we will change a few of the arguments in control.ergm to try and speed up the run time (e.g., relaxing the convergence criterion a bit). This can still take quite a bit to run. mod5c &lt;- ergm(bnet96_nomiss ~ edges + Offset(~nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade96_clubgrade&quot;, levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(&quot;club_type_detailed&quot;, levels = -2) + b2nodematch(&quot;club_type_detailed&quot;, beta = .25) + b2factor(&quot;club_profile&quot;, levels = c(3, 1, 4)) + b2nodematch(&quot;club_profile&quot;, beta = .25) + b1factor(&quot;race&quot;, levels = -c(1, 3, 4)) + b1nodematch(&quot;race&quot;, beta = .15) + gwb2nsp(3, fixed = T), control = control.ergm(MCMC.burnin = 30000, MCMC.samplesize = 5000, parallel = 4, parallel.type = &quot;PSOCK&quot;, MCMLE.confidence.boost = 1.25, MCMLE.confidence = .95)) summary(mod5c) ## Call: ## ergm(formula = bnet96_nomiss ~ edges + Offset(~nodemix(&quot;studentgender_clubgender&quot;, ## levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade96_clubgrade&quot;, ## levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(&quot;club_type_detailed&quot;, ## levels = -2) + b2nodematch(&quot;club_type_detailed&quot;, beta = 0.25) + ## b2factor(&quot;club_profile&quot;, levels = c(3, 1, 4)) + b2nodematch(&quot;club_profile&quot;, ## beta = 0.25) + b1factor(&quot;race&quot;, levels = -c(1, 3, 4)) + b1nodematch(&quot;race&quot;, ## beta = 0.15) + gwb2nsp(3, fixed = T), control = control.ergm(MCMC.burnin = 30000, ## MCMC.samplesize = 5000, parallel = 4, parallel.type = &quot;PSOCK&quot;, ## MCMLE.confidence.boost = 1.25, MCMLE.confidence = 0.95)) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -6.76588 0.14940 0 -45.287 &lt;1e-04 *** ## b2factor.club_type_detailed.Academic Competition -0.03669 0.05035 0 -0.729 0.4662 ## b2factor.club_type_detailed.Ethnic Interest -0.20416 0.11096 0 -1.840 0.0658 . ## b2factor.club_type_detailed.Individual Sports 0.41768 0.05720 0 7.302 &lt;1e-04 *** ## b2factor.club_type_detailed.Leadership 0.13056 0.13033 0 1.002 0.3164 ## b2factor.club_type_detailed.Media 0.18204 0.09297 0 1.958 0.0502 . ## b2factor.club_type_detailed.Performance Art 0.17674 0.03207 0 5.510 &lt;1e-04 *** ## b2factor.club_type_detailed.Service 0.14303 0.03126 0 4.575 &lt;1e-04 *** ## b2factor.club_type_detailed.Team Sports 0.96811 0.05028 0 19.254 &lt;1e-04 *** ## b2nodematch.club_type_detailed 0.39250 0.06824 0 5.752 &lt;1e-04 *** ## b2factor.club_profile.moderate 0.10803 0.04112 0 2.627 0.0086 ** ## b2factor.club_profile.high -0.04403 0.07813 0 -0.564 0.5731 ## b2factor.club_profile.very_high 0.23785 0.09240 0 2.574 0.0101 * ## b2nodematch.club_profile 0.79680 0.09384 0 8.491 &lt;1e-04 *** ## b1factor.race.black -1.14717 0.05869 0 -19.546 &lt;1e-04 *** ## b1factor.race.white -1.54076 0.06792 0 -22.685 &lt;1e-04 *** ## b1nodematch.race 5.00890 0.18445 0 27.156 &lt;1e-04 *** ## gwb2nsp.fixed.3 -0.04544 0.01793 0 -2.534 0.0113 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 118331 on 85358 degrees of freedom ## Residual Deviance: 20432 on 85340 degrees of freedom ## ## AIC: 20468 BIC: 20636 (Smaller is better. MC Std. Err. = 0.9783) We see that the coefficient on gwb2nsp is negative, and the fit is not improved over mod4 (at least with BIC), which has the same terms but excludes gwb2nsp. Thus, while clubs do share members at fairly high rates (looking at mod5a and mod5b), this is explained by the fact that students join clubs with similar attributes. This suggests that students join sets of clubs together, and that we have effectively identified what those clusters are. We see students track along club type and profile: high profile team sports; low profile team sports; low profile academic interest clubs, and so on. If students had crossed club type more strongly (joining wrestling and French club) then we might have seen a positive, significant coefficient for gwb2nsp, even controlling for homophily on club attributes. Our final model includes a large number of terms, but a researcher might consider running even more complicated models; for example, adjusting more carefully for the level of connectivity in the network. There are a number of options here. First, we could include stronger controls for the size, or degree, of the clubs. For example, we could include dummy terms for each club, rather than the nodefactor terms currently employed (using b2sociality). Second, we could control for the number of two-paths between clubs. This would allow us to estimate the homophily coefficients net of the baseline tendency for clubs to be connected to other clubs (through students). The basic idea is to include a term that counts all two-paths between clubs. We would accomplish this by including a b2nodematch term for an attribute where clubs all have the same value; the type attribute in this case. We would set beta to the same value as with the other b2nodematch terms: b2nodematch(\"type\", beta = .25). Finally, we could control for the number of two-paths between students, using an analogous b1nodematch term: b1nodematch(\"type\", beta = .15). Note that we are unlikely to include all of these terms in the same model (given the estimation problems). Overall, we have learned a lot from our initial ERG models. For example, we see that students are more likely to be in clubs where there are other student's of the same race. We also see that students are members of clubs that match on type and profile, so that incongruent membership is discouraged. And finally, we see that there are high levels of overlap in club memberships (so clustering of students in clubs); this is explained, however, by the tracked nature of club membership, where students join clubs of a similar type and profile. More generally, we see the ideas of intersecting social circles and duality come out in this formal analysis (see Chapter 11); as individuals are constituted by the club 'track' they are in (academic, sports, etc.), while the divides between clubs (i.e., the status hierarchy) is based on the memberships of students. 13.19 Two-mode Longitudinal Networks We have so far run an initial set of cross-sectional models. We now turn to network change, where we want to model how students change club memberships over time. We thus want to predict when students add an edge (join a club) or drop an edge (quit a club). Here we will employ STERGMs, which we explored previously in Part 2. The basic idea is to run two models, formation and persistence. For formation, we predict the adding of an edge, given that the edge did not exist in the previous period; for persistence, we predict the keeping of an edge, given that the edge did exist in the previous period. 13.19.1 Entry and Exit of Nodes We will examine changes in club membership between the 1996 and 1997 school years. In examining over time change, we must deal with the fact that the student population is not constant from one year to the next. Students exit the network through graduation, with most students in grade 12 in 1996 no longer in the network in 1997. Students enter the network as they move from middle school (grades 6 and 7) to high school (8-12). Thus, those in grade 7 in 1996 would not be in the 1996 network, but would be in the 1997 network. We must deal with the problem of node entry/exit prior to estimating the model. The simplest option is to remove any node that is not present in both years. This reduces the case base to just those nodes who have the opportunity to join and quit clubs during the observation period. This strategy has the advantage of offering a clear interpretation and is simple to implement. The downside is that we needlessly remove nodes, and thus edges, from the time 1 network, which the STERGM is ultimately conditioned on. For example, what is actually a clustering effect (shared partners between clubs) may look like a nodal effect (some clubs attract more members), if the students connecting the two clubs are removed from the network. Alternatively, we can estimate the model while trying to retain information about the exiting nodes. The basic idea is to include all nodes in the analysis who are present in the first time period, 1996, even if they exit in 1997. This keeps the 1996 network exactly as we ran it above, avoiding any distortion from removing the exiting nodes. It also means that students who exit the school (i.e., graduate) will be technically present when constructing the 1997 network. We can deal with this problem by adding structural zeros to the model, indicating that all students who are missing in 1997 should have no ties to clubs. In this way, the exiting nodes inform the model only in providing information about the baseline network (from which we model ties being added or dropped). We also need to deal with nodes who come into the school in 1997 (i.e., were 7th graders in 1996 and 8th graders in 1997). Nodes entering in 1997 have no edge information for 1996, while the model rests on being able to condition on the 1996 network. We thus remove all nodes from the analysis who enter in 1997. They would, however, have been kept in any model that examined the transitions between 1997 and 1998. Thus, nodes are incorporated into the model from the point in which they fully enter the setting. In sum, the set of nodes in our 1996 network will match the set of nodes in our 1997 network, both consisting of all nodes present in 1996. Let’s go ahead and get our networks ready to run the models. The 1996 network is the same as above, so we just need to deal with 1997. Let's first create the 1997 network with all nodes included. In this case, the attributes are the same as with 1996, so we can use attribute_list again as input. affiliations97 &lt;- as.matrix(affiliations97) bnet97 &lt;- network(affiliations97, bipartite = T, vertex.attr = attribute_list) And now we remove all nodes from the 1997 network that are not present in 1996 (defined in missing96). bnet97_present96 &lt;- bnet97 delete.vertices(bnet97_present96, missing96) Let’s check and make sure that worked as expected. We will do a quick table on student grade for the 1997 network (after extracting the attribute using get.vertex.attribute()). table(get.vertex.attribute(bnet97_present96, &quot;grade97&quot;)) ## ## 9 10 11 12 13 ## 191 204 208 153 182 This looks right. There are no 8th graders but there is a grade '13', indicating that they have already graduated and are no longer in the school. And now we can go ahead and create a networkDynamic object from our two networks. The networkDynamic object will serve as one of the main inputs to the STERGMs run below. Here, we will create a very simple networkDynamic object, using a list of already created networks (in order) as input. net_list &lt;- list(bnet96_nomiss, bnet97_present96) net_dynamic_9697 &lt;- networkDynamic(network.list = net_list) ## Neither start or onsets specified, assuming start=0 ## Onsets and termini not specified, assuming each network in network.list should have a discrete spell of length 1 ## Argument base.net not specified, using first element of network.list instead ## Created net.obs.period to describe network ## Network observation period info: ## Number of observation spells: 1 ## Maximal time range observed: 0 until 2 ## Temporal mode: discrete ## Time unit: step ## Suggested time increment: 1 It will be useful to have a bit more information on the missing nodes in 1997 (i.e., those who were in the 1996 network but then exited). Nodes who are not present in 1997 cannot have memberships in clubs, and we need to treat any dyads involving those missing nodes as structural zeros for that year. To accomplish this, we need to know which dyads should be treated as missing for 1997. We will create a matrix of 0s and 1s, 0 if an edge could exist between student and club and 1 if not. We will use an outer() function to create this matrix. The basic idea is to first grab two vectors, the type of node (student or club) and missing status in 1997 (0 = not missing and 1 = missing). We then take the student values for missing and add it to the club values for missing, doing this over all pairs of student-clubs using an outer() function. A 0 means that student and club are both present; a 1 means that one node in the dyad (student or club) is missing; and 2 means that both student and club are missing (although we do not see this in our data). We then set all values greater than 0 to missing, indicating that at least one node in the dyad is not present in 1997. type &lt;- get.vertex.attribute(net_dynamic_9697, &quot;type&quot;) missing97 &lt;- get.vertex.attribute(net_dynamic_9697, &quot;missing97&quot;) missing_dyads97 &lt;- outer(missing97[type == &quot;student&quot;], missing97[type==&quot;club&quot;], &#39;+&#39;) missing_dyads97[missing_dyads97 &gt; 0] &lt;- 1 And now we use a set.network.attribute() function to attach our matrix to the networkDynamic object. set.network.attribute(net_dynamic_9697, &quot;missing_dyad&quot;, missing_dyads97) We are now in a position to run our longitudinal network models. 13.20 STERGM on a Two-mode Network We will run a series of models to capture change in our two-mode network. For the first model, we will keep things simple and just estimate a baseline model of tie change. The model will include edges, as well as an offset term for the missing dyads. We include the missing dyad matrix as an edge covariate, setting the coefficient to -1e100, as these edges cannot exist in 1997 (we could set the coefficient to -Inf, but this can sometimes lead to problems when calculating the fit statistics). The model is kept the same for both the formation and persistence models. We again set the offset coefficients using Offset(). We set times to 0:1 as there are two time periods. offset_coef_miss &lt;- -1e100 stergm_mod1 &lt;- tergm(net_dynamic_9697 ~ Form(~ edges + Offset(~ edgecov(&quot;missing_dyad&quot;), coef = offset_coef_miss)) + Persist(~ edges + Offset(~ edgecov(&quot;missing_dyad&quot;), coef = offset_coef_miss)), estimate = &quot;CMLE&quot;, times = 0:1) summary(stergm_mod1) ## Call: ## tergm(formula = net_dynamic_9697 ~ Form(~edges + Offset(~edgecov(&quot;missing_dyad&quot;), ## coef = offset_coef_miss)) + Persist(~edges + Offset(~edgecov(&quot;missing_dyad&quot;), ## coef = offset_coef_miss)), estimate = &quot;CMLE&quot;, times = 0:1) ## ## Conditional Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## Form(1)~edges -3.87434 0.02762 0 -140.269 &lt;1e-04 *** ## Persist(1)~edges -0.41837 0.04640 0 -9.016 &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 118331 on 85358 degrees of freedom ## Residual Deviance: 15678 on 85356 degrees of freedom ## ## AIC: 15682 BIC: 15700 (Smaller is better. MC Std. Err. = 0) This tells us that there is a exp(-3.874) / (1 + exp(-3.874)) = .02 probability of a student joining a new club between 1996 and 1997 (looking at the formation model). Similarly, there is a exp(-0.4184) / (1 + exp(-0.4184)) = .397 probability of staying in a club that one is currently a member of. These probabilities only pertain to students who have the opportunity to join and quit clubs between 1996 and 1997 (i.e., only those students who have values of 0 in the missing dyad matrix). Note that the fixed coefficients (for the missing dyad edge covariates) are not printed here. We will now start to build up our model, first including offset terms for gender and grade specific clubs. Controlling for gender specific clubs is the same as before, but we need to update our offset term for grade (as there are no eighth graders in the 1997 network). We perform the same basic procedure as in the cross-sectional case, but here student grade runs from 9 to 13. student_grades_9697 &lt;- sort(as.character(c(9:13))) grade_levels_9697 &lt;- paste(rep(student_grades_9697, times = length(club_grades)), rep(club_grades, each = length(student_grades_9697)), sep = &quot;.&quot; ) data.frame(grade_levels_9697) ## grade_levels_9697 ## 1 10.all_grades ## 2 11.all_grades ## 3 12.all_grades ## 4 13.all_grades ## 5 9.all_grades ## 6 10.eight ## 7 11.eight ## 8 12.eight ## 9 13.eight ## 10 9.eight ## 11 10.ninth ## 12 11.ninth ## 13 12.ninth ## 14 13.ninth ## 15 9.ninth ## 16 10.ninth_tenth_eleventh ## 17 11.ninth_tenth_eleventh ## 18 12.ninth_tenth_eleventh ## 19 13.ninth_tenth_eleventh ## 20 9.ninth_tenth_eleventh ## 21 10.ninth+ ## 22 11.ninth+ ## 23 12.ninth+ ## 24 13.ninth+ ## 25 9.ninth+ Let's identify which of those terms should be included in the model. Again, we want to include all combinations that are not possible (or at least very unlikely), such as a 12th grader in an eighth grade club (12.eighth). We will, however, not include any terms involving grade 13, as this is handled separately in the offset term for the missing dyads (as everyone in grade 13 is missing in 1997). We include terms for: 6 (10.eighth), 7 (11.eighth), 8 (12.eighth), 10 (9.eighth), 11 (10.ninth), 12 (11.ninth), 13 (12.ninth) and 18 (12.ninth_tenth_eleventh). grade_terms_9697 &lt;- c(6, 7, 8, 10, 11, 12, 13, 18) As above, we will set the coefficients for these terms to be -10. Let's go ahead and create a vector of coefficients (of -10s) to use in the tergm() function. offset_coefs_grade_9697 &lt;- rep(-10, length(grade_terms_9697)) We are now ready to run our model, including controls for gender and grade specific clubs (note that we add the offset coefficients for gender and grade twice, once for the formation and once for the persistence models). stergm_mod2 &lt;- tergm(net_dynamic_9697 ~ Form(~ edges + Offset(~ edgecov(&quot;missing_dyad&quot;), coef = offset_coef_miss) + Offset(~ nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~ nodemix(&quot;studentgrade97_clubgrade&quot;, levels2 = grade_terms_9697), coef = offset_coefs_grade_9697)) + Persist(~ edges + Offset(~ edgecov(&quot;missing_dyad&quot;), coef = offset_coef_miss) + Offset(~ nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~ nodemix(&quot;studentgrade97_clubgrade&quot;, levels2 = grade_terms_9697), coef = offset_coefs_grade_9697)), estimate = &quot;CMLE&quot;, times = 0:1) summary(stergm_mod2) ## Call: ## tergm(formula = net_dynamic_9697 ~ Form(~edges + Offset(~edgecov(&quot;missing_dyad&quot;), ## coef = offset_coef_miss) + Offset(~nodemix(&quot;studentgender_clubgender&quot;, ## levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade97_clubgrade&quot;, ## levels2 = grade_terms_9697), coef = offset_coefs_grade_9697)) + ## Persist(~edges + Offset(~edgecov(&quot;missing_dyad&quot;), coef = offset_coef_miss) + ## Offset(~nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), ## coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade97_clubgrade&quot;, ## levels2 = grade_terms_9697), coef = offset_coefs_grade_9697)), ## estimate = &quot;CMLE&quot;, times = 0:1) ## ## Conditional Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## Form(1)~edges -3.44446 0.02777 0 -124.030 &lt; 1e-04 *** ## Persist(1)~edges -0.15714 0.04908 0 -3.202 0.00137 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 118331 on 85358 degrees of freedom ## Residual Deviance: 14382 on 85356 degrees of freedom ## ## AIC: 14386 BIC: 14405 (Smaller is better. MC Std. Err. = 0) And now let's go ahead and add nodefactor and nodematch terms for our club attributes. We will also include a nodefactor term for whether the club is a feeder club, part of a clear developmental trajectory in the school (e.g., low French feeds high French). set.seed(104) stergm_mod3 &lt;- tergm(net_dynamic_9697 ~ Form(~ edges + Offset(~ edgecov(&quot;missing_dyad&quot;), coef = offset_coef_miss) + Offset(~ nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~ nodemix(&quot;studentgrade97_clubgrade&quot;, levels2 = grade_terms_9697), coef = offset_coefs_grade_9697) + b2factor(&quot;club_type_detailed&quot;, levels = -2) + b2nodematch(&quot;club_type_detailed&quot;, beta = .25) + b2factor(&quot;club_profile&quot;, levels = c(3, 1, 4)) + b2nodematch(&quot;club_profile&quot;, beta = .25) + b2factor(&quot;club_feeder&quot;)) + Persist(~ edges + Offset(~ edgecov(&quot;missing_dyad&quot;), coef = offset_coef_miss) + Offset(~ nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~ nodemix(&quot;studentgrade97_clubgrade&quot;, levels2 = grade_terms_9697), coef = offset_coefs_grade_9697) + b2factor(&quot;club_type_detailed&quot;, levels = -2) + b2nodematch(&quot;club_type_detailed&quot;, beta = .25) + b2factor(&quot;club_profile&quot;, levels = c(3, 1, 4)) + b2nodematch(&quot;club_profile&quot;, beta = .25) + b2factor(&quot;club_feeder&quot;)), estimate = &quot;CMLE&quot;, times = 0:1, control = control.tergm(CMLE.ergm = control.ergm(MCMC.burnin = 50000, MCMC.samplesize = 4000, parallel = 4, parallel.type = &quot;PSOCK&quot;))) summary(stergm_mod3) ## Call: ## tergm(formula = net_dynamic_9697 ~ Form(~edges + Offset(~edgecov(&quot;missing_dyad&quot;), ## coef = offset_coef_miss) + Offset(~nodemix(&quot;studentgender_clubgender&quot;, ## levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade97_clubgrade&quot;, ## levels2 = grade_terms_9697), coef = offset_coefs_grade_9697) + ## b2factor(&quot;club_type_detailed&quot;, levels = -2) + b2nodematch(&quot;club_type_detailed&quot;, ## beta = 0.25) + b2factor(&quot;club_profile&quot;, levels = c(3, 1, ## 4)) + b2nodematch(&quot;club_profile&quot;, beta = 0.25) + b2factor(&quot;club_feeder&quot;)) + ## Persist(~edges + Offset(~edgecov(&quot;missing_dyad&quot;), coef = offset_coef_miss) + ## Offset(~nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), ## coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade97_clubgrade&quot;, ## levels2 = grade_terms_9697), coef = offset_coefs_grade_9697) + ## b2factor(&quot;club_type_detailed&quot;, levels = -2) + b2nodematch(&quot;club_type_detailed&quot;, ## beta = 0.25) + b2factor(&quot;club_profile&quot;, levels = c(3, ## 1, 4)) + b2nodematch(&quot;club_profile&quot;, beta = 0.25) + b2factor(&quot;club_feeder&quot;)), ## estimate = &quot;CMLE&quot;, control = control.tergm(CMLE.ergm = control.ergm(MCMC.burnin = 50000, ## MCMC.samplesize = 4000, parallel = 4, parallel.type = &quot;PSOCK&quot;)), ## times = 0:1) ## ## Monte Carlo Conditional Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## Form(1)~edges -4.67344 0.10933 0 -42.748 &lt; 1e-04 *** ## Form(1)~b2factor.club_type_detailed.Academic Competition 0.06181 0.11382 0 0.543 0.587102 ## Form(1)~b2factor.club_type_detailed.Ethnic Interest -0.35747 0.25486 0 -1.403 0.160730 ## Form(1)~b2factor.club_type_detailed.Individual Sports -0.23608 0.12794 0 -1.845 0.064997 . ## Form(1)~b2factor.club_type_detailed.Leadership -0.23584 0.27521 0 -0.857 0.391474 ## Form(1)~b2factor.club_type_detailed.Media 0.20294 0.18768 0 1.081 0.279559 ## Form(1)~b2factor.club_type_detailed.Performance Art -0.04830 0.09775 0 -0.494 0.621195 ## Form(1)~b2factor.club_type_detailed.Service 0.75422 0.08283 0 9.105 &lt; 1e-04 *** ## Form(1)~b2factor.club_type_detailed.Team Sports 0.14426 0.12871 0 1.121 0.262347 ## Form(1)~b2nodematch.club_type_detailed 1.22390 0.08041 0 15.221 &lt; 1e-04 *** ## Form(1)~b2factor.club_profile.moderate 0.19375 0.08097 0 2.393 0.016709 * ## Form(1)~b2factor.club_profile.high 0.12995 0.12495 0 1.040 0.298318 ## Form(1)~b2factor.club_profile.very_high 0.35921 0.16618 0 2.162 0.030654 * ## Form(1)~b2nodematch.club_profile 0.63381 0.10365 0 6.115 &lt; 1e-04 *** ## Form(1)~b2factor.club_feeder.yes 0.18988 0.12656 0 1.500 0.133535 ## Persist(1)~edges -0.66138 0.11327 0 -5.839 &lt; 1e-04 *** ## Persist(1)~b2factor.club_type_detailed.Academic Competition -0.08587 0.22444 0 -0.383 0.702010 ## Persist(1)~b2factor.club_type_detailed.Ethnic Interest 0.45137 0.43037 0 1.049 0.294274 ## Persist(1)~b2factor.club_type_detailed.Individual Sports 0.07897 0.20851 0 0.379 0.704875 ## Persist(1)~b2factor.club_type_detailed.Leadership -1.24746 0.46130 0 -2.704 0.006847 ** ## Persist(1)~b2factor.club_type_detailed.Media -1.21137 0.41235 0 -2.938 0.003306 ** ## Persist(1)~b2factor.club_type_detailed.Performance Art -0.57202 0.16506 0 -3.465 0.000529 *** ## Persist(1)~b2factor.club_type_detailed.Service 0.48884 0.15411 0 3.172 0.001514 ** ## Persist(1)~b2factor.club_type_detailed.Team Sports -0.22293 0.29295 0 -0.761 0.446670 ## Persist(1)~b2nodematch.club_type_detailed 0.60685 0.17468 0 3.474 0.000513 *** ## Persist(1)~b2factor.club_profile.moderate 0.85247 0.17311 0 4.924 &lt; 1e-04 *** ## Persist(1)~b2factor.club_profile.high 1.45816 0.29432 0 4.954 &lt; 1e-04 *** ## Persist(1)~b2factor.club_profile.very_high 1.83857 0.36209 0 5.078 &lt; 1e-04 *** ## Persist(1)~b2nodematch.club_profile 0.44846 0.16246 0 2.761 0.005771 ** ## Persist(1)~b2factor.club_feeder.yes -1.37295 0.29975 0 -4.580 &lt; 1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 118331 on 85358 degrees of freedom ## Residual Deviance: 15548 on 85328 degrees of freedom ## ## AIC: 15608 BIC: 15888 (Smaller is better. MC Std. Err. = 2.589) Looking at the formation model, we see that students join service clubs at high rates (compared to academic interest clubs, the reference) while there are relatively small differences in the rate of joining across low to high profile clubs (looking at the b2factor terms). For the nodematch terms, we see that students join clubs in a manner consistent with homophily, in terms of club type and club profile. Students are more likely to join a club if that club is similar to the clubs they are already members of (thus creating a two-step that matches on club type). Students who are members of one team sport (varsity football) in 1996 are likely to join another team sport in 1997. There is also homophily on club profile, as students in high profile clubs (in 1996) are likely to join other high profile clubs (in 1997). Moving to the persistence model, we see that students tend to stay in clubs (or persist) when the clubs are the same type and profile. Students are thus more likely to leave clubs when their memberships are not of the same type/profile. For example, a student in quiz bowl and academic decathlon is more likely to remain in quiz bowl than a student in quiz bowl and boys basketball. Students thus join clubs consistent with their general membership milieu and drop clubs when there are contradictions. We also see that high profile clubs are more likely to keep their members (looking at the b2factor terms on profile, with low as the reference), as are service clubs. Thus, while high profile clubs do not necessarily recruit members at higher rates (based on the formation results), they are more likely to retain members over time. We also see that feeder clubs lose members at high rates (as students develop, moving from the feeder club to the next level; e.g., low French to high French). More generally, we can see that service clubs tend to be integrating in the school, as students join and stay in these clubs at high rates. This is partly due to the low level of commitment, serving as easy to join 'blow-off' clubs. Still, as we saw in Chapter 11, these clubs are heterogeneous along gender and grade lines, serving a potentially important role in bringing disparate parts of the school together. The fact that students tend to stay in these clubs makes the integrating story more compelling. On the other hand, we see how high profile clubs serve, if anything, to divide the school: high profile clubs tend to be smaller and exclusive, and members are likely to remain members through time. Let's check to make sure the model is fitting okay. mcmc.diagnostics(stergm_mod3) Looks generally okay, so let's move to our fourth model, where we add nodefactor and nodematch terms for race. To limit the run time of this model, we will only include nodefactor terms for the club features (although the results are substantively similar if we also included nodematch terms for club_type_detailed and club_profile). We will also tweak some of the control arguments. stergm_mod4 &lt;- tergm(net_dynamic_9697~ Form(~ edges + Offset(~ edgecov(&quot;missing_dyad&quot;), coef = offset_coef_miss) + Offset(~ nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~ nodemix(&quot;studentgrade97_clubgrade&quot;, levels2 = grade_terms_9697), coef = offset_coefs_grade_9697) + b2factor(&quot;club_type_detailed&quot;, levels = -2) + b2factor(&quot;club_profile&quot;, levels = c(3, 1, 4)) + b2factor(&quot;club_feeder&quot;) + b1factor(&quot;race&quot;, levels = -c(1, 3, 4)) + b1nodematch(&quot;race&quot;, beta = .15)) + Persist(~ edges + Offset(~ edgecov(&quot;missing_dyad&quot;), coef = offset_coef_miss) + Offset(~ nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~ nodemix(&quot;studentgrade97_clubgrade&quot;, levels2 = grade_terms_9697), coef = offset_coefs_grade_9697) + b2factor(&quot;club_type_detailed&quot;, levels = -2) + b2factor(&quot;club_profile&quot;, levels = c(3, 1, 4)) + b2factor(&quot;club_feeder&quot;) + b1factor(&quot;race&quot;, levels = -c(1, 3, 4)) + b1nodematch(&quot;race&quot;, beta = .15)), estimate = &quot;CMLE&quot;, times = 0:1, control = control.tergm(CMLE.ergm = control.ergm(MCMC.burnin = 50000, MCMC.samplesize = 4000, parallel = 4, parallel.type = &quot;PSOCK&quot;, MCMLE.confidence.boost = 1.25))) summary(stergm_mod4) ## Call: ## tergm(formula = net_dynamic_9697 ~ Form(~edges + Offset(~edgecov(&quot;missing_dyad&quot;), ## coef = offset_coef_miss) + Offset(~nodemix(&quot;studentgender_clubgender&quot;, ## levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade97_clubgrade&quot;, ## levels2 = grade_terms_9697), coef = offset_coefs_grade_9697) + ## b2factor(&quot;club_type_detailed&quot;, levels = -2) + b2factor(&quot;club_profile&quot;, ## levels = c(3, 1, 4)) + b2factor(&quot;club_feeder&quot;) + b1factor(&quot;race&quot;, ## levels = -c(1, 3, 4)) + b1nodematch(&quot;race&quot;, beta = 0.15)) + ## Persist(~edges + Offset(~edgecov(&quot;missing_dyad&quot;), coef = offset_coef_miss) + ## Offset(~nodemix(&quot;studentgender_clubgender&quot;, levels2 = gender_terms), ## coef = offset_coefs_gender) + Offset(~nodemix(&quot;studentgrade97_clubgrade&quot;, ## levels2 = grade_terms_9697), coef = offset_coefs_grade_9697) + ## b2factor(&quot;club_type_detailed&quot;, levels = -2) + b2factor(&quot;club_profile&quot;, ## levels = c(3, 1, 4)) + b2factor(&quot;club_feeder&quot;) + b1factor(&quot;race&quot;, ## levels = -c(1, 3, 4)) + b1nodematch(&quot;race&quot;, beta = 0.15)), ## estimate = &quot;CMLE&quot;, control = control.tergm(CMLE.ergm = control.ergm(MCMC.burnin = 50000, ## MCMC.samplesize = 4000, parallel = 4, parallel.type = &quot;PSOCK&quot;, ## MCMLE.confidence.boost = 1.25)), times = 0:1) ## ## Monte Carlo Conditional Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## Form(1)~edges -6.43616 0.20573 0 -31.284 &lt; 1e-04 *** ## Form(1)~b2factor.club_type_detailed.Academic Competition -0.18054 0.11017 0 -1.639 0.101265 ## Form(1)~b2factor.club_type_detailed.Ethnic Interest -0.21284 0.22075 0 -0.964 0.334957 ## Form(1)~b2factor.club_type_detailed.Individual Sports 0.13026 0.12463 0 1.045 0.295954 ## Form(1)~b2factor.club_type_detailed.Leadership -0.54876 0.25495 0 -2.152 0.031365 * ## Form(1)~b2factor.club_type_detailed.Media -0.24800 0.16769 0 -1.479 0.139164 ## Form(1)~b2factor.club_type_detailed.Performance Art 0.03911 0.08510 0 0.460 0.645787 ## Form(1)~b2factor.club_type_detailed.Service 0.30507 0.07847 0 3.888 0.000101 *** ## Form(1)~b2factor.club_type_detailed.Team Sports 0.46728 0.11894 0 3.929 &lt; 1e-04 *** ## Form(1)~b2factor.club_profile.moderate 0.09424 0.07112 0 1.325 0.185153 ## Form(1)~b2factor.club_profile.high 0.03809 0.09997 0 0.381 0.703155 ## Form(1)~b2factor.club_profile.very_high 0.03686 0.14343 0 0.257 0.797205 ## Form(1)~b2factor.club_feeder.yes 0.29947 0.09891 0 3.028 0.002466 ** ## Form(1)~b1factor.race.black -1.20560 0.10228 0 -11.787 &lt; 1e-04 *** ## Form(1)~b1factor.race.white -1.62450 0.11137 0 -14.587 &lt; 1e-04 *** ## Form(1)~b1nodematch.race 4.67305 0.24453 0 19.110 &lt; 1e-04 *** ## Persist(1)~edges -0.65762 0.25458 0 -2.583 0.009791 ** ## Persist(1)~b2factor.club_type_detailed.Academic Competition 0.22187 0.22902 0 0.969 0.332656 ## Persist(1)~b2factor.club_type_detailed.Ethnic Interest 0.41329 0.44072 0 0.938 0.348372 ## Persist(1)~b2factor.club_type_detailed.Individual Sports 0.05751 0.22187 0 0.259 0.795489 ## Persist(1)~b2factor.club_type_detailed.Leadership -1.33674 0.46275 0 -2.889 0.003868 ** ## Persist(1)~b2factor.club_type_detailed.Media -1.40794 0.41452 0 -3.397 0.000682 *** ## Persist(1)~b2factor.club_type_detailed.Performance Art -0.57270 0.16953 0 -3.378 0.000730 *** ## Persist(1)~b2factor.club_type_detailed.Service 0.59693 0.15675 0 3.808 0.000140 *** ## Persist(1)~b2factor.club_type_detailed.Team Sports -0.28851 0.30476 0 -0.947 0.343795 ## Persist(1)~b2factor.club_profile.moderate 0.89011 0.16632 0 5.352 &lt; 1e-04 *** ## Persist(1)~b2factor.club_profile.high 1.47020 0.28784 0 5.108 &lt; 1e-04 *** ## Persist(1)~b2factor.club_profile.very_high 1.67350 0.35484 0 4.716 &lt; 1e-04 *** ## Persist(1)~b2factor.club_feeder.yes -1.20948 0.29844 0 -4.053 &lt; 1e-04 *** ## Persist(1)~b1factor.race.black -0.17794 0.21208 0 -0.839 0.401463 ## Persist(1)~b1factor.race.white 0.07521 0.21206 0 0.355 0.722823 ## Persist(1)~b1nodematch.race 0.38011 0.30579 0 1.243 0.213848 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 118331 on 85358 degrees of freedom ## Residual Deviance: 15545 on 85326 degrees of freedom ## ## AIC: 15609 BIC: 15908 (Smaller is better. MC Std. Err. = 2.539) We see that the results for tie formation (joining clubs) are different than for tie persistence (staying in clubs). Race only really matters for tie formation and adds very little to the persistence model. We see in the formation model that students join clubs based on race, joining clubs where they match the race of the current members (looking at b1nodematch.race). This likely happens because students are recruited into clubs through friendship and friendships are homophilous in terms of race. See the work of Miller McPherson, as well as Chapter 11 in this book (the main text), particularly the discussion of ecological niches and voluntary affiliations (McPherson 1983). On the other hand, there is a much weaker effect of racial homophily in the persistence model. Students may leave clubs because they are racially different than other members, but this tendency is much weaker than the tendency to avoid racially discordant clubs in the first place. Students thus tend to avoid clubs where they are racially different than most other students; but once that initial leap happens, they are not especially likely to leave. Overall, our longitudinal models offer a number of insights into the school. We find that club attributes operate fairly consistently across the formation and persistence models. Students join clubs that are of the same type and profile, and leave if they are part of clubs that are incongruent along these lines. More generally, we see how club features serve to constrain students. Students join certain types of clubs and leave when they 'mistakenly' join incongruent ones (joining a sports team and performance art). This suggests that there are clear patterns of club memberships that students are allowed to take on, with students joining and dropping clubs to be consistent with such rules. Homophily looks very different in the case of student attributes, with students joining, but not leaving, clubs based on the racial makeup. Demographic sorting is thus an impediment to integration, but once students overcome this initial sorting, the organization can potentially act to bring people together from different backgrounds. This is important as we might think that segregation along racial lines could be weakened (i.e., if a school took specific steps to create integrated clubs), while other kinds of segregation, based on patterns of club membership, status, etc., may be harder to alleviate. There are a number of other questions we could address with these kinds of data and models. For example, we could add information about friendships between students. We could then ask if students recruit their friends into clubs, and whether they are more likely to stay if they have friends in the club. This would allow us to test the hypothesis we posed about racial homophily and club membership (that homophily arises in clubs because there is homophily in friendship and students recruit their friends into clubs). We could also examine the clustering of clubs between students, asking if pairs of students tend to join the same clubs (gwb1nsp instead of gwb2nsp). And similarly, we could add terms to capture differential homophily. We could ask, for example, if students in team sports are especially likely to join other team sports; compared to say, academic interest, where the tendency to match (i.e., join other academic clubs) might be weaker. This tutorial has utilized ERGMs and STERGMs in the case of two-mode networks. In the next tutorial, we take up the problem of using streaming, continuous network data. We will explore streaming network data using one-mode networks. "],["ch13-Relational-Event-Models-R.html", "13, Part 4. Relational Event Models 13.21 Getting the Data Ready 13.22 Running Initial Models 13.23 Micro Rules of Interaction 13.24 Assessing Model Fit 13.25 Comparison to a Second Date", " 13, Part 4. Relational Event Models This is the fourth tutorial for Chapter 13 on statistical network models. The first tutorial covered the case of cross-sectional network data. The second tutorial covered statistical models for discrete, longitudinal networks. The third tutorial covered statistical models for two-mode networks. Here, we will walk through relational event models, appropriate for continuous-time network data. Relational event models are based on micro-interaction data. The model assumes that there is time-stamped (or at least ordered) information on the interactions between a set of actors. This shifts the focus from discrete relationships (friend, advice, etc.) to the specific interactions between actors in a setting. The goal of the model is to predict what the next event is likely to be, based on the interactional tendencies, or rules, of behavior in the setting. Compare this to STERGM, where the goal is to predict the adding/dropping of ties from one period to the next, based on discretely defined networks. Our data for this tutorial are based on streaming interaction data collected by Daniel McFarland on students in classrooms. Time-stamped interactions in each classroom were recorded, with information on the 'sender' and 'receiver' of the interaction, as well as the nature of the interaction. Interactions could be social or task-based, for example, although we focus just on social interactions in this tutorial. Data were collected across a large number of classrooms and days. See also Chapter 3, Part 2 (data processing for dynamic network data) and Chapter 5 (on visualization). Here we consider one classroom on two different days; both days are in the second semester of the year. We pick two days as a means of comparison. The first day was relatively uneventful and class was orderly. The second day was different, as there was a much higher rate of sanctioning behavior (i.e., disagreements between students and teacher on what was going on in the classroom). By examining two days, we see if the interactional signatures of order and disorder are different. 13.21 Getting the Data Ready Let’s begin by loading the main packages and getting the data ready to run the models. library(relevent) library(sna) relevent (Butts 2023a) contains the functions to run relational event models. Now, let's read in the interactional data for the first date. This is a data set reporting on the social interactions, i.e. talking, between individuals in the classroom (specifically classroom 182). This will serve as the outcome of interest, as we will predict what interactional features make certain events more likely to occur than others. url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class_interactions_date1.txt&quot; interactions_date1 &lt;- read.table(file = url1, header = T) Here we take a look at the first six rows of the data, for the main variables of interest: send_col (id of sender), receive_col (id of receiver), and time_estimate_col (time that interaction occurred). head(interactions_date1[, c(&quot;send_col&quot;, &quot;receive_col&quot;, &quot;time_estimate_col&quot;)]) ## send_col receive_col time_estimate_col ## 1 11 2 0.143 ## 2 2 11 0.286 ## 3 2 5 0.429 ## 4 5 2 0.571 ## 5 9 8 0.714 ## 6 8 9 0.857 Each row corresponds to an interaction between sender and receiver. For example, we can see that the third social interaction in this class period involved node 2 talking to node 5. We need to manipulate the data a bit to get it in a form that the relational event model can use. For example, the events must be sorted in sequential order before we run any models. In this case, this is already done, but if it were not we would have to sort our data appropriately. Additionally, each interaction must also happen at a unique time period. The models are based on sequences of interactions, so a distinct order of events must be possible to establish. This means that relational event models are not so appropriate in cases where there are a large number of simultaneous events. For the sake of simplicity, we will remove all interactions directed from the teacher to all students or from all students to the teacher. We can use the to_all_col and from_all_col to exclude these cases. to_all_col is equal to 1 if node i broadcasts to all other nodes simultaneously, while from_all_col is equal to 1 if node j receives from all nodes simultaneously. We will only keep those interactions where those variables are equal to 0 (i.e., i is not broadcasting to everyone in the class). not_to_all &lt;- interactions_date1$to_all_col == 0 not_from_all &lt;- interactions_date1$from_all_col == 0 interactions_date1 &lt;- interactions_date1[not_to_all &amp; not_from_all, ] Now, in order to run the model, we need to create an edgelist (as a matrix object), where the first column is the time of the event, the second column is the sender and the third column is the receiver. Again, the events must be ordered sequentially. edgelist_date1 &lt;- as.matrix(interactions_date1[, c(&quot;time_estimate_col&quot;, &quot;send_col&quot;, &quot;receive_col&quot;)]) As a final data manipulation, we need to add a row to the end of the edgelist, showing the stop time where no more interactions are possible. Let's look at the end of the data frame: tail(edgelist_date1) ## time_estimate_col send_col receive_col ## 274 42.528 11 17 ## 275 42.623 17 11 ## 276 42.717 3 11 ## 277 42.811 11 3 ## 278 42.906 7 8 ## 279 43.000 8 7 We can see that the last social interaction occurred at minute 43 in the class period. We will set the end of the interactional period at 43.10 (i.e., 6 seconds after the final interaction). To do this we add a row to the end of the edgelist, with the end time and then two NA values (for sender and receiver). edgelist_date1 &lt;- rbind(edgelist_date1, c(43.10, NA, NA)) Now, we will read in some attribute data, as we want to use information on gender, grade, etc. as predictors in the model. url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class_attributes.txt&quot; attributes &lt;- read.table(file = url2, header = T) head(attributes) ## id gnd grd rce ## 1 1 2 10 4 ## 2 2 2 10 3 ## 3 3 2 10 3 ## 4 4 2 10 3 ## 5 5 2 10 3 ## 6 6 1 10 4 There are four variables: id, gnd (gender: 1 = male; 2 = female); grd (grade: 10 = 10; 11 = 11; 16 = teacher); rce (race: 3 = Black; 4 = White). The relevent package requires that a researcher construct the node-level predictors as distinct columns (as opposed to using a factor() function within the formula). So, we will recode our variables to create the desired dummy variables of interest. In this case, we will create a predictor for whether the node is a teacher or not and for gender. We also need to create a term for the intercept. We will utilize the recode() function in the car package. library(car) We first will create a variable for the intercept, which is a simple 1 for all nodes in the class. attributes$intercept &lt;- 1 Now, we create a variable called male that is a 0 if gnd is equal to 2 (female) and 1 if gnd is equal to 1 (male). attributes$male &lt;- recode(attributes$gnd, as.factor = F, &quot;1 = 1; 2 = 0&quot;) And here we do the same thing for grd, creating a binary variable called teacher. teacher is equal to 1 if they are a teacher (grd = 16) and 0 otherwise. attributes$teacher &lt;- recode(attributes$grd, as.factor = F, &quot;16 = 1; NA = NA; else = 0&quot;) Finally, it will also be useful to have the size of the class handy. We can calculate that as the number of rows in the attribute data frame. class_size &lt;- nrow(attributes) 13.22 Running Initial Models Relational event modeling is based on the logic of hazard models (or event history models), where the model predicts the risk of an event occurring (i.e., the hazard) as a function of different kind of interactional terms. There are a number of terms that we can include, including terms for baseline node effects (e.g., girls interact more than boys). We can also include terms that capture more micro-dynamics. These are labeled p-shifts, or participation shifts, and are only based on the previous event in the sequence. For example, if A talks to B, then we might expect the very next event to be B talking to A. The model allows us to include these different kinds of terms as a means of seeing what rules govern the interactions in the case of interest. We are now in a position to run an initial relational event model. The function is rem.dyad(). The main arguments are: edgelist = input edgelist in the form of time of event, sender, receiver n = size of network effects = vector with names of effects to be included in the model covar = list of covariates that must correspond to the terms specified in effects ordinal = T/F; T if data are ordinal (ordered but without specific time stamps); F if data include time specific information for each event 13.22.1 Intercept Only Model Our first model will be very simple and just includes an intercept, capturing the baseline rate for events to occur. We will set the effects to CovSnd. CovSnd is a basic sender effect, in this case initiating social interactions with others. We use the covar option to include specific terms for CovSnd. In this case we include the intercept (so all nodes are assumed to initiate interactions at the same rate). We set ordinal to FALSE as the data has time stamped information. Let's also set a seed to make it easier to replicate. set.seed(1000) mod1 &lt;- rem.dyad(edgelist = edgelist_date1, n = class_size, effects = c(&quot;CovSnd&quot;), covar = list(CovSnd = attributes$intercept), ordinal = FALSE, hessian = TRUE) And now we look at the results: summary(mod1) ## Relational Event Model (Temporal Likelihood) ## ## Estimate Std.Err Z value Pr(&gt;|z|) ## CovSnd.1 -4.057353 0.066213 -61.277 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Null deviance: 2306.34 on 228 degrees of freedom ## Residual deviance: 2306.34 on 228 degrees of freedom ## Chi-square: -3.830135e-05 on 0 degrees of freedom, asymptotic p-value 1 ## AIC: 2308.34 AICC: 2308.357 BIC: 2311.769 The coefficient for the intercept (CovSnd.1) isn't all that interesting in itself, but it is important to understand what the coefficients mean (and how we can manipulate them) before moving to more complicated models. The first thing to note is that if we exponentiate this coefficient, we get the hazard of any event (i talking to j) occurring. Higher hazards mean the risk for an event occurring is higher. Second, if we multiple the hazard rate by the number of possible node pairs who could interact in a given moment, n * (n - 1), we should get the expected number of interactions occurring per minute in the classroom. Finally, if we take the inverse of that (1 / number of interactions per minute), we get the expected amount of time between events, or the wait time between events. 1 / (18 * 17 * exp(-4.057353)) ## [1] 0.1889577 The expected time between any event occurring is .189 standardized minutes (or .189 * 60 = 11.34 seconds). And let's check this against the real data. We will take the total number of minutes for that class and divide that by the total number of interactions that occurred. We will define the total number of minutes as 43.1, the end time set above. total_classtime &lt;- 43.1 We now define the total number of interactions. We will take the number of rows in the edgelist and subtract 1, as the last row is the stop time (not an interaction). num_interations &lt;- nrow(edgelist_date1) - 1 time_between_events &lt;- total_classtime / num_interations time_between_events ## [1] 0.1890351 We can see the estimate from the model approximates the raw data quite well. 13.22.2 Adding Sender and Receiver Effects We can do more substantively interesting things by incorporating the attributes of the nodes into the model. Let's first add a term for gender (coded as male = 1 and female = 0). We will add a sender effect, capturing whether males initiate fewer or greater interactions than females, as well as a receiver effect, capturing whether males receive fewer/greater interactions than females. Here we create two matrices, one for sending and one for receiving. Each matrix will include the covariates we want to include for the sending or receiving effects. We start with the sender covariate matrix, where we will include variables for the intercept (it still must be included) and male. CovSnd1 &lt;- cbind(attributes[, c(&quot;intercept&quot;, &quot;male&quot;)]) And now we do the same thing for the receiver covariate matrix, including a variable for male (note that no intercept term is included here). Note that even though we already had male as a sender effect we need to include it separately as part of the receiver covariate matrix if we want to specify it as a receiver effect. CovRec1 &lt;- cbind(attributes[, c(&quot;male&quot;)]) And now we are ready to estimate the model. The only difference from before is that we include \"CovRec\" in the effects and CovRec1 as part of the covariate (covar) list. mod2a &lt;- rem.dyad(edgelist_date1, n = class_size, effects = c(&quot;CovSnd&quot;, &quot;CovRec&quot;), covar = list(CovSnd = CovSnd1, CovRec = CovRec1), ordinal = FALSE, hessian = TRUE) summary(mod2a) ## Relational Event Model (Temporal Likelihood) ## ## Estimate Std.Err Z value Pr(&gt;|z|) ## CovSnd.1 -3.760406 0.084699 -44.3973 &lt; 2.2e-16 *** ## CovSnd.2 -0.601249 0.160226 -3.7525 0.0001751 *** ## CovRec.1 -0.460132 0.154017 -2.9875 0.0028123 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Null deviance: 2306.34 on 228 degrees of freedom ## Residual deviance: 2282.406 on 226 degrees of freedom ## Chi-square: 23.93399 on 2 degrees of freedom, asymptotic p-value 6.350391e-06 ## AIC: 2288.406 AICC: 2288.513 BIC: 2298.694 We can see that the names of the variables are difficult to interpret. So, let's create a vector of more useful names, and put that on the outputted object. coef_names2a &lt;- c(&quot;Intercept&quot;, &quot;Sender_male&quot;, &quot;Receiver_male&quot;) And now we put those names on the coef part of the rem object and redo the summary of the model. names(mod2a$coef) &lt;- coef_names2a summary(mod2a) ## Relational Event Model (Temporal Likelihood) ## ## Estimate Std.Err Z value Pr(&gt;|z|) ## Intercept -3.760406 0.084699 -44.3973 &lt; 2.2e-16 *** ## Sender_male -0.601249 0.160226 -3.7525 0.0001751 *** ## Receiver_male -0.460132 0.154017 -2.9875 0.0028123 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Null deviance: 2306.34 on 228 degrees of freedom ## Residual deviance: 2282.406 on 226 degrees of freedom ## Chi-square: 23.93399 on 2 degrees of freedom, asymptotic p-value 6.350391e-06 ## AIC: 2288.406 AICC: 2288.513 BIC: 2298.694 The summarized output now has more easily interpretable names for the variables. Let's go ahead and interpret the coefficient on sender_male. We can start by interpreting the sender_male coefficient in terms of hazards. By exponentiating the coefficient, we get the relative hazard for males to initiate the next interaction compared to females. exp(-0.601249) = 0.548. This means that an event with males initiating has a hazard that is 0.548 times lower than an event with females initiating. Hazards themselves are bit hard to interpret. As an alternative, we can calculate mean wait times, or the expected time between events. Let's first calculate the expected time between two male-male events (where there is a male sender and a male receiver). Note that this calculation must incorporate all of the coefficients (intercept, Sender_male and Receiver_male). The calculation is the same as we did above with the intercept only model, where the expected wait is equal to: 1 / (dyads_at_risk * hazard), where dyads_at_risk is the number of different ways that the event could occur, in this case the number of possible events that could involve two males. Let's do a quick table to see how many males are in the classroom. table(attributes$male) ## ## 0 1 ## 12 6 We can see that there are 12 females and 6 males. This means that there are 6 * 5 different ways that we could have a boy as a sender and a boy as a receiver. We will use that in the calculation below: dyads_at_risk &lt;- 6 * 5 Now we calculate the hazard. We take the coefficients, multiply them by the vector of input values (here setting Sender_male and Receiver_male to 1), sum it up and then exponentiate it. inputs &lt;- c(intercept = 1, Sender_male = 1, Receiver_male = 1) hazard_male_male &lt;- exp(sum(mod2a$coef * inputs)) hazard_male_male ## [1] 0.008052391 And now to calculate wait time: 1 / (dyads_at_risk * hazard_male_male) ## [1] 4.139557 This means that we would expect to wait 4.14 minutes between events that involve two boys. Now, let's do the same thing for girl-girl interactions. Here we set Sender_male to 0 and Receiver_male to 0. inputs &lt;- c(intercept = 1, Sender_male = 0, Receiver_male = 0) We define the dyads at risk to be 12 * 11 as there are 12 females in the class. dyads_at_risk &lt;- 12 * 11 Here we calculate the hazard. hazard_female_female &lt;- exp(sum(mod2a$coef * inputs)) hazard_female_female ## [1] 0.02327429 We can see that the hazard for female-female events is higher than male-male events. And now for the wait time. 1 / (dyads_at_risk * hazard_female_female) ## [1] 0.325499 We can see also that the wait time between female-female events is much lower. This is the case both because there are more females in the class and because males have a lower hazard of taking part in social interactions. Now, let's add our teacher variable to the model. This is accomplished by creating new CovSnd and CovRec matrices that include the teacher variable. CovSnd2 &lt;- cbind(attributes[, c(&quot;intercept&quot;, &quot;male&quot;, &quot;teacher&quot;)]) CovRec2 &lt;- cbind(attributes[, c(&quot;male&quot;, &quot;teacher&quot;)]) And now we rerun our model with the updated CovSnd and CovRec matrices. mod2b &lt;- rem.dyad(edgelist_date1, n = class_size, effects = c(&quot;CovSnd&quot;, &quot;CovRec&quot;), covar = list(CovSnd = CovSnd2, CovRec = CovRec2), ordinal = FALSE, hessian = TRUE) Again, we can add better labels to the variable names and summarize the results. coef_names2b &lt;- c(&quot;Intercept&quot;, &quot;Sender_male&quot;, &quot;Sender_teacher&quot;, &quot;Receiver_male&quot;, &quot;Receiver_teacher&quot;) names(mod2b$coef) &lt;- coef_names2b summary(mod2b) ## Relational Event Model (Temporal Likelihood) ## ## Estimate Std.Err Z value Pr(&gt;|z|) ## Intercept -3.759707 0.084668 -44.4054 &lt; 2.2e-16 *** ## Sender_male -0.483081 0.164160 -2.9427 0.003253 ** ## Sender_teacher -1.132818 0.596658 -1.8986 0.057617 . ## Receiver_male -0.514723 0.169125 -3.0434 0.002339 ** ## Receiver_teacher 0.280787 0.325987 0.8613 0.389048 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Null deviance: 2306.34 on 228 degrees of freedom ## Residual deviance: 2276.504 on 224 degrees of freedom ## Chi-square: 29.83545 on 4 degrees of freedom, asymptotic p-value 5.286827e-06 ## AIC: 2286.504 AICC: 2286.775 BIC: 2303.651 It looks like the teacher variables do not add much to the model. Let's compare the fit using BIC across the two models. mod2a$BIC - mod2b$BIC ## [1] -4.957228 Given that we want lower values, the simple model (mod2a, which just includes gender) would appear to be the better option. 13.22.3 Adding Covariate Event Terms As a third kind of term, we will consider adding covariate event terms to the model. Covariate events are predictors that are based on attributes of a dyad. Here, we will add the seating structure of the class to the model. The basic idea is that nodes who are close in the classroom are more likely to talk to one another. Let's first read in the data: url3 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class_seating_date1.txt&quot; seating_date1 &lt;- read.table(file = url3, header = T) head(seating_date1) ## ego_id alter_id ## 1 1 7 ## 2 1 9 ## 3 1 15 ## 4 2 3 ## 5 2 5 ## 6 2 11 As in the previous tutorial, the data is stored as an edgelist, indicating if node i is sitting adjacent to node j. The rem.dyad() function requires that this information be transformed into a matrix. So, we will go ahead and create a matrix of seating, where there is a 1 if i and j are next to each other in the class and 0 otherwise. We will accomplish this by taking the seating edgelist, turning it into a directed network object, and then symmetrizing it to make it undirected (this simply fixes any mistakes in the data where i may be recorded as being next to j but j is not recorded as being next to i). We will use a 'weak' rule when symmetrizing the matrix, so if i is recorded as sitting next to j or j is recorded as sitting next to i, the matrix will have a 1 for both ij and ji. Note that the symmetrize() function will output a matrix by default. seating_network_date1 &lt;- network(x = seating_date1, directed = T, vertices = data.frame(ids = 1:class_size)) seating_matrix_date1 &lt;- symmetrize(seating_network_date1, rule = &quot;weak&quot;) seating_matrix_date1 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] ## [1,] 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 ## [2,] 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 ## [3,] 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 ## [4,] 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 ## [5,] 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 ## [6,] 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 ## [7,] 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 ## [8,] 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 ## [9,] 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 ## [10,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [11,] 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 ## [12,] 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 ## [13,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [14,] 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 ## [15,] 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 ## [16,] 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ## [17,] 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 ## [18,] 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 Now, we can run our model. We must include \"CovEvent\" in the effects input. We must also add the seating matrix to the list of covariates (covar), set with CovEvent. We will use the CovSnd and CovRec matrices that only include gender (so no teacher variable). mod3a &lt;- rem.dyad(edgelist_date1, n = class_size, effects = c(&quot;CovSnd&quot;, &quot;CovRec&quot;, &quot;CovEvent&quot;), covar = list(CovSnd = CovSnd1, CovRec = CovRec1, CovEvent = seating_matrix_date1), ordinal = FALSE, hessian = TRUE) Let's compare the fit between our previously preferred model and our new model. mod2a$BIC - mod3a$BIC ## [1] 416.5262 It looks like the seating arrangement does strongly shape what events occur in the classroom, as the fit is dramatically improved. Now, let’s go ahead and add a second covariate event matrix to the model. Here, we add information about the friendships that exist in the classroom. Friendship information was collected for each semester. Students were asked who they hung around with in the class. We will treat this information like a covariate event, with the idea that interactions during the class period are more likely to involve friends than non-friends. We will first read in the data for friendship during the second semester (when the class of interest took place). url4 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class_edgelist_sem2.txt&quot; friends_sem2 &lt;- read.table(file = url4 , header = T) head(friends_sem2) ## sender receiver ## 1 1 4 ## 2 1 5 ## 3 1 7 ## 4 1 9 ## 5 1 15 ## 6 2 3 The edgelist captures if student i nominated student j as a friend. Note that the ids must match that found on the other data (interaction data, attributes, etc.). As before, we need to need to turn our edgelist into a matrix of 0s and 1s. friends_sem2_network &lt;- network(x = friends_sem2, directed = T, vertices = data.frame(ids = 1:class_size)) friends_matrix_sem2 &lt;- as.matrix(friends_sem2_network) friends_matrix_sem2 ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 ## 2 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 ## 3 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 ## 4 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 ## 5 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 ## 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 7 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 ## 8 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 ## 9 1 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 ## 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 11 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 ## 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 13 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 14 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 ## 15 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 ## 16 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## 17 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 ## 18 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 While it is relatively simple to include a single covariate event matrix in the model (see seating example above), it is a bit tricky to include multiple event matrices. The rem.dyad() function requires that multiple matrices first be put together as an p X n X n array, where p is the number of matrices and n is the size of the network. We will go ahead and create that array. We will first create an array of NAs with the right structure (2 X 18 X 18). CovEvent_date1 &lt;- array(data = NA, dim = c(2, class_size, class_size)) We will now put the first matrix, the seating matrix, in the first slot. CovEvent_date1[1, , ] &lt;- seating_matrix_date1 We will now put the second matrix, the friendship matrix, in the second slot. CovEvent_date1[2, , ] &lt;- friends_matrix_sem2 Checking the dimensions: dim(CovEvent_date1) ## [1] 2 18 18 Note, that we would get unexpected (i.e., wrong) results if we had created the array to be 18 X 18 X 2. We are now in a position to run the model, putting in the newly created CovEvent array as input into the covar list. mod3b &lt;- rem.dyad(edgelist_date1, n = class_size, effects = c(&quot;CovSnd&quot;, &quot;CovRec&quot;, &quot;CovEvent&quot;), covar = list(CovSnd = CovSnd1, CovRec = CovRec1, CovEvent = CovEvent_date1), ordinal = FALSE, hessian = TRUE) And let's add some useful names to the output (adding seating and friendship to the original vector of names) and summarize the results. coef_names3b &lt;- c(coef_names2a, &quot;Seating&quot;, &quot;Friendship&quot;) names(mod3b$coef) &lt;- coef_names3b summary(mod3b) ## Relational Event Model (Temporal Likelihood) ## ## Estimate Std.Err Z value Pr(&gt;|z|) ## Intercept -5.49675 0.17648 -31.1470 &lt; 2.2e-16 *** ## Sender_male -0.47785 0.16200 -2.9498 0.003180 ** ## Receiver_male -0.43877 0.15428 -2.8440 0.004455 ** ## Seating 1.82666 0.23713 7.7031 1.332e-14 *** ## Friendship 1.52491 0.23856 6.3921 1.636e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Null deviance: 2306.34 on 228 degrees of freedom ## Residual deviance: 1812.664 on 224 degrees of freedom ## Chi-square: 493.6753 on 4 degrees of freedom, asymptotic p-value 0 ## AIC: 1822.664 AICC: 1822.935 BIC: 1839.811 We can see that both friendship and adjacent seating predict the occurrence of a social interaction event between i and j. Thus, an interaction event (i talks to j) is much more likely to occur if i sits next to j and/or i is friends with j. The gender differences in sending and receiving still seem to be present. 13.23 Micro Rules of Interaction So far we have built up a simple but plausible model of interactions in this classroom. Girls tend to talk more than boys, while friends and those sitting close to each other also tend to interact during class. What we have yet to capture is something about the 'rules' of interaction. For example, we might expect turn taking (i talks to j and then j talks to i) above what we can capture from friendship and seating effects alone. We will build up our model slowly, including more complicated rules as we go along. As a start, let's include terms that capture recency of events. The two terms of interest are \"RRecSnd\" and \"RSndSnd\". With RRecSnd, we test if i is more likely to talk to j if j recently talked to i. With RSndSnd, we test if i is more likely to talk to j if i recently talked to j. The effects capture the idea that if j recently talked to i (or i recently talked to j), then an i-&gt;j event is more likely to be the next event. We will specify this model by including the \"RRecSnd\" and \"RSndSnd\" in the vector of effects. Note that we do not need to add anything to the covar list. For this first model we will not control for friendship or seating. mod4a &lt;- rem.dyad(edgelist_date1, n = class_size, effects = c(&quot;CovSnd&quot;, &quot;CovRec&quot;, &quot;RRecSnd&quot;, &quot;RSndSnd&quot;), covar = list(CovSnd = CovSnd1, CovRec = CovRec1), ordinal = FALSE, hessian = TRUE) Let's check the order of the coefficients using: names(mod4a$coef) ## [1] &quot;RRecSnd&quot; &quot;RSndSnd&quot; &quot;CovSnd.1&quot; &quot;CovSnd.2&quot; &quot;CovRec.1&quot; In this case, the recency effects are the first variables in the summary output. Let's set the names with this order in mind (adding the recency effects to the original vector of names). coef_names4a &lt;- c(&quot;Recency_ji&quot;, &quot;Recency_ij&quot;, coef_names2a) names(mod4a$coef) &lt;- coef_names4a summary(mod4a) ## Relational Event Model (Temporal Likelihood) ## ## Estimate Std.Err Z value Pr(&gt;|z|) ## Recency_ji 6.31792 0.22376 28.2355 &lt; 2.2e-16 *** ## Recency_ij -2.44296 0.21764 -11.2250 &lt; 2.2e-16 *** ## Intercept -5.14311 0.14179 -36.2717 &lt; 2.2e-16 *** ## Sender_male -0.82672 0.17750 -4.6575 3.2e-06 *** ## Receiver_male -0.47810 0.16423 -2.9112 0.003601 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Null deviance: 2306.34 on 228 degrees of freedom ## Residual deviance: 1419.167 on 224 degrees of freedom ## Chi-square: 887.1722 on 4 degrees of freedom, asymptotic p-value 0 ## AIC: 1429.167 AICC: 1429.438 BIC: 1446.314 We see a positive coefficient on the recency receiver effects, suggesting that if j recently talked to i then i is likely to talk to j. On the other hand, there is a negative coefficient for the recency sender effects. This means that if i recently talked to j the next event is actually less likely to be i to j again. Note that the gender effects remain. Now we run the full model with seating and friendship included. mod4b &lt;- rem.dyad(edgelist_date1, n = class_size, effects = c(&quot;CovSnd&quot;, &quot;CovRec&quot;, &quot;CovEvent&quot;, &quot;RRecSnd&quot;, &quot;RSndSnd&quot;), covar = list(CovSnd = CovSnd1, CovRec = CovRec1, CovEvent = CovEvent_date1), ordinal = FALSE, hessian = TRUE) coef_names4b &lt;- c(&quot;Recency_ji&quot;, &quot;Recency_ij&quot;, coef_names3b) names(mod4b$coef) &lt;- coef_names4b summary(mod4b) ## Relational Event Model (Temporal Likelihood) ## ## Estimate Std.Err Z value Pr(&gt;|z|) ## Recency_ji 5.726043 0.223170 25.6577 &lt; 2.2e-16 *** ## Recency_ij -3.051556 0.224246 -13.6080 &lt; 2.2e-16 *** ## Intercept -6.070958 0.218092 -27.8367 &lt; 2.2e-16 *** ## Sender_male -0.201135 0.175123 -1.1485 0.2507488 ## Receiver_male -0.089231 0.164541 -0.5423 0.5876111 ## Seating 1.060937 0.278799 3.8054 0.0001416 *** ## Friendship 0.919033 0.272150 3.3769 0.0007330 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Null deviance: 2306.34 on 228 degrees of freedom ## Residual deviance: 1348.516 on 222 degrees of freedom ## Chi-square: 957.8238 on 6 degrees of freedom, asymptotic p-value 0 ## AIC: 1362.516 AICC: 1363.025 BIC: 1386.521 The first thing to note is that the model fit is much improved from our previously preferred model. mod3b$BIC - mod4b$BIC ## [1] 453.2898 The second thing to note is that the gender coefficients are no longer significant. This suggests that controlling for both recency of events and seating explains the gender differences in social interaction. Girls are more likely to sit next to each other; couple this with the interactional tendencies to respond to a recent interaction, and we see why girls are more likely to be involved in events. Now, let's see if we can consider other rules that may be shaping how nodes in this classroom interact with each other. We will now consider terms that capture p-shifts, or participation shifts. p-shifts are based strictly on the most recent event that occurred, rather than recency effects which can go back further in time. The idea is to capture micro rules in how interactions play out, based on the last interaction that took place. For our first example, we will add a turn taking rule, where A talks to B and the very next event is B talking to A. This is specified as \"PSAB-BA\" (as part of the effects vector). The rest of the model is the same as the previous model. mod4c &lt;- rem.dyad(edgelist_date1, n = class_size, effects = c(&quot;CovSnd&quot;, &quot;CovRec&quot;, &quot;CovEvent&quot;, &quot;RRecSnd&quot;, &quot;RSndSnd&quot;, &quot;PSAB-BA&quot;), covar = list(CovSnd = CovSnd1, CovRec = CovRec1, CovEvent = CovEvent_date1), ordinal = FALSE, hessian = TRUE) coef_names4c &lt;- c(coef_names4b, &quot;PSAB_BA&quot;) names(mod4c$coef) &lt;- coef_names4c summary(mod4c) ## Relational Event Model (Temporal Likelihood) ## ## Estimate Std.Err Z value Pr(&gt;|z|) ## Recency_ji 3.140606 0.300287 10.4587 &lt; 2.2e-16 *** ## Recency_ij -1.447230 0.209216 -6.9174 4.6e-12 *** ## Intercept -6.023163 0.207242 -29.0635 &lt; 2.2e-16 *** ## Sender_male 0.003476 0.168344 0.0206 0.983526 ## Receiver_male 0.135468 0.162346 0.8344 0.404031 ## Seating 0.960246 0.252733 3.7995 0.000145 *** ## Friendship 0.892416 0.249509 3.5767 0.000348 *** ## PSAB_BA 3.334932 0.187187 17.8161 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Null deviance: 2306.34 on 228 degrees of freedom ## Residual deviance: 1006.862 on 221 degrees of freedom ## Chi-square: 1299.478 on 7 degrees of freedom, asymptotic p-value 0 ## AIC: 1022.862 AICC: 1023.519 BIC: 1050.296 We can see that the fit is improved greatly, as there is a large effect of turn taking in social interactions. We can also see that the effects for recency are much reduced from the previous model. Now, let's add a somewhat more complicated interactional rule. Here we will add a term for 'turn continuing', \"PSAB-AY\". This means that A talks with B and the very next event is A talking to someone else (besides B). We will also add a term for 'turn receiving', \"PSAB-BY\". Here, A talks to B and the very next event is B talking to someone else (besides A). mod4d &lt;- rem.dyad(edgelist_date1, n = class_size, effects = c(&quot;CovSnd&quot;, &quot;CovRec&quot;, &quot;CovEvent&quot;, &quot;RRecSnd&quot;, &quot;RSndSnd&quot;, &quot;PSAB-BA&quot;, &quot;PSAB-BY&quot;, &quot;PSAB-AY&quot;), covar = list(CovSnd = CovSnd1, CovRec = CovRec1, CovEvent = CovEvent_date1), ordinal = FALSE, hessian = TRUE) coef_names4d &lt;- c(coef_names4b, &quot;PSAB_BA&quot;, &quot;PSAB_BY&quot;, &quot;PSAB_AY&quot;) names(mod4d$coef) &lt;- coef_names4d summary(mod4d) ## Relational Event Model (Temporal Likelihood) ## ## Estimate Std.Err Z value Pr(&gt;|z|) ## Recency_ji 3.277781 0.306184 10.7053 &lt; 2.2e-16 *** ## Recency_ij -1.445992 0.209454 -6.9036 5.069e-12 *** ## Intercept -6.166525 0.218196 -28.2614 &lt; 2.2e-16 *** ## Sender_male 0.026820 0.168885 0.1588 0.8738219 ## Receiver_male 0.148590 0.162915 0.9121 0.3617316 ## Seating 0.960858 0.253539 3.7898 0.0001508 *** ## Friendship 0.858706 0.249644 3.4397 0.0005823 *** ## PSAB_BA 3.364859 0.190066 17.7036 &lt; 2.2e-16 *** ## PSAB_BY 1.180523 0.305242 3.8675 0.0001100 *** ## PSAB_AY 0.098728 0.423907 0.2329 0.8158395 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Null deviance: 2306.34 on 228 degrees of freedom ## Residual deviance: 995.3731 on 219 degrees of freedom ## Chi-square: 1310.967 on 9 degrees of freedom, asymptotic p-value 0 ## AIC: 1015.373 AICC: 1016.387 BIC: 1049.667 Looking at our two added terms, we see that only the PSAB-BY coefficient is significant (at traditional levels). This suggest that there are norms about whose 'turn it is' to talk. Once A talks to B, it is now B's turn to talk. They are very likely to talk back to A (PSAB-BA) but may also talk to another node (PSAB-BY). There is little evidence for nodes talking twice in a row to different people. It looks like the model fit is improved very slightly from the previous model: mod4c$BIC - mod4d$BIC ## [1] 0.629751 Let's look at one more set of terms, here focusing on interactional tendencies related to usurping the conversation. We include p-shift terms for \"PSAB-XA\" and \"PSAB-XB\". With PSAB-XA, A talks to B and then another node (X) usurps the conversation and answers A. With PSAB-XB, A talks to B and then another node (X) usurps the conversation and talks to B. In both cases, a node talks 'out of turn' relative to the previous event. Let's go ahead and add these terms to the model. mod4e &lt;- rem.dyad(edgelist_date1, n = class_size, effects = c(&quot;CovSnd&quot;, &quot;CovRec&quot;, &quot;CovEvent&quot;, &quot;RRecSnd&quot;, &quot;RSndSnd&quot;, &quot;PSAB-BA&quot;, &quot;PSAB-BY&quot;, &quot;PSAB-AY&quot;, &quot;PSAB-XA&quot;, &quot;PSAB-XB&quot;), covar = list(CovSnd = CovSnd1, CovRec = CovRec1, CovEvent = CovEvent_date1), ordinal = FALSE, hessian = TRUE) Let's check the order of the coefficients: names(mod4e$coef) ## [1] &quot;RRecSnd&quot; &quot;RSndSnd&quot; &quot;CovSnd.1&quot; &quot;CovSnd.2&quot; &quot;CovRec.1&quot; &quot;CovEvent.1&quot; &quot;CovEvent.2&quot; &quot;PSAB-BA&quot; &quot;PSAB-BY&quot; &quot;PSAB-XA&quot; &quot;PSAB-XB&quot; &quot;PSAB-AY&quot; And now we set the names to be consistent with the output. coef_names4e &lt;- c(coef_names4b, &quot;PSAB_BA&quot;, &quot;PSAB_BY&quot;, &quot;PSAB_XA&quot;, &quot;PSAB_XB&quot;, &quot;PSAB_AY&quot;) names(mod4e$coef) &lt;- coef_names4e summary(mod4e) ## Relational Event Model (Temporal Likelihood) ## ## Estimate Std.Err Z value Pr(&gt;|z|) ## Recency_ji 3.255338 0.305936 10.6406 &lt; 2.2e-16 *** ## Recency_ij -1.435095 0.208876 -6.8706 6.394e-12 *** ## Intercept -6.345151 0.229722 -27.6210 &lt; 2.2e-16 *** ## Sender_male 0.052905 0.169351 0.3124 0.7547377 ## Receiver_male 0.189967 0.163464 1.1621 0.2451804 ## Seating 0.965638 0.252719 3.8210 0.0001329 *** ## Friendship 0.869857 0.248674 3.4980 0.0004688 *** ## PSAB_BA 3.532669 0.200569 17.6133 &lt; 2.2e-16 *** ## PSAB_BY 1.332496 0.311164 4.2823 1.850e-05 *** ## PSAB_XA 0.389227 0.397964 0.9780 0.3280511 ## PSAB_XB 1.179550 0.286743 4.1136 3.895e-05 *** ## PSAB_AY 0.256821 0.428279 0.5997 0.5487334 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Null deviance: 2306.34 on 228 degrees of freedom ## Residual deviance: 981.8918 on 217 degrees of freedom ## Chi-square: 1324.448 on 11 degrees of freedom, asymptotic p-value 0 ## AIC: 1005.892 AICC: 1007.343 BIC: 1047.044 We can see that the PSAB-XB effect is much stronger than the PSAB-XA effect. This suggests that if one usurps the conversation from A to B, then one must interact with B in the next interaction, in essence giving B their rightful turn coming up next. And again, it looks like the model fit is improved a bit from the previous model. mod4d$BIC - mod4e$BIC ## [1] 2.622677 So, the overall story is one where there are clear micro rules to interacting in a classroom (above the effects for friendship, seating and so on). The basic rules could be summarized as: if A talks to B then the next event should be B to A; B to someone else; or someone else to B. Thus, there are clear norms about turn taking. If A talks to B, B is very likely to be part of the next event (one way or another). Of course, we could imagine looking at other kinds of terms, but this is pretty good start to the model. 13.24 Assessing Model Fit In interpreting our results, it is useful to see if our models are fitting well. While BIC can offer some evidence if one model is preferred to another, we can look at the residuals and the predicted classification to see how well the model is actually predicting the data. Here we will work with mod4e, our preferred model from above. One useful part of the output is predicted.match. head(mod4e$predicted.match) ## send_col receive_col ## 1 FALSE FALSE ## 2 TRUE TRUE ## 3 FALSE FALSE ## 4 TRUE TRUE ## 5 FALSE FALSE ## 6 TRUE TRUE Each row corresponds to an observed event. The first column shows if the model predicted the sender of that event correctly and the second column shows if the model predicted the receiver of that event correctly. Note that the model is trying to predict the specific sequence of events (i.e., the exact order of sender-receiver events). Let's see how we did by doing a table of the send and receive columns. send_col &lt;- mod4e$predicted.match[, &quot;send_col&quot;] receive_col &lt;- mod4e$predicted.match[, &quot;receive_col&quot;] table(send_col, receive_col) ## receive_col ## send_col FALSE TRUE ## FALSE 89 6 ## TRUE 14 119 We can see that 119 times we predicted the exact sender and the exact receiver correct (in sequence), while 89 times we got neither the sender nor the receiver correct. And now let's transform the table into proportions, showing the proportion where we get the exact sequence right (wrong, etc.). prop.table(table(send_col, receive_col)) ## receive_col ## send_col FALSE TRUE ## FALSE 0.39035088 0.02631579 ## TRUE 0.06140351 0.52192982 We can see that about 52% of the time we get the exact event correct, while 39% of the time we miss completely and do not get the sender or receiver. The model is thus doing an okay job of prediction but is clearly missing some element that is important for predicting interaction events. Let's see if we can identify the cases (i.e., events) where the model is not doing such a good job at prediction. We first summarize the residuals for the model. summary(mod4e$residuals) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -2.2790 -0.8395 1.2354 4.3030 8.7932 25.5613 Now, we will identify some of the outlying cases, those with high residuals. We will define that for convenience as cases with residuals greater than 10. high_resids &lt;- which(mod4e$residuals &gt; 10) Here we take a look at the events with high residuals, reducing the edgelist to just those cases where the model is not fitting well. edgelist_date1[high_resids, ] ## time_estimate_col send_col receive_col ## 5 0.714 9 8 ## 9 1.429 3 11 ## 21 3.286 8 7 ## 25 4.286 5 2 ## 37 6.321 3 2 ## 45 7.393 18 4 ## 55 8.893 11 3 ## 57 10.041 14 18 ## 59 10.419 5 2 ## 71 12.595 5 17 ## 79 13.446 9 8 ## 101 16.391 3 12 ## 104 16.913 11 12 ## 106 17.826 3 12 ## 107 18.609 2 17 ## 111 19.789 15 8 ## 113 21.105 11 12 ## 119 22.289 11 2 ## 121 22.947 4 14 ## 127 24.500 8 12 ## 130 25.125 8 12 ## 154 26.970 11 12 ## 155 27.091 2 12 ## 156 27.333 11 12 ## 157 27.455 2 12 ## 160 28.909 11 12 ## 165 29.758 8 12 ## 184 33.152 11 17 ## 188 34.640 7 1 ## 200 37.520 11 17 ## 204 38.566 5 2 ## 210 39.132 8 15 ## 212 39.321 1 8 ## 216 39.698 8 15 ## 235 40.264 7 8 ## 260 41.208 8 15 ## 278 42.906 7 8 Node 12 seems to show up quite a bit in the receiver column of these events. Let's take a look at the attributes for node 12. attributes[12, ] ## id gnd grd rce intercept male teacher ## 12 12 1 16 3 1 1 1 Node 12 is the teacher in the class. So, perhaps we were too hasty in removing the teacher variable, as it looks like we are missing the set of interactions where students talk socially to the teacher. Let’s rerun our model but use CovSnd2 and CovRec2, which includes both the gender and teacher effects. mod4f &lt;- rem.dyad(edgelist_date1, n = class_size, effects = c(&quot;CovSnd&quot;, &quot;CovRec&quot;, &quot;CovEvent&quot;, &quot;RRecSnd&quot;, &quot;RSndSnd&quot;, &quot;PSAB-BA&quot;, &quot;PSAB-BY&quot;,&quot;PSAB-AY&quot;, &quot;PSAB-XA&quot;, &quot;PSAB-XB&quot;), covar = list(CovSnd = CovSnd2, CovRec = CovRec2, CovEvent = CovEvent_date1), ordinal = FALSE, hessian = TRUE) Let's check the fit compared to the previous model (with no teacher sender/receiver effects). mod4e$BIC - mod4f$BIC ## [1] 8.172225 It looks like adding the teacher terms did help the fit a bit. And now let's look at the results. coef_names4f &lt;- c(&quot;Recency_ji&quot;, &quot;Recency_ij&quot;, &quot;Intercept&quot;, &quot;Sender_male&quot;, &quot;Sender_teacher&quot;, &quot;Receiver_male&quot;, &quot;Receiver_teacher&quot;, &quot;Seating&quot;, &quot;Friendship&quot;, &quot;PSAB_BA&quot;, &quot;PSAB_BY&quot;, &quot;PSAB_XA&quot;, &quot;PSAB_XB&quot;, &quot;PSAB_AY&quot;) names(mod4f$coef) &lt;- coef_names4f summary(mod4f) ## Relational Event Model (Temporal Likelihood) ## ## Estimate Std.Err Z value Pr(&gt;|z|) ## Recency_ji 3.505637 0.324673 10.7975 &lt; 2.2e-16 *** ## Recency_ij -1.576398 0.210819 -7.4775 7.572e-14 *** ## Intercept -6.447691 0.242990 -26.5348 &lt; 2.2e-16 *** ## Sender_male 0.214817 0.171486 1.2527 0.2103209 ## Sender_teacher -1.149423 0.635645 -1.8083 0.0705631 . ## Receiver_male 0.015602 0.179619 0.0869 0.9307795 ## Receiver_teacher 1.431023 0.381847 3.7476 0.0001785 *** ## Seating 0.857973 0.256406 3.3461 0.0008194 *** ## Friendship 0.992714 0.292978 3.3884 0.0007031 *** ## PSAB_BA 3.478400 0.202099 17.2113 &lt; 2.2e-16 *** ## PSAB_BY 1.362145 0.311813 4.3685 1.251e-05 *** ## PSAB_XA 0.400141 0.398097 1.0051 0.3148317 ## PSAB_XB 1.151296 0.286724 4.0154 5.936e-05 *** ## PSAB_AY 0.231155 0.428187 0.5398 0.5893027 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Null deviance: 2306.34 on 228 degrees of freedom ## Residual deviance: 962.8608 on 215 degrees of freedom ## Chi-square: 1343.479 on 13 degrees of freedom, asymptotic p-value 0 ## AIC: 990.8608 AICC: 992.8327 BIC: 1038.872 We can see that the teacher is part of more interactions as the receiver (being talked to) than we would expect based on other terms in the model. This is likely the case because the teacher can easily talk to anyone in the class (i.e., the teacher is not subject to only talking to those adjacent to them in the classroom), and so the term on the seating arrangement pushes the previous model to under predict interactions with the teacher. This is now rectified in the current model. 13.25 Comparison to a Second Date We have so far run a number of models, interpreted the results and learned a bit about the interactional dynamics in this classroom. Here, we run through the same exercise (an abbreviated version) using interactional data from a different date. The classroom is the same, so the actors are the same, but this class takes place later in the second semester. More importantly, this was a date where there was a great deal more misbehaving in the class and the teacher had to sanction students to a much larger extent. Our question is how (or if) the interactional tendencies are different in a day where the class is less orderly and controlled. We begin by reading in the interactional data for this second date. url5 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class_interactions_date2.txt&quot; interactions_date2 &lt;- read.table(file = url5, header = T) And again, we will take out those interactions where one node talks to the entire class simultaneously. not_to_all2 &lt;- interactions_date2$to_all_col == 0 not_from_all2 &lt;- interactions_date2$from_all_col == 0 interactions_date2 &lt;- interactions_date2[not_to_all2 &amp; not_from_all2, ] And now we create the edgelist matrix, adding a row for the stop time for the interactions (again, .10 standardized minutes after the last recorded interaction). var_names &lt;- c(&quot;time_estimate_col&quot;, &quot;send_col&quot;, &quot;receive_col&quot;) edgelist_date2 &lt;- as.matrix(interactions_date2[, var_names]) tail(edgelist_date2) ## time_estimate_col send_col receive_col ## 398 32.242 17 11 ## 399 32.394 11 17 ## 400 32.545 17 5 ## 401 32.697 5 17 ## 402 32.848 8 6 ## 403 33.000 6 8 edgelist_date2 &lt;- rbind(edgelist_date2, c(33.10, NA, NA)) The friendship data is the same as above (corresponding to the second semester) but we need to read in the seating data for this day. url6 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class_seating_date2.txt&quot; seating_date2 &lt;- read.table(file = url6, header = T) head(seating_date2) ## ego_id alter_id ## 1 1 7 ## 2 3 5 ## 3 3 11 ## 4 3 17 ## 5 4 14 ## 6 4 18 Here we turn the edgelist into a matrix, as we did before. seating_network_date2 &lt;- network(x = seating_date2, directed = T, vertices = data.frame(ids = 1:class_size)) seating_matrix_date2 &lt;- symmetrize(seating_network_date2, rule = &quot;weak&quot;) And once again, we need to create a covEvent array with the new seating matrix. CovEvent_date2 &lt;- array(data = NA, dim = c(2, class_size, class_size)) We will now put the first matrix, the seating matrix, in the first slot. CovEvent_date2[1, , ] &lt;- seating_matrix_date2 We will now put the second matrix, the friendship matrix, in the second slot. CovEvent_date2[2, , ] &lt;- friends_matrix_sem2 We are now in a position to run the same model as we did above. We will just run the preferred model (mod4f), with all terms included. Note that the CovRec and CovSnd matrices are the same as above (as the attributes are the same across time in this case). mod4f_date2 &lt;- rem.dyad(edgelist_date2, n = class_size, effects = c(&quot;CovSnd&quot;, &quot;CovRec&quot;, &quot;CovEvent&quot;, &quot;RRecSnd&quot;, &quot;RSndSnd&quot;, &quot;PSAB-BA&quot;, &quot;PSAB-BY&quot;,&quot;PSAB-AY&quot;, &quot;PSAB-XA&quot;, &quot;PSAB-XB&quot;), covar = list(CovSnd = CovSnd2, CovRec = CovRec2, CovEvent = CovEvent_date2), ordinal = FALSE, hessian = TRUE) Let's add some more meaningful variable names. In, this case the terms are the same as with mod4f, so we can use those names directly. names(mod4f_date2$coef) &lt;- coef_names4f summary(mod4f_date2) ## Relational Event Model (Temporal Likelihood) ## ## Estimate Std.Err Z value Pr(&gt;|z|) ## Recency_ji 2.750119 0.308910 8.9027 &lt; 2.2e-16 *** ## Recency_ij -0.669371 0.211804 -3.1603 0.001576 ** ## Intercept -7.099843 0.302566 -23.4655 &lt; 2.2e-16 *** ## Sender_male 0.151122 0.139798 1.0810 0.279695 ## Sender_teacher 0.634194 0.445500 1.4236 0.154575 ## Receiver_male 0.028309 0.143930 0.1967 0.844076 ## Receiver_teacher 2.368058 0.337425 7.0180 2.250e-12 *** ## Seating 1.569521 0.254591 6.1649 7.054e-10 *** ## Friendship 1.442215 0.294275 4.9009 9.540e-07 *** ## PSAB_BA 3.613222 0.160670 22.4885 &lt; 2.2e-16 *** ## PSAB_BY 1.980571 0.270852 7.3124 2.625e-13 *** ## PSAB_XA 0.722097 0.371719 1.9426 0.052066 . ## PSAB_XB 0.968778 0.313233 3.0928 0.001983 ** ## PSAB_AY 0.538744 0.398285 1.3527 0.176164 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Null deviance: 2598.114 on 284 degrees of freedom ## Residual deviance: 704.5596 on 271 degrees of freedom ## Chi-square: 1893.554 on 13 degrees of freedom, asymptotic p-value 0 ## AIC: 732.5596 AICC: 734.1209 BIC: 783.6452 We will create a little data frame to compare the coefficients from our two days. compare_coefs &lt;- data.frame(date1 = mod4f$coef, date2 = mod4f_date2$coef) compare_coefs ## date1 date2 ## Recency_ji 3.50563742 2.7501192 ## Recency_ij -1.57639768 -0.6693709 ## Intercept -6.44769065 -7.0998433 ## Sender_male 0.21481729 0.1511222 ## Sender_teacher -1.14942292 0.6341937 ## Receiver_male 0.01560242 0.0283086 ## Receiver_teacher 1.43102317 2.3680580 ## Seating 0.85797281 1.5695214 ## Friendship 0.99271415 1.4422153 ## PSAB_BA 3.47840049 3.6132221 ## PSAB_BY 1.36214460 1.9805708 ## PSAB_XA 0.40014146 0.7220974 ## PSAB_XB 1.15129647 0.9687781 ## PSAB_AY 0.23115535 0.5387436 Overall, much of the same interactional rules we saw above (in the 'normal' day) hold when looking at this second date, where the class was more unruly. We still see turn taking in interactions, for example (AB and then BA). We still see rules around usurping the conversation, such that when A talks to B and then X jumps in, they are likely to talk to B. Still, there would appear to be some potentially important differences (we would want to explore this more formally). For example, the effects for friendship and seating are particularly important for the second day. Similarly, there is some evidence that the tendency for PSAB-BY p-shifts are relatively high here. A class that is more unruly tends to have interactions that are based more on friendship and adjacent seating (i.e. talking to neighbors rather than doing discussion). Additionally, there may be a higher (relative) tendency for nodes to form a kind of two-step interaction (A-B-Y) rather than just a simple return to the person addressing them (A-B-A). This would potentially create more disruption in the classroom, as a larger number of students are brought into the initial interaction event. It is also useful to compare these results to the kind of models we saw in the previous tutorial on STERGM. In general, STERG models allow us to test hypotheses about formation and persistence of ties. This opens up questions about triadic effects and larger group structures. In the language of relational event models, A may talk to B, and then B may talk to Y; but when B talks to Y, Y is very likely to be someone that A generally talks with. Thus, little groups in the classroom emerge that are harder to see in the relational event model than with STERGM. On the other hand, STERGM completely obscures the actual dynamics of moment-to-moment interactions, missing some of the ‘rules’ of interaction that come out so clearly in the relational event results. Chapter 13 has covered statistical network models, moving from the cross-sectional case all the way up to continuous-time network data. In Chapter 14, we still utilize statistical network models, but we focus on problems related to diffusion. "],["ch14-Network-Diffusion-R.html", "14 Network Diffusion", " 14 Network Diffusion Chapter 14 covers models of diffusion. There are two tutorials. The first tutorial covers the diffusion of infectious diseases through a network. The second tutorial focuses on social diffusion processes, using the adoption of an innovation (like a new product) as the motivating case. Both tutorials build on the material from previous chapters, including the chapters on dyads/triads (Chapter 7), groups (Chapter 8), centrality (Chapter 9), and statistical network models (Chapter 13). "],["ch14-Network-Diffusion-Infectious-Diseases-R.html", "14, Part 1. Network Diffusion: Infectious Diseases 14.1 Reading in Data 14.2 Specifying the Network Model 14.3 Estimating the Network Model 14.4 Specifying the Epidemic Model 14.5 Running Simulations", " 14, Part 1. Network Diffusion: Infectious Diseases In this tutorial, we will cover dynamic epidemiological models of diffusion. We will focus on the substantive case of infection spread (like HIV or HCV) through a risk network (like drug use or a sex network). We are particularly interested in how the features of the network shape the potential for widespread contagion. For example, how much does the epidemic potential decrease if the number of current partners (e.g., number of sex partners during a given period) decreases dramatically, so that network density decreases? We are also interested in how risk behavior affects contagion potential. We will utilize a dynamic network simulation in this tutorial. Simulation models are useful as they can be conditioned on empirical data, while also making it possible to explore complex, non-linear dynamics; i.e., how individual behavior and wider network features combine to shape infection potential. The model is dynamic as the network is allowed to evolve as the simulation progresses. Thus, both the network ties and the item of interest (the infection spreading over the network) are allowed to update over time. The focus is still on contagion, but actors are able to form and break ties as the simulation progresses; making it possible to explore how the process of tie formation, dissolution and maintenance affect the risk profile of a population, in terms of infection spread. 14.1 Reading in Data We will utilize the EpiModel package (Jenness, Goodreau, and Morris 2018) for this tutorial, so let’s go ahead and load that first. library(EpiModel) With EpiModel, the basic idea is to specify two kinds of processes, one related to the formation and breaking of network ties, and one related to the spread of an infection through the network, dictating how an infection is spread from node i to node j. We will walk through both processes below, but first we read in the data for our example. Epidemiological simulations are often informed by empirical data. Given the difficulty of collecting full network data on at-risk populations (or sensitive relations), researchers will often use sampled data to inform the simulation. With sampled network data, respondents are sampled from the population (using a random sample, RDS or convenience sample) and they answer questions about themselves and the people they are connected to on the relation of interest, such as drug or sex partners (see Chapter 6). This information is then used to inform the simulation, both in terms of the network model and the model of infection spread. Here, we will use a faux ego network data set (constructed for this lab) to demonstrate how sampled network data can be used to inform diffusion models of infection spread. We can assume that this data is drawn from an at-risk population of drug users. The data includes the following information about the respondent, or ego: the number of alters named for each ego (with no cap) and information on where ego lives (city or suburbs). The data also includes information on up to 3 named drug partners; we have information on whether each named alter lives in the city or suburbs. We can assume that ego reports on the location for each alter. Let's first read in this example data set. url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/ego_network_example_data.txt&quot; ego_dat &lt;- read.table(file = url1, header = T, stringsAsFactors = FALSE) head(ego_dat) ## ids location degree location1 location2 location3 ## 1 1 city 1 city &lt;NA&gt; &lt;NA&gt; ## 2 2 city 9 city suburbs city ## 3 3 city 2 city city &lt;NA&gt; ## 4 4 city 2 city city &lt;NA&gt; ## 5 5 city 5 city city city ## 6 6 city 5 city suburbs city The main variables are: ids = ego id; location = does ego live in city or suburbs?; degree = number of current drug partners for ego (say in the last 3 months); location1 = does first named alter live in city or suburbs?; location2 = does second named alter live in city or suburbs?; location3 = does third named alter live in city or suburbs? We will use this data to walk through the process of specifying the network and epidemic models. We will begin by walking through the basic steps of setting up the simulation. We will then systematically vary the features of the simulation, to see how shifts in network formation and risk behaviors change the epidemic potential. 14.2 Specifying the Network Model The first step in the process is to specify the features of the network that will be utilized in the simulation. In this case, we will use the ego network data read in above as a means of constructing the network. This ensures that our simulation is based on information from the population of interest. We will first define the size of the network to be used in the simulation. For simplicity, we will set the network size as the size of the sample. Note that it is also possible to employ a network that is larger than the size of the sample. For example, if we had known the true size of the population we could have set the number of nodes to that value. Let's see how many people are in the sample: num_nodes &lt;- nrow(ego_dat) num_nodes ## [1] 750 Here we create an empty, undirected network of size 750 (defined in num_nodes). This will be in the network (not igraph) format. epi_network &lt;- network.initialize(n = num_nodes, directed = F) Now, we will take our constructed network and add some nodal attributes, based on the values seen in the observed data. To put the attributes onto the empty network we use a set.vertex.attribute() function. Here we will add location as a vertex attribute, setting the nodes in the network to have the same distribution of city/suburbs as seen in the ego network data. epi_network &lt;- set.vertex.attribute(epi_network, attrname = &quot;location&quot;, value = ego_dat$location) epi_network ## Network attributes: ## vertices = 750 ## directed = FALSE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 0 ## missing edges= 0 ## non-missing edges= 0 ## ## Vertex attribute names: ## location vertex.names ## ## No edge attributes Now that we have the most basic features of the network defined, we need to specify a model that determines how ties form and break within the simulation. Here, we will base the network formation model on the information found in our ego network data. The ego network data in this case is restricted to number of alters, location of ego and location of named alter. Actual surveys may include much more detailed information; for example, about the duration of the relationship and the frequency of interaction. We will now specify our tie formation formula, dictating how actors form ties over time within the simulation. The formula takes the same kinds of terms used in the ERGM and STERGM tutorials in Chapter 13 and is based on the available information from the ego network data. We will keep the model very simple and only include terms for edges (setting the baseline rate for number of partners), a nodefactor term for location (showing if people in the city have more/less ties than those in the suburbs), and a nodematch term for location (setting the strength of homophily for location—e.g., do people in the city disproportionately form ties with people in the city?). There are a number of other terms we could have added, including a concurrent term that captures how many nodes have more than 2 edges in a given time step (an important determinant of epidemic potential). Here we set the base formula. formation_formula &lt;- formula(~ edges + nodefactor(&quot;location&quot;) + nodematch(&quot;location&quot;)) Now, we set the target statistics for each term in the formula. This will determine what kinds of networks are generated during the simulation (in a given period). The networks will be generated to be consistent with the target statistics on each term. The key is figuring out how to use the ego network data to set empirically-grounded target statistics. First, we set the number of edges. This sets the baseline number of edges in the simulation, or the number of edges that exist (on average) per period. Here, we can use the mean degree in the ego network data, coupled with the size of the network, to set the desired number of edges. The total number of edges is just the mean degree * the number of nodes in the network. Let’s calculate mean degree from the ego network data. mean_degree &lt;- mean(ego_dat$degree) mean_degree ## [1] 3.706667 We can see that respondents have 3.707 edges on average. Now, we will take the mean degree, 3.707, multiply it by the number of nodes, 750, and divide it by 2 (as the network is undirected) to get the total number of edges. edges &lt;- mean_degree * num_nodes / 2 edges ## [1] 1390 Second, we set the nodefactor term on location. The nodefactor term is defined by the total number of ties emanating from one group. This is defined as: the mean degree of the group * the number of people in that group. Let's first calculate mean degree for those in the city and the suburbs. We will use a tapply() function to calculate mean degree by location. mean_degree_location &lt;- tapply(X = ego_dat$degree, INDEX = ego_dat$location, FUN = mean) mean_degree_location ## city suburbs ## 3.95 3.22 We see those in the city have higher degree than those in the suburbs. Now, let's calculate the number in the city and the suburbs. We do a simple table on location to see how many are in the city versus the suburbs. tab_location &lt;- table(ego_dat$location) tab_location ## ## city suburbs ## 500 250 We now need to calculate the total number of edges for each group by multiplying the mean degree for each group by the size of each group. edges_by_group &lt;- mean_degree_location * tab_location edges_by_group ## city suburbs ## 1975 805 In this case, city will serve as the reference category (by default the first category) in the formula and we thus only need the number of edges for those in the suburbs. edges_suburbs &lt;- edges_by_group[2] Finally, let's set the target statistic for the nodematch term, showing how strong homophily is for location. We need to calculate the number of edges we expect to match on location (so ij are both in the city or both in the suburbs). To get this from the ego network data, we first calculate the proportion of ego-alter pairs where ego and alter are in the same location (city/suburb). We will then take that proportion and multiply it by the total number of edges in the network. We begin by determining if the respondent has the same or different location as each alter. location_cols &lt;- c(&quot;location1&quot;, &quot;location2&quot;, &quot;location3&quot;) same_location &lt;- ego_dat[, &quot;location&quot;] == ego_dat[, location_cols] head(same_location) ## location1 location2 location3 ## [1,] TRUE NA NA ## [2,] TRUE FALSE TRUE ## [3,] TRUE TRUE NA ## [4,] TRUE TRUE NA ## [5,] TRUE TRUE TRUE ## [6,] TRUE FALSE TRUE This says that respondent 1 has 1 alter and they are in the same location; respondent 2 has 3 alters, and is in the same location as 1 and 3 but a different location as alter 2; and so on. Now, let's see what proportion of ego-alter pairs match on location. We will do a simple table on the data frame constructed above, with a prop.table() function applied to get the proportions: prop_match &lt;- prop.table(table(same_location)) prop_match ## same_location ## FALSE TRUE ## 0.1916966 0.8083034 So, about .808 of edges should match on location. Note that we still need to calculate the number of edges we expect to match on location (so far we just have the proportion). We will take the proportion matching (the second element in prop_match) and multiply it by the total number of edges in the desired network: num_match &lt;- round(prop_match[[2]] * edges) num_match ## [1] 1124 We see that 1124 edges should match on location. Now we put together the target statistics into one input vector. target_statistics_baseline &lt;- c(edges = edges, nodefactor.location = edges_suburbs, nodematch.location = num_match) target_statistics_baseline ## edges nodefactor.location.suburbs nodematch.location ## 1390 805 1124 This means that edges, in a given period, will form in a way consistent with these input statistics. For example, we would expect 1390 edges (or about 3.707 ties per person) in a given period, of which around 1124 should be within the same location. Note that we can alter these basic values in subsequent simulations, to see how shifting network formation patterns affect diffusion. We also need to specify a model that will dictate how edges are dropped over time. This is set via a dissolution_coefs() function. The arguments are: dissolution = a formula that determines how edges are dropped. This can take a number of forms. The simplest version is that all edges are dropped at the same rate; a more complicated model could allow edges of certain characteristics to end at different rates (so we may think that if two people share the same characteristic, then the edge may last longer). duration = the average duration of an edge in the simulation; this is recorded in arbitrary time units (but we can think of this as months for concreteness) d.rate = the death rate of the population (so that nodes can leave the network across time, at least potentially). Here, we create a fairly simple dissolution model. In this case, all edges dissolve at the same rate (set using offset(edges)), with average duration of 25 time units (not based on actual data, although it could be). The exit, or death rate, is set at .001. dissolution_coefs &lt;- dissolution_coefs(dissolution = ~ offset(edges), duration = 25, d.rate = .001) dissolution_coefs ## Dissolution Coefficients ## ======================= ## Dissolution Model: ~offset(edges) ## Target Statistics: 25 ## Crude Coefficient: 3.178054 ## Mortality/Exit Rate: 0.001 ## Adjusted Coefficient: 3.229321 14.3 Estimating the Network Model We now estimate the statistical network model that will be used in our epidemiological simulation. The function is netest(), which is a wrapper for the ergm() and tergm() functions explored in Chapter 13. The main arguments are: nw = network object used as basis for simulation formation = formula specifying formation of edges target.stats = input target statistics corresponding to formation formula coefs.diss = coefficients for dissolution of edges There are a number of other arguments which may be useful/necessary depending on the particular problem at hand. For example, it is possible to change the control parameters used in the estimation routine. Here, we set nw as the base network constructed above; formation is set to our formation formula; target.stats is set to our target statistics vector and coef.diss is set to our dissolution coefficients calculated above. We also use set.seed() to help in reproducing the model. set.seed(1002) net_mod &lt;- netest(nw = epi_network, formation = formation_formula, target.stats = target_statistics_baseline, coef.diss = dissolution_coefs) summary(net_mod) ## Call: ## ergm(formula = formation, constraints = constraints, offset.coef = coef.form, ## target.stats = target.stats, eval.loglik = FALSE, control = set.control.ergm, ## verbose = verbose, basis = nw) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -6.27626 0.06889 0 -91.105 &lt; 1e-04 *** ## nodefactor.location.suburbs 0.11724 0.03450 0 3.398 0.000678 *** ## nodematch.location 1.30176 0.06907 0 18.847 &lt; 1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Dissolution Coefficients ## ======================= ## Dissolution Model: ~offset(edges) ## Target Statistics: 25 ## Crude Coefficient: 3.178054 ## Mortality/Exit Rate: 0.001 ## Adjusted Coefficient: 3.229321 Before we can use the model in a simulation, we need to make sure it is acting like we expect. We can use the netdx() function to see if the model is producing networks that match the target statistics, an indication that the model is working correctly. The netdx() function will simulate networks from the estimated model and compare the statistics from the simulated networks to the input target statistics. There are two basic versions of the comparison, one dynamic and one static. For the static comparison, the model simulates networks based on the underlying model for one time period, with no tie gain/loss over time. This is directly akin to looking at convergence in an ERG model. For the dynamic comparison, the model compares the target statistics in the simulated network to the input target statistics (per period), while adjusting for edge dissolution and creation. The main arguments to netdx() are: x = model estimated from netest nsims = number of simulated networks dynamic = should look at dynamic statistics (T/F)? nsteps = number of time periods in dynamic simulation nwstats.formula = formula of statistics to test against, default is the target statistics specified in the formation formula. First, we will check the fit in the static case, to see if the base model reproduces the target statistics. Note that we do not include a nsteps argument as the simulated networks are only based on a single time period. mod_fit1 &lt;- netdx(x = net_mod, dynamic = F, nsims = 1000) plot(mod_fit1, legend = T) The fit looks good, with the simulated networks matching the target statistics in the cross-section. Now, let's look at the target statistics when we allow ties to dissolve and form over time. In this case, we will set dynamic equal to T and set the number of time periods, here equal to 300. Note that with 300 time periods for each simulation, we want to keep the number of simulations (nsims) fairly low (given the run time); here we set nsims to 5. mod_fit2 &lt;- netdx(x = net_mod, dynamic = T, nsims = 5, nsteps = 300) mod_fit2 ## EpiModel Network Diagnostics ## ======================= ## Diagnostic Method: Dynamic ## Simulations: 5 ## Time Steps per Sim: 300 ## ## Formation Diagnostics ## ----------------------- ## Target Sim Mean Pct Diff Sim SE Z Score SD(Sim Means) SD(Statistic) ## edges 1390 1390.714 0.051 7.269 0.098 31.195 44.990 ## nodefactor.location.suburbs 805 800.799 -0.522 5.253 -0.800 14.595 38.098 ## nodematch.location 1124 1127.045 0.271 8.195 0.372 30.132 45.903 ## ## Duration Diagnostics ## ----------------------- ## Target Sim Mean Pct Diff Sim SE Z Score SD(Sim Means) SD(Statistic) ## edges 25 25.028 0.11 0.089 0.309 0.282 0.611 ## ## Dissolution Diagnostics ## ----------------------- ## Target Sim Mean Pct Diff Sim SE Z Score SD(Sim Means) SD(Statistic) ## edges 0.04 0.04 0.008 0 0.023 0.001 0.005 We can see that there are two different sets of target statistics, one capturing the target statistics at different time points in the simulation and the second looking at the duration and dissolution of edges. Let's first look at the target statistics. plot(mod_fit2, legend = T) This generally looks okay. Now, let's look at duration and dissolution of ties. We will first set up the plot to have two columns. To plot the statistics dealing with the duration of ties we set type = \"duration\". To plot the statistics dealing with the dissolution of ties we set type = \"dissolution\" par(mfrow = c(1, 2)) plot(mod_fit2, type = &quot;duration&quot;) plot(mod_fit2, type = &quot;dissolution&quot;) The dotted line represents the expected duration of ties (set at 25 in this case). We can also look at the bottom part of the output from above to see if the duration and dissolution target values match the simulation. mod_fit2 ## EpiModel Network Diagnostics ## ======================= ## Diagnostic Method: Dynamic ## Simulations: 5 ## Time Steps per Sim: 300 ## ## Formation Diagnostics ## ----------------------- ## Target Sim Mean Pct Diff Sim SE Z Score SD(Sim Means) SD(Statistic) ## edges 1390 1390.714 0.051 7.269 0.098 31.195 44.990 ## nodefactor.location.suburbs 805 800.799 -0.522 5.253 -0.800 14.595 38.098 ## nodematch.location 1124 1127.045 0.271 8.195 0.372 30.132 45.903 ## ## Duration Diagnostics ## ----------------------- ## Target Sim Mean Pct Diff Sim SE Z Score SD(Sim Means) SD(Statistic) ## edges 25 25.028 0.11 0.089 0.309 0.282 0.611 ## ## Dissolution Diagnostics ## ----------------------- ## Target Sim Mean Pct Diff Sim SE Z Score SD(Sim Means) SD(Statistic) ## edges 0.04 0.04 0.008 0 0.023 0.001 0.005 We can see, for example, that the target percent of edges that dissolve is .04, matching what we see in the simulation (looking at the Sim Mean column). 14.4 Specifying the Epidemic Model The second main step is to specify the epidemic part of the simulation, determining the manner in which the infection spreads through social connections. EpiModel allows for a flexible range of models to be specified, although it is also possible to write additional code to extend the functionality of the package. Here, we will consider a simple contagion model, where actors, once infected, can pass it on to those they interact with (e.g., share needles with) in a given time period. In the language of epidemiological studies, we will consider a SIS model (susceptible-infected-susceptible) where actors are either infected or susceptible to be infected. We will assume that actors can be reinfected, once recovered. We could alternatively assume that actors cannot be reinfected, thus running an SIR model (susceptible-infected-recovered). To run an epidemiological simulation, in conjunction with our network model specified above, we need to create a number of inputs, shaping the features of the simulation. As a first step, we need to create a vector that captures the initial state that each node is in at the start of the simulation. Each node must be set to a \"s\" \"i\" or \"r\" for susceptible, infected or recovered. Here, let's randomly select 3% of the population to be infected. We will use a sample() function, randomly sampling an s, i, or r state for each node, with probability .97, .03 and 0 (so no one is recovered). We set size to num_nodes so that each node in the simulation will have an initial state. initial_status &lt;- sample(c(&quot;s&quot;, &quot;i&quot;, &quot;r&quot;), size = num_nodes, replace = T, prob = c(.97, .03, 0)) table(initial_status) ## initial_status ## i s ## 21 729 We then need to feed this vector of initial states to the init.net() function, with status.vector set to the vector of initial states. initial_status_inputs &lt;- init.net(status.vector = initial_status) Now, we will go ahead and set the probabilities of infection and recovery using a param.net() function. The arguments are: inf.prob = the probability of infection in a given interaction (or act) act.rate = number of interactions (or acts) in a time period between i and j, assuming that i and j are connected in that period. rec.rate = the rate of recovery in a time period Here we will we set the probability of infection to .025, so that in a given act between i and j (like sharing needles) the probability of infection is .025. We set the number of acts per period to 1. This means that a pair, ij, that have a relationship have 1 risk event per period. In each risk event, node i, if infected, can pass the infection to j. Finally, we set the recovery rate to .01, so that a node has a .01 probability of recovering in a given time period. These inputs could be informed by actual data or could represent theoretical inputs, as in this case (which could then be varied to see their effect on the epidemic potential). input_to_episim &lt;- param.net(inf.prob = 0.025, act.rate = 1, rec.rate = 0.01) input_to_episim ## Fixed Parameters ## --------------------------- ## inf.prob = 0.025 ## act.rate = 1 ## rec.rate = 0.01 Finally, we need to create an object that controls the simulation itself. Here we use the control.net() function. The main arguments are: type = SI, SIR, SIS nsteps = number of time periods for simulation nsims = number of simulations to perform ncores = number of processors to use in simulation (if multiple cores are to be used) Here we will we set type to \"SIS\", have a simulation with 300 time periods, do the simulation 4 times and run it over 2 processors. We would want to do this with more simulations in an actual analysis. control_episim &lt;- control.net(type = &quot;SIS&quot;, nsteps = 300, nsims = 4, ncores = 2) control_episim ## Network Model Control Settings ## =============================== ## type = SIS ## nsteps = 300 ## start = 1 ## nsims = 4 ## ncores = 2 ## resimulate.network = FALSE ## tergmLite = FALSE ## cumulative.edgelist = FALSE ## truncate.el.cuml = 0 ## module.order = ## save.nwstats = TRUE ## nwstats.formula = formation ## save.transmat = TRUE ## verbose = TRUE ## verbose.int = 1 ## skip.check = FALSE ## raw.output = FALSE ## tergmLite.track.duration = FALSE ## save.diss.stats = TRUE ## attr.rules = &lt;list&gt; ## save.network = TRUE ## Dynamic Modules: resim_nets.FUN summary_nets.FUN infection.FUN recovery.FUN departures.FUN arrivals.FUN nwupdate.FUN prevalence.FUN 14.5 Running Simulations 14.5.1 Running Baseline Model We can now run our epidemiological simulation using the netsim() function. The main arguments are: x = fitted network model, based on netest() function param = model parameters, based on param.net() function init = initial status inputs, based on init.net() function control = control object, based on control.net() function Let’s go ahead and run the simulation using the objects constructed above. episim_baseline &lt;- netsim(x = net_mod, param = input_to_episim, init = initial_status_inputs, control = control_episim) To get an initial sense of the results, we can do a quick summary on the simulation object. We will first convert the object to a data frame. We will use out = \"mean\" to tell R to output the mean values over all simulations. summary_data_baseline &lt;- as.data.frame(episim_baseline, out = &quot;mean&quot;) head(summary_data_baseline) ## time sim.num s.num i.num num si.flow is.flow ## 1 1 750 729.00 21.00 750 NaN NaN ## 2 2 750 727.50 22.50 750 2.50 1.00 ## 3 3 750 726.00 24.00 750 2.25 0.75 ## 4 4 750 723.25 26.75 750 2.75 0.00 ## 5 5 750 721.25 28.75 750 2.00 0.00 ## 6 6 750 719.00 31.00 750 2.25 0.00 Each row in the data frame corresponds to a different time period in the simulation. The s.num column shows how many people in that period are susceptible (but not infected) while i.num shows how many people are infected. i.num shows the total number infected, while si.flow shows how many move from susceptible to infected (and is.flow shows the opposite) in a given period. The values correspond to the mean number over all simulations. We can also print the rate of infected and susceptible at particular days using a summary command and an at argument. Here we look at period 1. summary(episim_baseline, at = 1) ## ## EpiModel Summary ## ======================= ## Model class: netsim ## ## Simulation Details ## ----------------------- ## Model type: SIS ## No. simulations: 4 ## No. time steps: 300 ## No. NW groups: 1 ## ## Model Statistics ## ------------------------------ ## Time: 1 ## ------------------------------ ## mean sd pct ## Suscept. 729 0 0.972 ## Infect. 21 0 0.028 ## Total 750 0 1.000 ## S -&gt; I NaN NA NA ## I -&gt; S NaN NA NA ## ------------------------------ Or period 100: summary(episim_baseline, at = 100) ## ## EpiModel Summary ## ======================= ## Model class: netsim ## ## Simulation Details ## ----------------------- ## Model type: SIS ## No. simulations: 4 ## No. time steps: 300 ## No. NW groups: 1 ## ## Model Statistics ## ------------------------------ ## Time: 100 ## ------------------------------ ## mean sd pct ## Suscept. 118.75 7.805 0.158 ## Infect. 631.25 7.805 0.842 ## Total 750.00 0.000 1.000 ## S -&gt; I 7.50 1.000 NA ## I -&gt; S 8.75 2.062 NA ## ------------------------------ We can also plot the number of infected and susceptible over time, generating the diffusion curve for the simulation. The basic inputs are the netsim object and then the items to plot. Here we want to plot the number infected and susceptible, denoted by \"i.num\" and \"s.num\". We also add legend = T. par(mfrow = c(1, 1)) plot(episim_baseline, y = c(&quot;i.num&quot;, &quot;s.num&quot;), legend = T) We can see that in this particular simulation the infection spreads relatively quickly through the network, with over 80% infected by the 100th time period. 14.5.2 Varying Network Features One key advantage of a simulation is that we can systematically alter the input parameters (while holding other things constant), allowing us to see how shifting conditions could, theoretically, affect diffusion through the population. Here, we will keep the basic epidemiological simulation the same but tweak the network features to see how this changes the epidemic potential. In particular, we will ask how the spread of infection through the population is affected when the number of partners per person decreases substantially. We will assume that the number of edges in the network in a given time period is cut in half. Here individuals have, on average, degree of 1.854, rather than 3.707. To construct inputs for the simulation, we will take the target statistics used originally and simply multiple all of them by .5. This will directly cut the number of edges in half. It will also ensure that all of the other target statistics, like nodematch on location, are based on the new desired number of edges (here half the original). target_statistics_lowdegree &lt;- round(target_statistics_baseline * .5) target_statistics_lowdegree ## edges nodefactor.location.suburbs nodematch.location ## 695 402 562 Now we run the network model using the new target statistics. All other inputs are the same. net_mod_lowdegree &lt;- netest(nw = epi_network, formation = formation_formula, target.stats = target_statistics_lowdegree, coef.diss = dissolution_coefs) And let's check to make sure the model is working as expected. mod_fit_lowdegree &lt;- netdx(x = net_mod_lowdegree, dynamic = T, nsims = 5, nsteps = 300) plot(mod_fit_lowdegree, legend = T) Looks okay, so we can go ahead and use the dynamic network model in the epidemiological simulation. The inputs are the same as before but now we use the network model based on the lower mean degree. episim_lowdegree &lt;- netsim(x = net_mod_lowdegree, param = input_to_episim, init = initial_status_inputs, control = control_episim) Let's get a summary data frame of the new, lower degree simulation: summary_data_lowdegree &lt;- as.data.frame(episim_lowdegree, out = &quot;mean&quot;) And let's compare period 10, 25, 50, 100, 150 and 300 between the two simulations, just including the number infected (i.num in the summary data frames). period &lt;- c(10, 25, 50, 100, 150, 300) i.num.baseline &lt;- summary_data_baseline[, &quot;i.num&quot;] i.num.lowdegree &lt;- summary_data_lowdegree[, &quot;i.num&quot;] compare_dat &lt;- data.frame(period = period, i.num.baseline = i.num.baseline[period], i.num.lowdegree = i.num.lowdegree[period]) compare_dat ## period i.num.baseline i.num.lowdegree ## 1 10 39.50 28.25 ## 2 25 93.75 40.75 ## 3 50 331.00 75.50 ## 4 100 631.25 203.75 ## 5 150 649.75 371.75 ## 6 300 653.75 545.25 Each row reports the number infected for each simulation for that time period. The second column reports the baseline model, run above (using mean degree of 3.707), while the third column reports the low degree simulation results. We can see that the low degree simulation has a lower number of infected at each time period, but the differences are striking around periods 50 to 150. We can also plot the difference in number infected between the two simulations. diff_infected &lt;- i.num.baseline - i.num.lowdegree plot(1:300, diff_infected, xlab = &quot;period&quot;, ylab = &quot;Difference in Number Infected&quot;, main = &quot;Comparison of Baseline to Low Degree Simulation&quot;) Now let's plot the diffusion curves for the baseline simulation and the low degree simulation on one plot. par(mfrow = c(1, 2)) plot(episim_baseline, y = c(&quot;i.num&quot;, &quot;s.num&quot;), legend = TRUE, main = &quot;Baseline&quot;) plot(episim_lowdegree, y = c(&quot;i.num&quot;, &quot;s.num&quot;), legend = TRUE, main = &quot;Low Degree&quot;) Overall, the average number of partners in a time period clearly affects the diffusion potential. With fewer edges per period, the potential to become infected and pass it on is greatly diminished. The decrease in average number of partners affects both the final number of infected, as well as the pace of infection, creating large differentials in infection in the middle periods. This suggests that decreasing the number of partners per period can reduce epidemic potential, but even here (in the low degree case) we still see a high rate of cases infected by the end of the simulation (around 75%). 14.5.3 Varying Epidemic Model Just as we were able to systematically vary the network model in the simulation, we can also vary the input parameters to the epidemic model. Here, we will keep the network the same as in the baseline model (so average degree is set to 3.707, based on the original ego network data). We will only alter the inputs to the epidemiological part of the simulation. We will focus on the activity rate parameter. This governs how many times two nodes with a connection interact in a given time period. Given a connection, node i and j may interact 0, 1, 2, 3... times in a given period, with increasing risk of infection as the number of interactions increases. Here we will create a new set of input parameters using the param.net() function. We will again set the infection probability to .025 and the recovery rate to .01. This time, however, we will lower the number of interactions per period (assuming i and j are connected) to .40. This effectively means that ij pairs who are connected will have .60 probability of having no risk events in a period and .40 probability of having one risk event (compare this to the previous simulation where all connected ij pairs had one risk event). Nodes thus have a relatively high degree, but lower probabilities of infection with each partner. Formally, we set the act.rate input to .40 in the param.net() function. input_to_episim_lowinteraction &lt;- param.net(inf.prob = 0.025, act.rate = .40, rec.rate = 0.01) We now rerun the epidemic simulation, using the lower interaction inputs. All other inputs are the same as in the baseline model. episim_lowinteraction &lt;- netsim(x = net_mod, param = input_to_episim_lowinteraction, init = initial_status_inputs, control = control_episim) Now let's plot the diffusion curves for the baseline simulation and the low interaction simulation on one plot. par(mfrow = c(1, 2)) plot(episim_baseline, y = c(&quot;i.num&quot;, &quot;s.num&quot;), legend = T, main = &quot;Baseline&quot;) plot(episim_lowinteraction, y = c(&quot;i.num&quot;, &quot;s.num&quot;), legend = T, main = &quot;Low Interactions&quot;) We can clearly see the effect of activity rate on epidemic size (looking at the \"i.num\" lines). When nodes have limited risk events in a given period (as the activity rate falls below 1), the potential for infection spread is greatly diminished. Even if nodes maintain a large set of partners, if they don’t interact with them at high rates (or do not engage in risky behavior), there is diminished chance for diffusion over the network. We thus begin to see the importance of distinguishing between relationships that exist and specific interactions that take place in a given period. Substantively, we have seen how average degree and number of interactions per period affect epidemic potential. By lowering both the number of partners and the number of interactions (within relationships), the pace of infection spread decreases. In each case, however, the number infected at the end of the simulation is still quite high. As a further analysis, we could uncover how low these two inputs (or other potential inputs) would have to be set to lower the final epidemic size. We could also use this simulation platform to ask different kinds of questions; for example we may be interested in seeing the rate of infection between urban and suburban actors. We may also want to know how the differences between urban/suburban change as the features of the simulation change, in terms of network features and epidemic behavior. Finally, note that the EpiModel package is quite flexible, making it possible to specify a wide range of network and disease spread processes. We have explored only a small part of the full range of possibilities. In the next tutorial, we will explore diffusion processes in the context of adopting a new innovation, shifting away from biological transmission to more social diffusion processes. "],["ch14-Network-Diffusion-Social-Processes-R.html", "14, Part 2. Social Diffusion Processes 14.6 Setting up the Session 14.7 Simple Diffusion Models 14.8 Threshold Models of Diffusion 14.9 Working with Empirical Data", " 14, Part 2. Social Diffusion Processes This is the second tutorial on diffusion processes in networks. The first tutorial focused on the spread of infectious diseases through a network. Here, we will focus on social diffusion processes, where the item being diffused is not biological and transmission happens via social processes (mimicry of friends, positive exposure to the item, etc.). We use the adoption of an innovation (like a new product) as the motivating case. We explore diffusion processes using a fixed network structure. The main question of this tutorial is how quickly an innovation becomes widely adopted. We are particularly interested in the conditions which increase/decrease the potential for widespread adoption. For example, how does network structure combine with adoption behavior to shape diffusion potential? Is diffusion slower when the network has strong group boundaries? If so, is this always the case, or does it depend on the process by which actors adopt the product? For example, what happens when the probability of adoption (in a given interaction) is very low or very high? We will consider two main approaches to modeling diffusion processes. First, we will consider simple contagion models, where the probability of adoption is based only on the probability of direct transmission from i to j. Second, we will consider threshold models, where there is some base number of friends that must adopt before an actor will consider adopting; e.g., an actor may only take up a risky behavior if more than 1 of their friends have already done so. 14.6 Setting up the Session We will utilize a network from the Add Health study for this tutorial. The basic idea is to condition, or constrain, our analyses on the features of an observed network. Add Health is a study of adolescents in schools. The study collected detailed health and behavior information for each student across a large number of schools. They also collected network information, asking each student to name up to 5 male and 5 female friends. Here, we take one school network as the basis for the analysis. We will utilize a synthetic version of the network; one that is generated based on the original data (with the same basic features) but cannot be linked back to the original data. The network of interest has 658 nodes. We begin by reading in the edgelist: url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/addhealth_edgelist.txt&quot; ah_edgelist &lt;- read.table(file = url1, header = T) head(ah_edgelist) ## sender receiver ## 1 1 274 ## 2 1 449 ## 3 1 457 ## 4 1 564 ## 5 2 417 ## 6 2 459 An edge exists if i nominated j as a friend. Note that the edgelist is based on a network that has already been symmetrized, so that if i nominated j or j nominated i we assume that a tie exists. Let's go ahead and make a network object to be used in our diffusion analysis. We will utilize the igraph package. library(igraph) Now, we will create an igraph object based on the edgelist read in above. We treat the network as undirected and denote that there are 658 nodes, setting the ids in the vertices statement. ah_network &lt;- graph_from_data_frame(d = ah_edgelist, directed = F, vertices = data.frame(ids = 1:658)) We will also utilize a second, comparison network in our diffusion analysis. For the second network, we will use a random network with the same number of nodes and degree distribution as in the Add Health network. This offers a nice comparison, as we can see how two networks with the same degree distribution (and thus density) but different structural features yield different diffusion results. Let's go ahead and read in the edgelist for this random network (one could also have created it within R using the sample_degseq() function). url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/randnet_edgelist_ahexample.txt&quot; randnet_edgelist &lt;- read.table(file = url2, header = T) And now we create the igraph object from the edgelist: rand_network &lt;- graph_from_data_frame(d = randnet_edgelist, directed = F, vertices = data.frame(ids = 1:658)) Let's calculate some basic network statistics for our two networks. We start with density. edge_density(ah_network) ## [1] 0.01327763 edge_density(rand_network) ## [1] 0.01327763 We can see the density is the same across the two networks. And now we calculate transitivity. transitivity(ah_network) ## [1] 0.1472136 transitivity(rand_network) ## [1] 0.01814668 Clearly transitivity is higher in the Add Health network than in the random network. And now we calculate distance, doing a summary over all distances, excluding the diagonal. dist_mat_ah &lt;- distances(graph = ah_network, mode = &quot;out&quot;) summary(dist_mat_ah[dist_mat_ah != 0]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 3.000 4.000 3.517 4.000 7.000 dist_mat_randnet &lt;- distances(graph = rand_network, mode = &quot;out&quot;) summary(dist_mat_randnet[dist_mat_randnet != 0]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 3.000 3.000 3.229 4.000 7.000 We can see that the distance between nodes tends to be a little higher in the Add Health network. More generally, we can see that the two networks have the same density but otherwise are structurally different, with the Add Health network with higher levels of transitivity and higher distance between nodes. This is indicative of subgroups forming in the school that do not emerge when edges form randomly. 14.7 Simple Diffusion Models We will utilize simulation to explore different diffusion processes. In general, simulations are useful as they naturally capture the complex, stochastic process by which items spread (or not) through a network. This would be difficult to capture using traditional statistical models. Simulation also makes it easy to systematically vary key features, while holding others constant. Note that in this analysis we will hold the network fixed within a given simulation. Thus, items flow over a network that does not change over time (see Chapter 15 for the coevolution of behaviors and networks). For concreteness, we can think of the innovation spreading through the network as a new product, where actors who are surrounded by people who adopt the product are more likely to take up the new product themselves. Let’s begin the process of specifying our diffusion model. There are a number of possible diffusion models that we may want to explore, but here we focus on simple contagion. In general, the model works by starting, in period 1, with an initial set of seeds who are 'infected' with the item of interest. This can be selected at random or using more specific criteria. In period 2, the first adopter 'interacts' with their immediate neighbor, passing it on with a given probability (either set to explore different conditions or based on actual data). Thus, once i has acquired the item of interest, they can pass it on (probabilistically) to all of the people they are directly connected to; individuals who pick it up from i can then pass it on to their friends, and so on. It is also possible to allow nodes to drop the product, so they no longer use a product they once did. In subsequent periods the process repeats itself with nodes adopting the product, dropping the product and passing it on. Thus, simple contagion works by focusing only on the direct probability of infection from i to j. To facilitate running diffusion simulations over these networks, we will make use of a set of simple R functions written for this tutorial. The functions make it easy to run a diffusion simulation where the model is based on simple contagion and the network is held fixed. Let's read these functions in: source(&quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/R/simple_contagion_function.R&quot;) The main function is simple_contagion_function(). The arguments are: net = network of interest, as an igraph object initial_seeds = vector indicating which nodes start as 'infected' time_periods = number of periods in simulation prob_infection = probability of passing from i to j prob_recovery = probability of recovering in a given time period The function will run a simulation model where the initial seeds start as infected, pass it to their neighbors with the probability set in prob_infection. They adopt probabilistically and the process continues until the simulation reaches the period set in time_periods. Here, we will set our inputs to explore the joint effects of network structure and adoption rules on diffusion. We set the probability of infection (or adopting the product) at two different levels, one 'high' and one 'low', to see how differing the probability of adoption shapes diffusion through the network. The high value will be set at .3 (so that j has a .3 probability of adopting the product if i has adopted the product). The low value is set at .1. We will set the recovery rate at .05 in all simulations (so that a node has a .05 probability of dropping the product once adopted). We will run the simulation over 25 time periods. We will pick one node to serve as our initial seed. Note that we would generally want to run this multiple times, with different seeds to see how this affects the results. Here we set node 3 as the seed, our initially infected node. initial_seeds &lt;- 3 time_periods &lt;- 25 prob_recovery &lt;- .05 First, let's run the simulation using the Add Health network and a high transmission probability (prob_infection = .3). For everything else we will use the values set above. Before we run the simulation, let's set a seed in R to make it easier to replicate. set.seed(5000) ah_simresults_highprob &lt;- simple_contagion_function(net = ah_network, initial_seeds = initial_seeds, time_periods = time_periods, prob_infection = .3, prob_recovery = prob_recovery) names(ah_simresults_highprob) ## [1] &quot;results&quot; &quot;cumulative_prop&quot; The output is a list with two elements. The first part of the list is a list of results, with one element corresponding to each time period in the simulation. For each time period, we see the nodes that are infected (i.e. adopters) and those that are susceptible, those cases who are not infected but could be. The second part of the list is a cumulative distribution, showing the total proportion infected at the end of each time period. head(ah_simresults_highprob$cumulative_prop) ## [1] 0.001519757 0.004559271 0.010638298 0.025835866 0.080547112 0.220364742 Now, let's do the same thing using the lower value for the probability of transmission (prob_infection = .1). All other inputs are the same. ah_simresults_lowprob &lt;- simple_contagion_function(net = ah_network, initial_seeds = initial_seeds, time_periods = time_periods, prob_infection = .1, prob_recovery = prob_recovery) And for the sake of comparison, let's run the same simulations using the random network. Here we use the high transmission probability. rand_simresults_highprob &lt;- simple_contagion_function(net = rand_network, initial_seeds = initial_seeds, time_periods = time_periods, prob_infection = .3, prob_recovery = prob_recovery) And here we use the low transmission probability. rand_simresults_lowprob &lt;- simple_contagion_function(net = rand_network, initial_seeds = initial_seeds, time_periods = time_periods, prob_infection = .1, prob_recovery = prob_recovery) Let's take a look at the results. The easiest way to compare the results across runs is to plot the proportion who adopt for each of the four scenarios, the two networks by the two levels of adoption. First, we set up the plot to have 25 time periods on the x-axis and 0 to 1 (proportion adopting) on the y-axis. We then add 4 lines to the plot. The high probability lines will be blue while the low probability lines will be red. We will use a solid line for the random network and a dashed line for the Add Health network. plot(x = 1:25, ylim = c(0, 1), type = &quot;n&quot;, xlab = &quot;Time Period&quot;, ylab = &quot;Proportion Adopting&quot;, main = &quot;Diffusion Curves across Networks and Adoption Behavior&quot;) # high transmission probability, random network lines(x = 1:25, y = rand_simresults_highprob$cumulative_prop, lty = 1, col = &quot;blue&quot;) # high transmission probability, Add Health network lines(x = 1:25, y = ah_simresults_highprob$cumulative_prop, lty = 2, col = &quot;blue&quot;) # low transmission probability, random network lines(x = 1:25, y = rand_simresults_lowprob$cumulative_prop, lty = 1, col = &quot;red&quot;) # low transmission probability, Add Health network lines(x = 1:25, y = ah_simresults_lowprob$cumulative_prop, lty = 2, col = &quot;red&quot;) # adding a legend legend(&quot;bottomright&quot;, c(&quot;Random Net High Prob&quot;, &quot;Add Health High Prob&quot;, &quot;Random Net Low Prob&quot;, &quot;Add Health Low Prob&quot;), col = c(&quot;blue&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;red&quot;), lty = c(1, 2, 1, 2)) We can see that in this case both network structure and adoption behavior (here set by the probability of adoption) have large impacts on diffusion. For example, looking at the low probability simulations, the random network has a steeper diffusion curve than the Add Health network. The random network has higher rates of early adoption, reaches saturation earlier, and has a higher level of adoption at the end of the 25 time periods. This demonstrates how two networks with the same density (volume of ties) but different patterns of interaction can have different potential for diffusion. More specifically, the Add Health network, with stronger clustering, tends to have slower diffusion (as least in this case). The differences between the random network and the Add Health network are, however, much smaller in the high probability analysis, suggestive of the potentially complex interaction between network structure and adoption behavior. Putting the effects of network structure and behavior together can lead to large differences in diffusion. For example, compare the diffusion curve for the low probability, Add Health network (dashed red line) to the curve for the high probability, random network (solid blue line). It is important to note that the results are based on a single run (or only one iteration). If we had run the same simulation again, we would have gotten somewhat different results. More generally, these kinds of simulations can be highly variable. It is thus important to gauge the uncertainty in the estimates (and substantive conclusions) by summarizing over a large number of simulations, although we will not walk through this here. For an analysis like this, we might do 1000 simulations (although the only cost to doing more is computing time). 14.8 Threshold Models of Diffusion We have so far utilized a fairly simple model of diffusion in our analysis. We assumed that nodes passed on the innovation (or cultural item more generally) to their nearest neighbors with a certain probability. But what about if the process of diffusion doesn't work like that? For example, drawing on the diffusion of innovation literature (see Tom Valente's work for example, Valente and Vega Yon (2020)) we might think that actors adopt (or not) based on the number of people they are connected to that have already adopted. An actor may, for example, only adopt the product if they see that a large number of their friends have already adopted. This is likely to be the case for many actual outcomes, such as adopting a new product, joining a social movement/protest and the like. This is the basic idea behind threshold models of diffusion; where diffusion occurs when a certain threshold of adopters around the focal node is met. Different outcomes may have different kinds of thresholds, while different nodes may have different thresholds for adopting. 14.8.1 Simulating Diffusion Processes using the netdiffuseR Package We will make use of the netdiffuseR package (Vega Yon and Valente 2022) to run our threshold-based diffusion models. This package has many useful functions, but we will primarily focus on the diffusion simulation capabilities. library(netdiffuseR) netdiffuseR takes the network as a matrix, so we will begin by transforming our network object (in igraph) into a matrix. We will do this for both the Add Health network and the random network. ah_network_mat &lt;- as_adjacency_matrix(graph = ah_network) rand_network_mat &lt;- as_adjacency_matrix(graph = rand_network) The function to run the diffusion simulation is rdiffnet(). The main arguments to rdiffnet() are: t = number of time periods in the simulation seed.nodes = nodes to be set as seeds, those who have already adopted the innovation at the beginning of the simulation. This can also be set as a character string, like \"random\", telling the function to select seeds at random. seed.p.adopt = proportion to be selected as seeds if seeds not set by researcher in seed.nodes seed.graph = network of interest, as a matrix (or name of random graph to create) rewire = T/F, should network slices over time be generated by randomly rewiring ties? threshold.dist = a single value or scalar setting the thresholds (at what point will node i adopt the innovation?). Could also be a function determining the thresholds of each node. exposure.args = inputs to exposure function, such as normalized (T/F, should exposure be calculated as proportion (T) or absolute number of alters who adopt (F)?) We need to specify the network of interest (seed.graph), either as a static or dynamic graph. We will continue our example and use the static version of the simulation, but it is important to note that it is possible to incorporate a dynamic network into the simulation (see Chapter 3, Part 2 for the data structure). We also need to specify the initial seeds (who have adopted the product prior to the start of the simulation), as well as the model of diffusion; i.e., what is the process/rules by which node i will adopt the innovation. Let's begin by setting the seeds to act as our initial adopters. We will randomly select 5% of the nodes to be seeds. Let's first define the number of nodes in the network. num_nodes &lt;- nrow(ah_network_mat) Now we will define the number to be selected as initial seeds (5% of the total). num_seeds &lt;- round(.05 * num_nodes) num_seeds ## [1] 33 And here we use a sample() function to randomly select 33 nodes from the full set of nodes. We use set.seed() to help in reproducing the results. set.seed(15) seeds &lt;- sample(1:num_nodes, size = num_seeds) We will set the rest of the inputs within the function itself. We set time (t) to 10 periods, the network (seed.graph) to the Add Health network and rewire to FALSE. We will also set threshold.dist to 1. This means that each node will adopt the period after at least 1 of their nearest neighbors has adopted the innovation. We can think of this as a low risk product, as nodes adopt with relatively weak signals from their neighbors. We also set normalized to FALSE (in exposure.args) to ensure that the exposure is based on the number of people who have adopted around the focal node and not the proportion (to match the threshold model of 1 neighbor adopting leading to ego's adoption). exposure_list &lt;- list(normalized = FALSE) ah_simresults_threshold1 &lt;- rdiffnet(t = 10, seed.nodes = seeds, seed.graph = ah_network_mat, rewire = FALSE, threshold.dist = 1, exposure.args = exposure_list) class(ah_simresults_threshold1) ## [1] &quot;diffnet&quot; We can see the resulting object is a diffnet object. Let's take a look at the pieces of this diffnet object. names(ah_simresults_threshold1) ## [1] &quot;graph&quot; &quot;toa&quot; &quot;adopt&quot; &quot;cumadopt&quot; &quot;vertex.static.attrs&quot; &quot;vertex.dyn.attrs&quot; &quot;graph.attrs&quot; &quot;meta&quot; There are a number of useful things here. For example, we can grab the toa vector, showing the time of adoption for each node in the simulation. head(ah_simresults_threshold1$toa) ## 1 2 3 4 5 6 ## 3 1 3 3 2 2 We see that node 1 adopted in period 3, node 2 adopted in period 1 and so on. We can also grab the cumadopt matrix, showing the point of adoption for each node. The rows are the nodes and the columns are the time periods. There is a 1 if node i has adopted by time period j. head(ah_simresults_threshold1$cumadopt) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0 0 1 1 1 1 1 1 1 1 ## [2,] 1 1 1 1 1 1 1 1 1 1 ## [3,] 0 0 1 1 1 1 1 1 1 1 ## [4,] 0 0 1 1 1 1 1 1 1 1 ## [5,] 0 1 1 1 1 1 1 1 1 1 ## [6,] 0 1 1 1 1 1 1 1 1 1 We can use this matrix to calculate the proportion adopting after each time period. We simply sum up the columns (number adopting in period j) and divide by the total number of nodes: cumul_adopt &lt;- colSums(ah_simresults_threshold1$cumadopt) / num_nodes head(cumul_adopt) ## [1] 0.05015198 0.39361702 0.91641337 0.99848024 1.00000000 1.00000000 This is the same as using the cumulative_adopt_count() function, focusing on the prop row. cumulative_adopt_count(ah_simresults_threshold1) ## 1 2 3 4 5 6 7 8 9 10 ## num 33.00000000 259.000000 603.0000000 657.00000000 6.58000e+02 658 658 658 658 658 ## prop 0.05015198 0.393617 0.9164134 0.99848024 1.00000e+00 1 1 1 1 1 ## rate 0.00000000 6.848485 1.3281853 0.08955224 1.52207e-03 0 0 0 0 0 We can see that 0.394 adopted by period 2, 0.916 by period 3, etc. It is also useful to plot the cumulative distribution of adopters. Here we can use the plot_adopters() function, with the diffnet object as input. plot_adopters(ah_simresults_threshold1) By default, the plot shows the proportion adopting at each time period and the cumulative proportion adopting. The plot_diffnet() function also offers a useful way of representing the results of the simulation. plot_diffnet() plots the network at different time slices, coloring the nodes by whether they have adopted the innovation (also differentiating between old and new adopters). Here we plot time period 1, 3, 5 and 7. plot_diffnet(ah_simresults_threshold1, slices = c(1, 3, 5, 7)) Finally, we can do a summary on the diffnet object. Note that this will take a couple seconds to run. sum_model1 &lt;- summary(ah_simresults_threshold1) ## Diffusion network summary statistics ## Name : A diffusion network ## Behavior : Random contagion ## ----------------------------------------------------------------------------- ## Period Adopters Cum Adopt. (%) Hazard Rate Density Moran&#39;s I (sd) ## -------- ---------- ---------------- ------------- --------- ---------------- ## 1 33 33 (0.05) - 0.01 -0.00 (0.00) ## 2 226 259 (0.39) 0.36 0.01 0.01 (0.00) *** ## 3 344 603 (0.92) 0.86 0.01 0.01 (0.00) *** ## 4 54 657 (1.00) 0.98 0.01 -0.00 (0.00) * ## 5 1 658 (1.00) 1.00 0.01 - ## 6 0 658 (1.00) 0.00 0.01 - ## 7 0 658 (1.00) 0.00 0.01 - ## 8 0 658 (1.00) 0.00 0.01 - ## 9 0 658 (1.00) 0.00 0.01 - ## 10 0 658 (1.00) 0.00 0.01 - ## ----------------------------------------------------------------------------- ## Left censoring : 0.05 (33) ## Right centoring : 0.00 (0) ## # of nodes : 658 ## ## Moran&#39;s I was computed on contemporaneous autocorrelation using 1/geodesic ## values. Significane levels *** &lt;= .01, ** &lt;= .05, * &lt;= .1. The summary offers the number of adopters in each period, the cumulative number, the cumulative proportion of adopters, as well as a number of other useful summary measures. For example, the hazard (of adopting) is the proportion adopting in that period given they had already 'survived' (i.e., not adopted) prior to that period. So, for period 2 the hazard is: 226 / (658 - 33) = .362, as there are 226 adopters, 658 total nodes and 33 had adopted in period 1. 14.8.2 Comparing Diffusion under Different Network and Adoption Behavior Conditions We are now in a position to systematically change aspects of the simulation, to see how shifts in network structure, adoption behavior, etc. affect the profile of adoption. This is one of the main strengths of simulation, as we can systematically change the features of the world, seeing how these items aggregate up to affect more global outcomes (like the cumulative adoption rate for the whole network). We begin by keeping everything the same but running the simulation over the random network generated above. Thus, only the network structure is allowed to vary from the previous simulation. We change the seed.graph argument (to the random network, specified as a matrix) but nothing else. randnet_simresults_threshold1 &lt;- rdiffnet(t = 10, seed.nodes = seeds, seed.graph = rand_network_mat, rewire = FALSE, threshold.dist = 1, exposure.args = exposure_list) Let's also vary the threshold model, governing adoption behavior in the simulation. Here, we will set threshold.dist = 3. This means that nodes must have at least three friends adopt the innovation before they will pick it up themselves. We can think of this as a case of complex contagion, where the item of interest is risky, or uncertain, so that nodes need a higher number of positive signals before adopting. Let's rerun the simulation with this higher threshold, first for the Add Health network and then for the random network. ah_simresults_threshold3 &lt;- rdiffnet(t = 10, seed.nodes = seeds, seed.graph = ah_network_mat, rewire = FALSE, threshold.dist = 3, exposure.args = exposure_list) randnet_simresults_threshold3 &lt;- rdiffnet(t = 10, seed.nodes = seeds, seed.graph = rand_network_mat, rewire = FALSE, threshold.dist = 3, exposure.args = exposure_list) We will now plot the results of our four simulations. This will make it easier to compare across the four conditions of interest (random, low threshold; Add Health, low threshold; random, high threshold; Add Health high threshold). We will make use of the plot_adopters function(). We will do this four times, once for each of the simulations. We begin with the random network, low threshold simulation. We will set up the plot so that the color (bg) is blue and the shape is a triangle (pch = 25). We also set the what argument to \"cumadopt\" to get the cumulative proportion adopted. We then plot the results for the Add Health network with the low threshold. Here we set the shape to a circle (pch = 21) and also set add = T to add it to the previous plot. We then do the same thing for the high threshold simulations, setting the color to red. We once again use a triangle symbol for the random network and a circle for the Add Health network. #random network, low threshold plot_adopters(randnet_simresults_threshold1, bg = &quot;blue&quot;, pch = 25, include.legend = FALSE, what = &quot;cumadopt&quot;, main = &quot;Diffusion across Networks and Adoption Behavior&quot;) #AH network, low threshold plot_adopters(ah_simresults_threshold1, bg = &quot;blue&quot;, pch = 21, add = TRUE, what = &quot;cumadopt&quot;) #random network, high threshold plot_adopters(randnet_simresults_threshold3, bg = &quot;red&quot;, pch = 25, add = TRUE, what = &quot;cumadopt&quot;) #AH network, high threshold plot_adopters(ah_simresults_threshold3, bg = &quot;red&quot;, pch = 21, add = TRUE, what = &quot;cumadopt&quot;) legend(&quot;bottomright&quot;, c(&quot;Random Net Low Threshold&quot;, &quot;Add Health Low Threshold&quot;, &quot;Random Net High Threshold&quot;, &quot;Add Health High Threshold&quot;), pt.bg = c(&quot;blue&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;red&quot;), pch = c(25, 21, 25, 21)) There are a number of substantively important results that come out of this figure. First, it is clear that setting a higher threshold dramatically reduces the rate of adoption in the network. The red lines, with high thresholds, are much flatter, with fewer people adopting each period and lower overall rates of adoption at the end of the simulation. This is the case as nodes require more friends to adopt before they adopt, meaning fewer people will take on the innovation in each period. Second, looking at the low threshold simulations, we can see that the random network has slightly higher rates of diffusion than the Add Health network, but the lines are very close together; suggesting (again) that network structure matters less when adoption is easy and fast. Third, the story is very different with the high threshold model. Here, the Add Health network actually has faster diffusion than the random network. Thus, counter to our previous results, the network with strong group structure and higher transitivity actually has faster diffusion when the item of interest is complex (see Centola and Macy (2007), for example). In the Add Health network, if i and j are friends they are likely to know the same people, making it more likely that a node will get the same kinds of signals (to adopt) from their nearest neighbors. In the random network, i and j are less likely to know the same people, making it harder to have a cluster of people all adopting. Diffusion in the random network is thus particularly slowed with complex contagion, as the focal node must have at least 3 (in this case) of their friends already adopt. More generally, we can see that random networks will not necessarily always have faster diffusion, and this depends crucially on the adoption behavior of the nodes. Threshold models can be sensitive to the initial seeds and it is substantively important to understand what happens to diffusion potential as different types of nodes are selected as early adopters. To answer this question, we will rerun the same simulations as before but this time select seeds that are central to the network. We will again use the high threshold adoption behavior model. We could, as above, set the seeds for the simulation ourselves. Alternatively, we can handle this within the rdiffnet() function by setting seed.nodes to \"central\" (with seed.p.adopt set to the proportion that are set as seeds, .05). This automates the process, selecting the nodes that are most central to the network and making them seeds. Centrality is defined based on the degree of the nodes. Here we run the simulation for the Add Health network, with central nodes as seeds and the threshold set to high (threshold.dist = 3) ah_simresults_threshold3_central &lt;- rdiffnet(t = 10, seed.nodes = &quot;central&quot;, seed.p.adopt = .05, seed.graph = ah_network_mat, rewire = FALSE, threshold.dist = 3, exposure.args = exposure_list) Here we run the same simulation for the random network. randnet_simresults_threshold3_central &lt;- rdiffnet(t = 10, seed.nodes = &quot;central&quot;, seed.p.adopt = .05, seed.graph = rand_network_mat, rewire = FALSE, threshold.dist = 3, exposure.args = exposure_list) Now, let's redo our plot, this time comparing the central versus random seed results. We will only consider the high threshold simulations. First, we plot the random network results using central seeds. We will color this blue and use a triangle as the shape (pch = 25). We then plot the Add Health network using central seeds. We color this blue and use the circle as the shape (pch = 21). We then do the same thing for the random seed simulations (here we use red for the color). #random network, central seeds plot_adopters(randnet_simresults_threshold3_central, bg = &quot;blue&quot;, pch = 25, include.legend = FALSE, what = &quot;cumadopt&quot;, main = &quot;Random Versus Central Seeds: High Threshold Results&quot;) #AH, random seeds plot_adopters(ah_simresults_threshold3_central, bg = &quot;blue&quot;, pch = 21, add = TRUE, what = &quot;cumadopt&quot;) #random network, central seeds plot_adopters(randnet_simresults_threshold3, bg = &quot;red&quot;, pch = 25, add = TRUE, what = &quot;cumadopt&quot;) #AH, random seeds plot_adopters(ah_simresults_threshold3, bg = &quot;red&quot;, pch = 21, add = TRUE, what = &quot;cumadopt&quot;) legend(&quot;bottomright&quot;, c(&quot;Random Net, Central Seeds&quot;, &quot;Add Health, Central Seeds&quot;, &quot;Random Net, Rand Seeds&quot;, &quot;Add Health, Rand Seeds&quot;), pt.bg = c(&quot;blue&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;red&quot;), pch = c(25, 21, 25, 21)) Substantively, we see that the seeds shape the simulation in important ways. Not surprisingly, when central nodes act as early adopters, diffusion tends to be quicker, and this is true for both the random and Add Health networks. What is perhaps more surprising is how much the shift in seeds affects the random network (compared to the Add Health network). With central nodes acting as seeds, the random network once again has faster diffusion than the Add Health network (at least after period 4), even with the complex contagion model. Thus, the Add Health network only has faster diffusion (under complex contagion) when seeds are selected at random. These results demonstrate the interdependencies between factors in the simulation, where having central nodes as seeds spreads the adoption far enough at the beginning to overcome the lack of group structure in the random network, leading to faster diffusion rates in the long run. There are a number of other factors we could alter in the simulation. For example, we could specify a more complicated adoption behavior model. So far we have changed the threshold levels for adoption but kept that level the same for every node in the simulation. But a more realistic simulation could allow different nodes to have different thresholds. Perhaps some actors are easily convinced to pick up the new product while others need more convincing. We can parametize this by creating a vector of thresholds, capturing how many friends must adopt before a given node will adopt. We will demonstrate how to handle heterogeneous thresholds with a simple example. We will begin by setting the threshold of each node by taking a random draw from a uniform distribution with mean 3 (min = 0 and max = 6). Thus, on average, the threshold is the same as in the previous simulations, but here nodes can be a bit above or below the mean value. thresholds &lt;- round(runif(n = num_nodes, min = 0, max = 6)) head(thresholds) ## [1] 2 2 4 5 5 2 We can see, for example, that node 1 will only adopt when 2 neighbors have already adopted, while node 3 requires 4 neighbors. We will now rerun our simulation, using the random seeds (same as used above), the Add Health network, but a different adoption behavioral model than before; in this case, setting threshold.dist to the vector of thresholds, allowing each node to have a different susceptibility to adoption. ah_simresults_diffthreshold &lt;- rdiffnet(t = 10, seed.nodes = seeds, seed.graph = ah_network_mat, rewire = FALSE, threshold.dist = thresholds, exposure.args = exposure_list) And now let's compare the results. We will compare this new simulation (with variable thresholds) to the simulation using the same network and seeds but fixing the threshold at 3 for all nodes. Let's first plot the new model, with the variable thresholds. We will color this blue. We then plot the results with the fixed threshold of 3. We make this red. #AH, variable thresholds plot_adopters(ah_simresults_diffthreshold, bg = &quot;blue&quot;, pch = 21, include.legend = FALSE, what = &quot;cumadopt&quot;, main = &quot;Comparing Variable and Fixed Thresholds&quot;) #AH, fixed thresholds plot_adopters(ah_simresults_threshold3, bg = &quot;red&quot;, pch = 21, add = TRUE, what = &quot;cumadopt&quot;) legend(&quot;bottomright&quot;, c(&quot;Add Health, Varied Thresholds&quot;, &quot;Add Health, Threshold = 3&quot;), pt.bg = c(&quot;blue&quot;, &quot;red&quot;), pch = c(21, 21)) The results clearly show that having variable thresholds increases the rate of diffusion. While some nodes are reluctant to take up the innovation, some need very little prompting (i.e., few neighbors need to adopt before they adopt), making diffusion easier in the early time periods. On the other hand, the variable threshold model plateaus at late time periods (as high threshold nodes are likely slowing down diffusion at the very end of the simulation), while the fixed threshold model does not. 14.8.3 Simulations using Multiple Runs So far, we have run the simulation models for different scenarios. In each case we ran the simulation one time, plotting the results for a particular run. The netdiffuseR package makes it easy to run a simulation multiple times, however. This is useful as it makes it possible to see how variable the results are across different runs of the same basic simulation. In this case, we will rerun our random seed models but do it multiple times, each time using different randomly selected seeds as starting points. Here we will employ a rdiffnet_multiple() function. This has identical arguments as rdiffnet but with a few added options to control the multiple runs: statistic = function describing the statistic of interest to be calculated for each run R = number of times to repeat simulation ncpus = number of cores to utilize if using parallel processing In this case, we first create a function to tell rdiffnet_multiple() what outcomes to save out for each simulation. We will create a function to save out the cumulative proportion adopting for each time period (using cumulative_adopt_count). Note that the input will be a diffnet object, based on the given simulation. stat_func &lt;- function(x) {cumulative_adopt_count(x)[&quot;prop&quot;, ]} This function then becomes an input to rdiffnet_multiple(). We will also set R to 100 (doing 100 simulations) and ncpus to 2 (using 2 processors). We will repeat the analysis twice, first using threshold set to 1 and then using threshold set to 3. In both cases we will use the Add Health network. Note that we need to set seed.nodes to \"random\" and seed.p.adopt to .05, telling the function to randomly select 5% of the nodes as seeds in each simulation. ah_simresults_multiple_threshold1 &lt;- rdiffnet_multiple(statistic = stat_func, R = 100, ncpus = 2, t = 10, seed.nodes = &quot;random&quot;, seed.p.adopt = .05, seed.graph = ah_network_mat, rewire = FALSE, threshold.dist = 1, exposure.args = exposure_list, stop.no.diff = FALSE) class(ah_simresults_multiple_threshold1) ## [1] &quot;matrix&quot; &quot;array&quot; dim(ah_simresults_multiple_threshold1) ## [1] 10 100 The output is a matrix with 10 rows and 100 columns. The rows correspond to the time periods in the simulation. The columns correspond to the different simulations performed, in this case 100 (set using R). Let's take a look at the first five columns, where each value shows the proportion adopting for that time period and simulation. ah_simresults_multiple_threshold1[, 1:5] ## [,1] [,2] [,3] [,4] [,5] ## 1 0.04863222 0.04863222 0.04863222 0.04863222 0.04863222 ## 2 0.34650456 0.40577508 0.35410334 0.34194529 0.35714286 ## 3 0.90729483 0.91945289 0.88449848 0.91945289 0.91337386 ## 4 0.99848024 1.00000000 0.99544073 1.00000000 0.99848024 ## 5 1.00000000 1.00000000 1.00000000 1.00000000 1.00000000 ## 6 1.00000000 1.00000000 1.00000000 1.00000000 1.00000000 ## 7 1.00000000 1.00000000 1.00000000 1.00000000 1.00000000 ## 8 1.00000000 1.00000000 1.00000000 1.00000000 1.00000000 ## 9 1.00000000 1.00000000 1.00000000 1.00000000 1.00000000 ## 10 1.00000000 1.00000000 1.00000000 1.00000000 1.00000000 We can see that the first period (row) always has the same proportion adopting (as the number of seeds does not vary across the simulations), but there is variation elsewhere, with different proportions of nodes adopting after each time period in different simulations. Now we do the same thing but set the threshold to 3 for all nodes. ah_simresults_multiple_threshold3 &lt;- rdiffnet_multiple(statistic = stat_func, R = 100, ncpus = 2, t = 10, seed.nodes = &quot;random&quot;, seed.p.adopt = .05, seed.graph = ah_network_mat, rewire = FALSE, threshold.dist = 3, exposure.args = exposure_list, stop.no.diff = FALSE) And now we will plot the two sets of results as a series of boxplots. We will have two plots, one for the low threshold model and one for the high threshold model. The x-axis will be the time period and each boxplot will capture the distribution of proportion adopting across simulations. We set up the plot to have two panels. Note that we transpose the matrices so that we put the time periods on the x-axis. par(mfrow = c(1, 2)) # Low threshold boxplot(t(ah_simresults_multiple_threshold1), xlab = &quot;Time&quot;, ylab = &quot;Proportion of Adopters&quot;, main = &quot;Low Threshold Model&quot;, boxwex = .5, ylim = c(0, 1)) # High threshold boxplot(t(ah_simresults_multiple_threshold3), xlab = &quot;Time&quot;, ylab = &quot;Proportion of Adopters&quot;, main = &quot;High Threshold Model&quot;, boxwex = .5, ylim = c(0, 1)) The variance is clearly lower (across simulation runs) for the low threshold model compared to the high threshold model. The low threshold model has such easy diffusion that global adoption happens quickly in every simulation. The high threshold model has the real potential for diffusion to not occur, meaning the initial seeds matter more and there is more variation run to run. 14.9 Working with Empirical Data Our analysis has thus far focused on using simulation to understand the conditions that amplify/dampen the potential for widespread diffusion. The netdiffuseR package also has a number of useful functions for dealing with empirical data. Here we assume that a researcher has collected information about the adoption of an innovation (product, etc.) over time and would like to model this is as a threshold process. Let's begin by reading in some example data for us to use. The data are based on the same Add Health network used above. We will read in a data frame containing the attributes of those nodes. url3 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/addhealth_attributes.txt&quot; ah_attributes &lt;- read.table(file = url3, header = T) head(ah_attributes) ## ids gender race grade toa ## 1 1 female white 12 NA ## 2 2 male white 11 NA ## 3 3 female white 11 8 ## 4 4 female white 9 4 ## 5 5 male white 12 4 ## 6 6 male white 10 5 We see that the data contain basic demographic information, like gender and grade. The data frame also contains a variable called toa, which is a variable showing the time of adoption for a new product (constructed for this example). NA indicates that the node never adopted the new product. A researcher with this kind of data, coupled with the network data used above, may be interested in knowing if people tend to adopt if their neighbors adopt. They may also want to know which actors have higher/lower thresholds of adoption, showing which actors take up the adoption easily and which need more prompting. The first step is to create a diffnet object based on the empirical data. Once we create our diffnet object we will able to make use of the various summary functions used above (demonstrated on the diffnet objects outputted from the simulation). We will make use of the new_diffnet() function to create the diffnet object. The main arguments are: graph = network of interest. This can be a static or dynamic (longitudinal) network. toa = vector showing time of adoption for each node vertex.static.attrs = data frame showing attributes for each node (restricted to attributes that don't change over time); see vertex.dyn.attrs for attributes that are allowed to change over time. Here, we create a new diffnet object based on the Add Health network, attributes and time of adoption. ah_diffnet &lt;- new_diffnet(graph = ah_network_mat, toa = ah_attributes$toa, vertex.static.attrs = ah_attributes) ah_diffnet ## Dynamic network of class -diffnet- ## Name : Diffusion Network ## Behavior : Unspecified ## # of nodes : 658 (1, 2, 3, 4, 5, 6, 7, 8, ...) ## # of time periods : 10 (1 - 10) ## Type : directed ## Final prevalence : 0.86 ## Static attributes : ids, gender, race, grade, toa (5) ## Dynamic attributes : - Let's do a summary on the diffnet object (again, this can take a few seconds to run). summary(ah_diffnet) ## Diffusion network summary statistics ## Name : Diffusion Network ## Behavior : Unspecified ## ----------------------------------------------------------------------------- ## Period Adopters Cum Adopt. (%) Hazard Rate Density Moran&#39;s I (sd) ## -------- ---------- ---------------- ------------- --------- ---------------- ## 1 33 33 (0.05) - 0.01 -0.00 (0.00) ## 2 31 64 (0.10) 0.05 0.01 0.00 (0.00) *** ## 3 47 111 (0.17) 0.08 0.01 0.01 (0.00) *** ## 4 74 185 (0.28) 0.14 0.01 0.01 (0.00) *** ## 5 104 289 (0.44) 0.22 0.01 0.02 (0.00) *** ## 6 109 398 (0.60) 0.30 0.01 0.02 (0.00) *** ## 7 87 485 (0.74) 0.33 0.01 0.01 (0.00) *** ## 8 55 540 (0.82) 0.32 0.01 0.01 (0.00) *** ## 9 18 558 (0.85) 0.15 0.01 0.01 (0.00) *** ## 10 7 565 (0.86) 0.07 0.01 0.01 (0.00) *** ## ----------------------------------------------------------------------------- ## Left censoring : 0.05 (33) ## Right centoring : 0.14 (93) ## # of nodes : 658 ## ## Moran&#39;s I was computed on contemporaneous autocorrelation using 1/geodesic ## values. Significane levels *** &lt;= .01, ** &lt;= .05, * &lt;= .1. A simple summary, as we saw above, offers important information, like number of adopters, cumulative proportion and the hazard rate of adoption. Note that we can use the plotting functions (like plot_adopters() and plot_diffnet()) to display the results. The first key question we will address is how the adoption thresholds vary across our nodes. As a researcher, we will not directly observe the threshold for each node, showing at what point they would have adopted the innovation (unlike with the simulations, where this was a known input). Rather, we simply get the exposure of each node to the innovation and the time of adoption. We can, however, use these two pieces of information to estimate the range of possible thresholds for each node (assuming that a threshold model of adoption fits this case well, so that nodes actually adopt the innovation after a certain number of their neighbors have done so). We will begin by getting the exposure of each node to the innovation using the exposure() function. The arguments are: graph = the diffnet object normalized = T/F; T if results should be presented as proportion of neighbors who have adopted; F if reported as counts lags = period, in lags, from which to output exposure; if set to 0 then outputs level of exposure in given period. First, let's grab the level of exposure at each time period with no lags. exposure_overall &lt;- exposure(ah_diffnet, normalized = FALSE, lags = 0) head(exposure_overall) ## 1 2 3 4 5 6 7 8 9 10 ## 1 0 0 0 1 3 4 4 4 4 4 ## 2 0 0 0 0 0 1 1 1 1 2 ## 3 0 0 0 1 2 5 6 6 7 7 ## 4 1 2 3 4 7 10 11 11 11 12 ## 5 2 3 8 10 12 13 13 13 13 13 ## 6 0 0 1 3 5 8 8 10 10 10 The rows correspond to the nodes in the network. The columns correspond to time periods. The values show the number of friends in that period, for that node, who have already adopted the innovation, thus capturing the exposure of node i. We can see, for example, that node 1 had 0 friends adopt until period 4, where they had 1 friend adopt, then 3 friends had adopted by period 5 and so on. Now, we will take our results from the exposure() function and use it to estimate the underlying threshold for each node. We will begin by calculating the level of exposure prior to adoption, or lagged exposure. The function is threshold(). The main arguments are: obj = exposure matrix toa = vector showing time of adoption lags = time of exposure to consider; lags = 1 is T - 1; lags = 2 is T - 2 and so on. The basic idea is that each node adopts the period after their threshold is met (assuming the threshold model is appropriate in this case). If a node adopts in period T, then they must have reached their threshold in period T - 1. We also know that they did not adopt in period T - 1, meaning the threshold had not been met in period T - 2. We can thus calculate the range of possible thresholds by calculating the exposure at T - 2 and T - 1. We thus need to calculate exposure using lags = 2 and lags = 1. Let's first look at the level of exposure two periods prior to adoption. thresh_lag2 &lt;- threshold(obj = exposure_overall, toa = ah_diffnet$toa, lags = 2) head(thresh_lag2) ## threshold ## 1 NA ## 2 NA ## 3 5 ## 4 2 ## 5 3 ## 6 1 We can see that for node 3 (node 1 and node 2 did not adopt) that 5 of their friends had adopted the innovation 2 periods prior to their own adoption. Now, let's do the same thing looking at the period prior to adoption. thresh_lag1 &lt;- threshold(obj = exposure_overall, toa = ah_diffnet$toa, lags = 1) head(thresh_lag1) ## threshold ## 1 NA ## 2 NA ## 3 6 ## 4 3 ## 5 8 ## 6 3 Here we see that node 3 had 6 friends adopt prior to their own adoption. Let's put those two vector together in a data frame. The latent threshold level must be greater than the exposure at T - 2 (as they did not adopt in T - 1) and less than or equal to the exposure at T - 1 (as they did adopt in T). threshold_dat &lt;- data.frame(lower_bound = thresh_lag2[, 1], upper_bound = thresh_lag1[, 1]) head(threshold_dat) ## lower_bound upper_bound ## 1 NA NA ## 2 NA NA ## 3 5 6 ## 4 2 3 ## 5 3 8 ## 6 1 3 This would suggest that node 3 has a latent threshold that must be greater than 5 and less than or equal to 6, making the threshold equal to 6 in this case. Or, looking at node 5, we see the threshold must be greater than 3 and less than or equal to 8 (so between 4 and 8). We do not know where the threshold is in that range, only that 3 was too little to have them adopt and that 8 was enough. Whether it took all 8 friends to convince them or less would have done it, is unclear from the data. We can go one step further and actually estimate a diffusion regression model, estimating the effect of exposure on the probability of adopting. The outcome of interest is whether a node adopted the innovation (0 = no adoption; 1 = adoption) at a given time period. The units are thus node X time period. So, if node i adopted in period 3, they would have a 0, 0, 1 as the outcome of interest. If node j adopted in period 4, they would have a 0, 0, 0, 1 as the outcome. The main independent variable is the level of exposure for each node in the previous period (how many friends around node i has already adopted the innovation?). Here we can make use of the handy diffreg() function, which will take our diffnet object and put it in a form appropriate for a logistic regression (estimated using glm). As a researcher, you simply need to put in a formula with the diffnet object as the outcome. The main predictor is exposure, here set to the exposure in T - 1 (so the exposure prior to the period in question). We will also include controls for time period, as adoption might start off quick but slow down over time. This is accomplished by using factor(per) in the formula. diffnet_model1 &lt;- diffreg(ah_diffnet ~ exposure(normalized = F, lags = 1) + factor(per), type = &quot;logit&quot;) summary(diffnet_model1) ## ## Call: ## glm(formula = Adopt ~ exposure + factor(per), family = binomial(link = &quot;logit&quot;), ## data = dat, subset = ifelse(is.na(toa), TRUE, toa &gt;= per)) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.85357 0.20086 -19.186 &lt; 2e-16 *** ## exposure 1.16615 0.05139 22.691 &lt; 2e-16 *** ## factor(per)3 -0.30245 0.26504 -1.141 0.253815 ## factor(per)4 -0.45997 0.26052 -1.766 0.077474 . ## factor(per)5 -0.54952 0.26307 -2.089 0.036718 * ## factor(per)6 -0.58495 0.27179 -2.152 0.031382 * ## factor(per)7 -0.97272 0.29785 -3.266 0.001092 ** ## factor(per)8 -1.09052 0.32706 -3.334 0.000855 *** ## factor(per)9 -2.00491 0.39573 -5.066 4.05e-07 *** ## factor(per)10 -2.80214 0.50595 -5.538 3.05e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2900.5 on 3258 degrees of freedom ## Residual deviance: 1700.3 on 3249 degrees of freedom ## (658 observations deleted due to missingness) ## AIC: 1720.3 ## ## Number of Fisher Scoring iterations: 6 We can see that the exposure does seem to predict adoption in this case (a positive, significant coefficient on exposure), net of the baseline time trends in adoption. The odds of adopting the innovation increase by exp(1.16615) = 3.210 times for every additional friend who had adopted the innovation in the previous period. We can also include controls for individual characteristics, important if we are trying to isolate the effect of exposure from individual propensities to adopt. Here we will control for gender and grade. These attributes are immediately available to be included in the formula as they were part of the original diffnet object constructed above. diffnet_model2 &lt;- diffreg(ah_diffnet ~ exposure(normalized = F, lags = 1) + factor(per) + factor(gender) + grade, type = &quot;logit&quot;) summary(diffnet_model2) ## ## Call: ## glm(formula = Adopt ~ exposure + factor(per) + factor(gender) + ## grade, family = binomial(link = &quot;logit&quot;), data = dat, subset = ifelse(is.na(toa), ## TRUE, toa &gt;= per)) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.15794 0.64658 -6.431 1.27e-10 *** ## exposure 1.16647 0.05147 22.662 &lt; 2e-16 *** ## factor(per)3 -0.30610 0.26589 -1.151 0.24963 ## factor(per)4 -0.44895 0.26124 -1.719 0.08570 . ## factor(per)5 -0.53517 0.26393 -2.028 0.04259 * ## factor(per)6 -0.55529 0.27236 -2.039 0.04147 * ## factor(per)7 -0.94968 0.29917 -3.174 0.00150 ** ## factor(per)8 -1.02542 0.32808 -3.126 0.00177 ** ## factor(per)9 -1.94833 0.39604 -4.919 8.68e-07 *** ## factor(per)10 -2.76384 0.50760 -5.445 5.18e-08 *** ## factor(gender)male -0.38988 0.12814 -3.043 0.00235 ** ## grade 0.04644 0.05824 0.797 0.42525 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2900.5 on 3258 degrees of freedom ## Residual deviance: 1690.7 on 3247 degrees of freedom ## (658 observations deleted due to missingness) ## AIC: 1714.7 ## ## Number of Fisher Scoring iterations: 6 The results suggest that exposure is still an important predictor of adoption, net of these individual-level controls. Note that the model assumes that there is no selection effect (so that individuals do not strongly form ties based on both having already adopted the innovation). If this assumption is badly violated, we would want to consider alternative modeling strategies, such as the models explored in Chapter 15. Overall, this tutorial has demonstrated the complex ways that network structure and adoption behavior come together to shape diffusion rates. For example, we saw that random networks tend to have fast diffusion, but not always, particularly when the item of interest is a risky one, requiring many people to adopt before actors will take up the innovation. We explored many different simulations but there are a number of ways that we could have extended our analysis. For example, we could have considered how homophily affects diffusion rates. It may be the case that individuals are influenced to a greater extent by those similar to them (here in terms of grade or gender). We could alter the adoption behavior model to put more weight on certain friends. We could then see how diffusion rates vary, given this homophily-based adoption model, in the random network (low homophily) and Add Health network (high homophily). Similarly, our analysis utilized a static network, but it is possible to incorporate a dynamic network, where the ties change over time. By incorporating a dynamic network, we can see how patterns of tie gain and loss affect the potential for diffusion in the network. We explore similar problems in Chapter 15, where we cover the coevolution of behaviors and networks using SIENA models (see also Part 1 from this chapter). "],["ch15-coevolution-networks-behaviors-siena-saom-R.html", "15 Social Influence: Coevolution of Networks and Behaviors 15.1 Reading in Classroom Data 15.2 SIENA Model Setup 15.3 Model Specification 15.4 Model Estimation 15.5 Interpretation 15.6 Checking Model Fit 15.7 Example with 3 Time Periods", " 15 Social Influence: Coevolution of Networks and Behaviors Chapter 15 covers models of social influence. We are interested in how actors come to resemble their peers in terms of behaviors (or attitudes, tastes, etc.). We will address questions of peer influence using SIENA (Simulation Investigation for Empirical Network Analysis), a stochastic actor orientated model (SAOM) designed to statistically capture the coevolution of networks and behaviors (Snijders, Van de Bunt, and Steglich 2010; Steglich, Snijders, and Pearson 2010). SIENA models accomplish similar tasks as the STERG models in Chapter 13, in terms of modeling network dynamics. With SIENA, however, it becomes possible to simultaneously look at changes in behaviors and network ties, opening up questions about influence, selection and wider network processes. Thus, with SIENA, we can still predict how ties change from T to T+1, as a function of mechanisms like reciprocity, but here we can also ask about peer influence on behavior (e.g., alcohol consumption). In this way, the tutorial connects the statistical models of Chapter 13 (e.g., ERGM) to the diffusion models of Chapter 14. And more substantively, we can think of SIENA as combining ERGM-like models with an explicit model of influence. Empirically, we focus on problems related to peer influence and selection in the case of adolescents at school. We offer two empirical examples. The first example utilizes a small classroom-based network. The data were collected by Daniel McFarland. There are two waves of data, corresponding to semester 1 and semester 2. The key outcome of interest is how much each student liked the subject of the class, measured in both semester 1 and semester 2. There is also data on friendships for both semesters. We want to know if students tend to converge on attitudes (about how much they like the subject) with people they are friends with. We also want to see how peer influence fits into a larger picture of network change, where ties are being added and dropped over time. For example, are students similar in terms of liking the subject because of peer influence or because they select friends with similar attitudes about the class (i.e., we want to differentiate between influence and selection)? The second part of the tutorial will offer a short example focused on alcohol use. It is worth noting that SIENA models are designed for fairly small networks. Researchers interested in estimating peer influence effects in larger network settings (e.g., n &gt; 2500) are likely to consider alternative approaches; such as those using more conventional regression models, which tend to scale better (see Ragan et al. (2022) for a comparison of conventional regression models and SIENA). 15.1 Reading in Classroom Data We begin with the classroom example. Let's read in the data and get it ready for the analysis. Here we read in the semester 1 edgelist. url1 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class237_sem1_edgelist.txt&quot; sem1_edgelist &lt;- read.table(file = url1, header = T) head(sem1_edgelist) ## sender receiver ## 1 113214 121470 ## 2 113214 125522 ## 3 113214 149552 ## 4 113214 122728 ## 5 113214 122706 ## 6 115909 127535 The first column shows the sender of the tie and the second column shows the receiver of the tie. An edge exists between i-&gt;j if i named j as a friend. Let's read in the semester 2 edgelist. url2 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class237_sem2_edgelist.txt&quot; sem2_edgelist &lt;- read.table(file = url2, header = T) Let’s also read in the attribute file, containing nodal characteristics. url3 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class237_attributes.txt&quot; attributes &lt;- read.table(file = url3, header = T) head(attributes) ## ids sem_id cls_id expected_grade like_subject had_teacher_before like_teacher course_challenging ## 1 113214 1 237 4 4 0 3 1 ## 2 121470 1 237 3 2 0 3 3 ## 3 122728 1 237 4 2 0 3 2 ## 4 125522 1 237 4 3 0 3 4 ## 5 126359 1 237 4 4 0 4 3 ## 6 122706 1 237 4 3 0 3 3 The main variables are: ids = id of actor; sem_id = semester where data comes from; expected_grade: D = 1 C = 2 B = 3 A = 4; like_subject: 1-4 scale, with 1 = strong dislike to 4 = like it a lot; like_teacher: 1-4 scale, with 1 = strong dislike to 4 = like it a lot. Let’s load some useful packages. We will utilize the igraph package to start. library(igraph) library(reshape) library(car) As this is over time data, we need to be aware of the fact that nodes can come in and out of the network through time. It is possible to do the analysis while incorporating the changing composition of the network (e.g., using structural zeros for the missing cases, as in the two-mode STERGM example in Chapter 13) and we will consider this in the second example below. Here we will keep things simple and only consider students who were in both semester 1 and semester 2. Let's find out which students were in both semesters. ids_sem1 &lt;- attributes$ids[attributes$sem_id == 1] ids_sem2 &lt;- attributes$ids[attributes$sem_id == 2] Here we identify the cases from semester 1 that were also in semester 2. ids_keep &lt;- ids_sem1[ids_sem1 %in% ids_sem2] Now, let's subset the attribute data frame and the edgelists to only include those students in both semesters. attributes &lt;- attributes[attributes$ids %in% ids_keep, ] send_in_sem1 &lt;- sem1_edgelist[,1] %in% ids_keep rec_in_sem1 &lt;- sem1_edgelist[,2] %in% ids_keep sem1_edgelist &lt;- sem1_edgelist[send_in_sem1 &amp; rec_in_sem1, ] send_in_sem2 &lt;- sem2_edgelist[,1] %in% ids_keep rec_in_sem2 &lt;- sem2_edgelist[,2] %in% ids_keep sem2_edgelist &lt;- sem2_edgelist[send_in_sem2 &amp; rec_in_sem2, ] Let's also create separate attribute data frames for semester 1 and semester 2. sem1_attributes &lt;- attributes[attributes$sem_id == 1, ] sem2_attributes &lt;- attributes[attributes$sem_id == 2, ] With our attribute and edgelist data frames together, we can now go ahead and construct the networks for each semester. First, we create an igraph object using the semester 1 inputs: sem1_net &lt;- graph_from_data_frame(d = sem1_edgelist, directed = T, vertices = sem1_attributes) Now using the semester 2 inputs: sem2_net &lt;- graph_from_data_frame(d = sem2_edgelist, directed = T, vertices = sem2_attributes) We will begin with a simple plot of the two networks. The goal is to get an initial picture of what the network looks like and whether there is any evidence of influence, in terms of liking the subject matter. For our plot, we want to color the nodes by how much they like the subject. We will code it such that blue means not liking the subject and red means liking the subject. We will set missing values to black. cols_sem1 &lt;- recode(sem1_attributes$like_subject, as.factor = F, &quot;1 = &#39;blue&#39;; 2 = &#39;light blue&#39;; 3 = &#39;pink&#39;; 4 =&#39;red&#39;; NA = &#39;black&#39;&quot;) cols_sem2 &lt;- recode(sem2_attributes$like_subject, as.factor = F, &quot;1 = &#39;blue&#39;; 2 = &#39;light blue&#39;; 3 = &#39;pink&#39;; 4 = &#39;red&#39;; NA = &#39;black&#39;&quot;) We want to plot both networks (semester 1 and semester 2) side by side. We will first get the coordinates of the nodes so the networks are aligned the same in the plot. We will base the layout on the first semester network. layout &lt;- layout.fruchterman.reingold(sem1_net) Here we plot the semester 1 and semester 2 networks, coloring the nodes based on how much they like the subject in that semester. par(mfrow = c(1, 2)) plot(sem1_net, vertex.label = NA, layout = layout, edge.arrow.size = .3, edge.arrow.width = 1, edge.color=&quot;light gray&quot;, vertex.frame.color = NA, vertex.color = cols_sem1) plot(sem2_net, vertex.label = NA, layout = layout, edge.arrow.size = .3, edge.arrow.width = 1, edge.color = &quot;light gray&quot;, vertex.frame.color = NA, vertex.color = cols_sem2) It looks like there may be some weak clustering by how much people like the subject. It is a bit difficult to tell from the plot if people are converging with their friend's attitudes, although it does look like students who really like the subject in semester 1 tend to have more moderate views by semester 2 (moving from red to pink). That could be a sign of peer influence, but we need to examine such hypotheses more carefully. 15.2 SIENA Model Setup We now move to more formally testing our hypotheses about peer influence. The question is whether students converge with their friends in terms of how much they like the subject matter. We will address this question while also considering the larger context in which actors are forming and breaking ties. With SIENA models, the basic idea is to model the coevolution of network ties and outcomes, simultaneously predicting changes in ties (what factors make a friendship more likely to be added/kept?) and changes in behaviors (what factors make it more likely to increase liking the subject?). In this way, we can explore the existence (or not) of peer influence while also controlling for nodal, dyadic and triadic processes that affect both the formation of ties and the outcome of interest (here interest in the subject). The analysis requires the RSiena package (Ripley et al. 2023). library(RSiena) RSiena requires that, before the model is run, the data be shaped into objects specifying their role in the analysis. Here we walk through that process on our classroom network data. Let's first get the network data in a matrix form. We will do this for both semester 1 and semester 2. sem1_matrix &lt;- as_adjacency_matrix(graph = sem1_net, sparse = F) sem2_matrix &lt;- as_adjacency_matrix(graph = sem2_net, sparse = F) And let's check that the row names are the same between the semester 1 and semester 2 matrices: table(rownames(sem1_matrix) == rownames(sem2_matrix)) ## ## TRUE ## 24 Looks good. Let's see how much change there is in terms of ties from semester 1 to semester 2. tab_change &lt;- table(sem1_matrix, sem2_matrix) tab_change ## sem2_matrix ## sem1_matrix 0 1 ## 0 429 58 ## 1 47 42 We can see that 42 ties are stable, 58 new ties were formed and 47 were dropped. We can use this information to calculate the Jaccard index, showing how similar (or stable) the network in time 1 is to the network in time 2. Formally, the Jaccard index is defined as: N11 / (N01 + N10 + N11), where N11 is the number of ties that were in both semesters (42), N01 is the number of ties that were not in time 1 but were in time 2 (58) and N10 is the number of ties that were in time 1 but not in time 2 (47). 42 / (58 + 47 + 42) ## [1] 0.2857143 We can see that .286 of the total number of ties (time 1 and time 2) were in both semesters. When the average degree is stable period to period, we want Jaccard values above .3, while values below .2 suggest there might be estimation problems. Lower Jaccard values are less problematic when average degree is increasing or decreasing. Here, we see a value slightly under .3, while the average degree has gone up somewhat between semester 1 and semester 2 (3.708 to 4.167). This suggests that we should be sensitive to possible estimation problems, but we are okay to proceed with our models. We should also keep in mind that the network itself is quite small (24 actors), which might lead to uncertain estimates. As a final preliminary check, let's also make sure that the ids in the attribute data frames are sorted in the same order. table(sem1_attributes$ids == sem2_attributes$ids) ## ## TRUE ## 24 Looks right. We can now go ahead and start to manipulate our data, putting it in a form that the RSiena package can use. As a first step, we will gather the matrices into an array. RSiena requires that the networks be put together as an n X n X p array, where n is the size of the network and p is the number of time periods. Let's first define the size of the network (based on the number of rows in the input matrix). net_size &lt;- nrow(sem1_matrix) net_size ## [1] 24 Now, we can create the array based on the two matrices, setting the dimensions based on net_size, as well as the number of time periods (2). sem12_array &lt;- array(c(sem1_matrix, sem2_matrix), dim = c(net_size, net_size, 2)) dim(sem12_array) ## [1] 24 24 2 We can see that we have a 24 x 24 x 2 array. We can now begin to set up the model, preparing certain objects for the main siena07() function. Remember that SIENA models allow for the joint modeling of network change and behavior change. We thus have two kinds of models, and thus two kinds of dependent variables. As a first step, we will construct a dependent variable based on the network, so the ties (or tie change) is the outcome of interest. The function is sienaDependent(). The main arguments are: netarray = matrix (for behaviors) or array (for network outcomes) showing behavioral or network values to predict type = type of dependent variable (e.g., behavior, oneMode) Here we include the array constructed above based on the semester 1 and semester 2 networks. The type is oneMode, indicating a one-mode network. networks &lt;- sienaDependent(netarray = sem12_array, type = &quot;oneMode&quot;) Now, we will construct the second dependent variable, based on our 'behavior' of interest, here how much they liked the subject in semester 1 and semester 2. The inputs are the over time values for liking the subject, constructed as a matrix. like_subject_matrix &lt;- as.matrix(cbind(sem1_attributes$like_subject, sem2_attributes$like_subject)) head(like_subject_matrix) ## [,1] [,2] ## [1,] 4 3 ## [2,] 2 1 ## [3,] 2 3 ## [4,] 3 1 ## [5,] 4 3 ## [6,] 3 3 We again use the sienaDependent() function to construct our dependent variable, but here netarray is set to the like_subject matrix and type is set to \"behavior\". like_subject &lt;- sienaDependent(netarray = like_subject_matrix, type = &quot;behavior&quot;) Let's also create an object that captures covariates (i.e., predictors) to be included in the model. This will typically be fixed characteristics, like gender and race/ethnicity. Here, we will include a covariate capturing if the students liked the teacher in semester 1. The function to create covariates is coCovar(). like_teacher &lt;- coCovar(sem1_attributes$like_teacher) We would want to use the varCovar() function if the variable of interest has changing values over time. The input is an n by M - 1 matrix, where n is the number of nodes in the network and M is the number of time points (the final observation is excluded). It is only appropriate to use varCovar() if there are at least 3 periods of data. Now, we will put the constructed objects together using a sienaDataCreate() function. This siena object will be one of the main inputs into the function to run the model. net_behavior_covar &lt;- sienaDataCreate(networks, like_subject, like_teacher) net_behavior_covar ## Dependent variables: networks, like_subject ## Number of observations: 2 ## ## Nodeset Actors ## Number of nodes 24 ## ## Dependent variable networks ## Type oneMode ## Observations 2 ## Nodeset Actors ## Densities 0.16 0.18 ## ## Dependent variable like_subject ## Type behavior ## Observations 2 ## Nodeset Actors ## Range 1 - 4 ## ## Constant covariates: like_teacher We see basic information about our network, like density and number of nodes, as well as useful information about the behavior of interest (such as the range of values). 15.3 Model Specification We now take our constructed siena object and specify the model we want to use to predict changes in ties and changes in behaviors, here about how much they like the subject. The model is based on an evaluation function, where actors evaluate adding/dropping ties and changing behavior based on the specified model. Positive coefficients suggest the change is more likely than chance expectations, while negative coefficients suggest the change is less likely. The coefficients are estimated based on the observed tendencies seen in the input data, in terms of network and behavioral change. For example, with the network model, if actors select friends with similar behaviors as themselves, we would expect a positive coefficient on the selection term. Or, for the behavior model, if actors tend to converge on behaviors with their friends, we would expect a positive peer influence term. RSiena works by 'building up' the model. We first get the base terms using a getEffects() function and then use an includeEffects() function to add terms to the base model. Let's start with getting the base model, with the main input as the siena object created above. siena_effects &lt;- getEffects(net_behavior_covar) siena_effects ## name effectName include fix test initialValue parm ## 1 networks basic rate parameter networks TRUE FALSE FALSE 9.12260 0 ## 2 networks outdegree (density) TRUE FALSE FALSE -0.71940 0 ## 3 networks reciprocity TRUE FALSE FALSE 0.00000 0 ## 4 like_subject rate like_subject period 1 TRUE FALSE FALSE 0.81028 0 ## 5 like_subject like_subject linear shape TRUE FALSE FALSE -0.10773 0 ## 6 like_subject like_subject quadratic shape TRUE FALSE FALSE 0.00000 0 The first thing to note is that there are two kinds of model terms here, one predicting how the network changes over time (name = networks) and one predicting how much they like the subject changes over time (name = like_subject). We can see that the network model, by default, includes a term for density, tie change (the rate of change between T1 and T2) and reciprocity. The default model for attitudes about the subject are the base rate of change for liking the subject, and linear and quadratic terms, showing tendencies for actors with too high (or low) values (compared to the mean) to increase/decrease their preferences. Now, let's start adding terms to the model. 15.3.1 Model Specification: Behavioral Model First, we will focus on the behavioral model, predicting how like_subject changes across semesters. The function to add terms is includeEffects(). The main arguments are: myeff = siena object of effects name of term to include interaction1 = siena object from which effects are being calculated name = name of dependent variable for which effects are being included First, let's include a peer influence term, asking if people tend to converge on liking the subject over time. Here we add an avSim term, although there are other options to capture peer influence. avSim is the average similarity effects, showing if actors tend to have values similar to the average of their network alters. We set interaction1 = \"networks\" as the network data must be used to construct the effect and we set name = \"like_subject\" as the outcome of interest is liking the subject between semester 1 and 2. siena_effects &lt;- includeEffects(siena_effects, avSim, interaction1 = &quot;networks&quot;, name = &quot;like_subject&quot;) Now, let's include a term that captures if people with higher indegree tend to decrease/increase their liking of the subject (compared to people with lower indegree). The base term is called indeg, and we again have the same inputs for interaction1 and name as above. siena_effects &lt;- includeEffects(siena_effects, indeg, interaction1 = &quot;networks&quot;, name = &quot;like_subject&quot;) It is useful to control for other individual-level characteristics that may affect the outcome of interest, here liking the subject. In this case, let's control for how much the student likes the teacher (like_teacher). Thus, we may think that students will increase their level of liking the subject if they like the teacher (measured in semester 1 and held fixed). The term is called effFrom. The interaction1 input is now \"like_teacher\" as we are interested using that variable as the predictor; name is still is equal to \"like_subject\" as this is the outcome of interest. siena_effects &lt;- includeEffects(siena_effects, effFrom, interaction1 = &quot;like_teacher&quot;, name = &quot;like_subject&quot;) 15.3.2 Model Specification: Network Model Here, we will add terms to the model that predicts the evolution of friendship ties as the outcome of interest, where students form and drop ties between semester 1 and 2. There are a number of terms we could add to the model but let's focus on 4 common ones. First, we often want to know if actors form (and keep) ties based on sharing some attitude or behavior of interest. In this case, we want to know if actors select friends with similar attitudes about the subject. We can think of this as selection, allowing us to include terms for both influence and selection in the model. We set interaction1 to \"like_subject\" and name to \"networks\", as the outcome of interest is the ties, rather than liking the subject (as above). The term is simX. siena_effects &lt;- includeEffects(siena_effects, simX, interaction1 = &quot;like_subject&quot;, name = &quot;networks&quot;) Let's also add degree effects, showing if people with certain attributes (here liking the subject) send out more/less ties. The term is egoX and the rest is the same. This captures the effect of liking the subject on outdegree. siena_effects &lt;- includeEffects(siena_effects, egoX, interaction1 = &quot;like_subject&quot;, name = &quot;networks&quot;) Here we add the analogous term for ties coming in, showing if people with certain attributes (here liking the subject) receive more/less ties. The term is altX and the rest is the same. This captures the effect of liking the subject on indegree. siena_effects &lt;- includeEffects(siena_effects, altX, interaction1 = &quot;like_subject&quot;, name = &quot;networks&quot;) Finally, let's add a term for transitivity, or local clustering, in tie formation. transTrip captures the number of transitive patterns where i is tied to h and j and h is tied j. siena_effects &lt;- includeEffects(siena_effects, transTrip, name = &quot;networks&quot;) siena_effects ## name effectName include fix test initialValue parm ## 1 networks basic rate parameter networks TRUE FALSE FALSE 9.12260 0 ## 2 networks outdegree (density) TRUE FALSE FALSE -0.71940 0 ## 3 networks reciprocity TRUE FALSE FALSE 0.00000 0 ## 4 networks transitive triplets TRUE FALSE FALSE 0.00000 0 ## 5 networks like_subject alter TRUE FALSE FALSE 0.00000 0 ## 6 networks like_subject ego TRUE FALSE FALSE 0.00000 0 ## 7 networks like_subject similarity TRUE FALSE FALSE 0.00000 0 ## 8 like_subject rate like_subject period 1 TRUE FALSE FALSE 0.81028 0 ## 9 like_subject like_subject linear shape TRUE FALSE FALSE -0.10773 0 ## 10 like_subject like_subject quadratic shape TRUE FALSE FALSE 0.00000 0 ## 11 like_subject like_subject average similarity TRUE FALSE FALSE 0.00000 0 ## 12 like_subject like_subject indegree TRUE FALSE FALSE 0.00000 0 ## 13 like_subject like_subject: effect from like_teacher TRUE FALSE FALSE 0.00000 0 We could include a much more complicated set of terms to the model. This is often desirable (and necessary), to answer the question of interest and to obtain a reasonable fit. See the RSiena manual for a full list and discussion of possible terms. 15.4 Model Estimation We will now go ahead and estimate our model. Before we can estimate the model we need to create an object of input specifications using the sienaAlgorithmCreate() function. There are a number of possible inputs but here we will leave most things at the defaults. We will include a MaxDegree argument, telling the model what is the max degree possible in the network. We set MaxDegree to 5 as students were restricted to naming only up to 5 friends in the classroom. We also add a seed argument to make it easier to replicate. input_options &lt;- sienaAlgorithmCreate(projname = &quot;class_model&quot;, MaxDegree = c(networks = 5), seed = 30000) Now, we are ready to estimate the model using the siena07() function. The main arguments are: x = object of input specifications constructed using sienaAlgorithmCreate() data = main siena object constructed from sienaDataCreate() function effects = model effects specified using includeEffects() function It is also possible to specify arguments to speed up the estimation. In particular, we can tell R to use multiple processors in the estimation. The main added arguments are: useCluster = T/F; should we use multiple clusters? nbrNodes = number of processors to use Here we estimate the model using 2 processors and the inputs specified above, along with the siena object and the effects object. mod1 &lt;- siena07(x = input_options, data = net_behavior_covar, effects = siena_effects, useCluster = TRUE, nbrNodes = 2) Let's take a look at the results. mod1 ## Estimates, standard errors and convergence t-ratios ## ## Estimate Standard Convergence ## Error t-ratio ## Network Dynamics ## 1. rate basic rate parameter networks 9.1907 ( 1.6434 ) -0.0663 ## 2. eval outdegree (density) -0.1832 ( 0.7413 ) -0.0102 ## 3. eval reciprocity 0.3977 ( 0.2482 ) -0.0088 ## 4. eval transitive triplets 0.1643 ( 0.1151 ) -0.0103 ## 5. eval like_subject alter -0.1678 ( 0.2467 ) 0.0013 ## 6. eval like_subject ego -1.0923 ( 1.3832 ) -0.0409 ## 7. eval like_subject similarity -0.0888 ( 1.4954 ) -0.0293 ## ## Behavior Dynamics ## 8. rate rate like_subject period 1 2.3505 ( 1.3174 ) -0.0540 ## 9. eval like_subject linear shape -0.4566 ( 1.7687 ) -0.0074 ## 10. eval like_subject quadratic shape -0.3407 ( 0.9653 ) -0.0990 ## 11. eval like_subject average similarity 4.9813 ( 7.0186 ) 0.0161 ## 12. eval like_subject indegree 0.0885 ( 0.4135 ) -0.0100 ## 13. eval like_subject: effect from like_teacher 0.2683 ( 0.8997 ) -0.0566 ## ## Overall maximum convergence ratio: 0.1859 ## ## ## Degrees constrained to maximum values: ## networks : 5 ## ## ## Total of 2494 iteration steps. First, let's check if the model converged. The overall convergence ratio is under .25, which is considered acceptable. We can also look at the convergence ratio of each estimated parameter in the model. Here values above .1 (taking the absolute value) suggest problems with the estimation. In this case the convergence looks good and we can move ahead looking at our model results. If the model did not converge, we would likely need to rerun the model under different specifications. For example, adding/dropping terms, starting the model at better coefficients (i.e., the coefficients estimated from the previous run) and allowing for a longer run time. The main items of interest in our results are the estimates and the standard errors. Note that by dividing the estimates by the standard errors we can do traditional t-tests on the coefficients. For example, we can calculate the t-statistic for reciprocity (0.3977 / 0.2482) = 1.602, suggestive of a positive, but not significant, effect of reciprocity on the formation and keeping of ties. The average similarity effect captures peer influence, showing if the focal actor tends to converge with their peers on liking the subject. We can see that that the standard errors are quite high, as is the estimated peer effect. So, while there may be peer influence, the estimate is very uncertain. The network only has 24 actors, so our behavioral model might have too many terms relative to the number of actors, leading to inflated standard errors. We might try a simpler model with fewer terms in the behavioral portion of the model. Here, we will drop indegree and the main effect of liking the teacher (setting include to FALSE). siena_effects &lt;- includeEffects(siena_effects, indeg, interaction1 = &quot;networks&quot;, name = &quot;like_subject&quot;, include = FALSE) siena_effects &lt;- includeEffects(siena_effects, effFrom, interaction1 = &quot;like_teacher&quot;, name = &quot;like_subject&quot;, include = FALSE) We will also allow the model to run longer by increasing the number of iterations used in phase 3 of the estimation routine. We use a n3 argument here. input_options &lt;- sienaAlgorithmCreate(projname = &quot;class_model&quot;, MaxDegree = c(networks = 5), n3 = 3000, seed = 30000) Let’s estimate the model. We will add returnDeps = TRUE to tell siena to return the simulated networks, which will be useful when doing model fit below. mod2 &lt;- siena07(x = input_options, data = net_behavior_covar, effects = siena_effects, returnDeps = TRUE, useCluster = TRUE, nbrNodes = 2) mod2 ## Estimates, standard errors and convergence t-ratios ## ## Estimate Standard Convergence ## Error t-ratio ## Network Dynamics ## 1. rate basic rate parameter networks 9.2289 ( 1.7967 ) -0.0102 ## 2. eval outdegree (density) -0.1704 ( 0.6859 ) 0.0211 ## 3. eval reciprocity 0.3990 ( 0.2431 ) 0.0122 ## 4. eval transitive triplets 0.1553 ( 0.1171 ) 0.0076 ## 5. eval like_subject alter -0.1578 ( 0.2777 ) 0.0163 ## 6. eval like_subject ego -1.0739 ( 1.2577 ) -0.0103 ## 7. eval like_subject similarity -0.0108 ( 1.6674 ) -0.0212 ## ## Behavior Dynamics ## 8. rate rate like_subject period 1 2.2906 ( 1.2435 ) -0.0138 ## 9. eval like_subject linear shape -0.0664 ( 0.3862 ) -0.0049 ## 10. eval like_subject quadratic shape -0.2609 ( 0.8606 ) -0.0219 ## 11. eval like_subject average similarity 5.3366 ( 6.8374 ) 0.0535 ## ## Overall maximum convergence ratio: 0.0777 ## ## ## Degrees constrained to maximum values: ## networks : 5 ## ## ## Total of 4412 iteration steps. As we can see, the standard errors are still quite high and the estimates are pretty similar, even with dropping terms and increasing the number of iterations. This suggests that the limitations of the data are the likely culprit. The network is rather small (only 24 actors) and only has 2 waves, making it difficult to parse the effect of peer influence. Practically, a researcher may need to employ data on a larger network, with, ideally, more time points. We consider an example on a larger network below that has 3 waves. Before we move to that second example, let's continue with our classroom network, seeing how we can interpret the results and check model fit. 15.5 Interpretation In interpreting the coefficients, it is important to consider the form of the evaluation function specified in the model. It is also useful to distinguish between the network dynamics model and the behavioral dynamics model. We will start by interpreting the network dynamics model. 15.5.1 Interpreting Network Model The coefficients can be understood as increasing/decreasing the ratio of the log-probability that i adds (or keeps) a tie to j compared to i adding (or keeping) a tie to h, where j and h are different nodes in the network with different attributes and structural positions relative to i. For example, we see a coefficient for reciprocity of 0.399. This would suggest that the odds of a tie from i being sent to j is exp(0.399) times higher than being sent to h, assuming that j-&gt;i exists but h-&gt;i does not. We can interpret each term in an analogous fashion, although some terms offer more complicated functional forms and are thus harder to interpret. For example, some attributes, like liking the subject may be in multiple terms (indegree, outdegree, selection), making it potentially important to consider all terms together when interpreting effects. 15.5.2 Interpreting Behavioral Model We now turn to the behavioral model, where we are particularly interested in peer influence effects. Note that our average similarity term had very high standard errors and we should be hesitant in pushing the interpretation of that coefficient too far, even as we use that coefficient in our calculations below. More generally, peer influence effects can be a little tricky to interpret as we need to incorporate the proper functional form used in the evaluation function and we need to include multiple terms in interpreting the effect of peer influence (in particular the linear and quadratic shape terms along with the average similarity term). Our goal here is to calculate a table of peer influence that shows how strongly students are drawn to having the same values as their peers. We will calculate the evaluation function for different students with friends who are more/less similar to them. This table of peer influence effects will aid in interpretation. It is important to note that the function used to calculate the peer effects will be different, depending on the exact term used in the model. Here we will consider the evaluation function related to average similarity. A researcher can calculate the influence table on their own (see RSiena Manual) but there are now functions available from the SIENA development team to make this task much easier. As of the writing of this tutorial, these functions must be read into R separately from the base RSiena package. This can be downloaded from: https://www.stats.ox.ac.uk/~snijders/siena/InfluenceTables.r. Once this file is in the working directory, we can source it into R as follows: source(file = &quot;InfluenceTables.r&quot;) The main function we will make use of is influenceMatrix(). The arguments are: x = estimated model xd = siena network/behavior object netname = network relation name behname = behavioral name levls = levels possible for behavioral outcome influence_matrix &lt;- influenceMatrix(x = mod2, xd = net_behavior_covar, netname = &quot;networks&quot;, behname = &quot;like_subject&quot;, levls = 1:4) influence_matrix ## 1 2 3 4 ## 1 0.8824755 -0.3803648 -2.1651001 -4.4717305 ## 2 -0.8963811 1.3984919 -0.3862435 -2.6928738 ## 3 -2.6752378 -0.3803648 1.3926132 -0.9140172 ## 4 -4.4540944 -2.1592214 -0.3862435 0.8648395 The columns correspond to the potential behavior of ego (values 1 through 4 corresponding to how much they like the subject). The rows correspond to a given set of behaviors of the alters. The values in the matrix contain the contribution to the evaluation function, showing the relative attractiveness of ego's behavior (given a set of alter behavior, on the row). Looking at the first row, we see that for an ego with all friends who have a value of 1, the most attractive option is also to have a value of 1. Compare the positive value in the first column, 0.8825, to the negative value in column 4, -4.4717 (i.e., taking a value of 4 when everyone else has a value of 1). We can use this table to see how much having all of one's friends like the subject increases the odds of a student increasing how much they like the subject. For example, if all of ego's friends had a value of 2 (row 2) and ego had a value of 1, the odds of ending up at a 2 is 9.923 times higher than staying a 1: exp(1.3985) / exp(-0.8964) = 9.923. This is evidence of peer influence, but we must, again, remember that our estimated parameter was quite uncertain, and thus the estimates in this table are quite uncertain. 15.6 Checking Model Fit Finally, it is a good idea (like with ERGM) to check the fit of the model. Here, we ask if the estimated model can reproduce key features of the network and behaviors. The basic idea is to simulate from the model and then check if the networks and behavioral distributions based on the model match the observed data. Let's first see if the model is reproducing the indegree distribution. The function is sienaGOF(). The arguments are: sienaFitObject = model, auxiliaryFunction = function to calculate varName = network relation or behavior of interest gof1 &lt;- sienaGOF(sienaFitObject = mod2, auxiliaryFunction = IndegreeDistribution, verbose = TRUE, join = TRUE, varName = &quot;networks&quot;) And now let's plot the results. plot(gof1) We are looking for any large deviations between the observed and simulated networks. Low p-values would indicate a potential problem. The fit looks okay here. And now we look at the triad census. gof2 &lt;- sienaGOF(sienaFitObject = mod2, auxiliaryFunction = TriadCensus, verbose = TRUE, join = TRUE, varName = &quot;networks&quot;) We will set center and scale to TRUE to make a better-looking plot: plot(gof2, center = TRUE, scale = TRUE) Again, the fit looks okay. Here we look at the overall fit of the behavior distribution. gof_behaviour &lt;- sienaGOF(sienaFitObject = mod2, auxiliaryFunction = BehaviorDistribution, varName = &quot;like_subject&quot;) plot(gof_behaviour) The fit looks okay. Note that RSiena makes it possible to add other, user-defined statistics that are not currently defined in the RSiena package (e.g., we may want to look at fit for distance). See ?sienaGOF-auxiliary for examples. 15.7 Example with 3 Time Periods We now turn to a short example with 3 time periods, to see how the mechanics and interpretation change in cases where there are more than 2 time periods. The network is also larger than in our first example. In this case, we will make use of one of the example data sets from the siena website, the glasgow friendship data: https://www.stats.ox.ac.uk/~snijders/siena/Glasgow_data.htm. The data are based on students in a secondary school in Glasgow. The data contain friendship information, demographic information and substance use data for three waves. Here we focus on alcohol use. Let's read in the data. url4 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/Glasgow-friendship.RData&quot; load(url(description = url4)) url5 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/Glasgow-substances.RData&quot; load(url(description = url5)) url6 &lt;- &quot;https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/Glasgow-demographic.RData&quot; load(url(description = url6)) Let's get the objects together to run the model. The friendship data is housed in friendship.1, friendship.2 and friendship.3. Each is a matrix showing if there is a tie between i and j, where 0 = no tie; 1 = best friend; 2 = friend; 10 = structural absence of tie (as at least one student in the ij pair was not in the school for that wave). There are also some NAs in the matrices, indicating that there is no information on whether a tie exists between i and j. Let's look at a piece of the matrix: friendship.1[1:10, 1:10] ## s001 s002 s003 s004 s005 s006 s007 s008 s009 s010 ## s001 0 0 0 0 0 0 0 0 0 0 ## s002 0 0 0 0 0 0 0 0 0 0 ## s003 0 0 0 0 0 0 0 0 0 0 ## s004 0 0 0 0 0 0 0 2 0 0 ## s005 0 0 0 0 0 2 0 0 0 2 ## s006 0 0 0 0 0 0 0 0 0 0 ## s007 0 0 0 2 0 0 0 0 0 0 ## s008 0 0 0 2 0 0 2 0 0 0 ## s009 0 0 0 0 0 0 0 0 0 0 ## s010 0 0 0 0 2 0 0 0 0 0 The alcohol data is housed in alcohol. head(alcohol) ## t1 t2 t3 ## s001 3 1 3 ## s002 NA NA NA ## s003 2 2 2 ## s004 2 2 3 ## s005 2 3 3 ## s006 3 2 5 The data show the value on alcohol use in the three waves of data: 1 = none; 2 = once or twice a year; 3 = once a month; 4 = once a week; 5 = more than once a week. There is a moderate amount of missing data in a given wave, around 13% missing on key variables, like alcohol use (excluding students who were not in the school for that wave, having entered at a later wave or already exiting the school; this is about 4% of students). It is generally considered problematic if one has more than 20% missing data. See RSiena manual for more details. For this analysis, we will keep all students who were ever in the school, thus allowing students to join and leave the network through time. This is handled by the structural zeros in the matrices, showing that no tie was possible from i to j and j to i because (at least) one of the students was not in the school for that wave. This is analogous to how we treated the longitudinal data in the two-mode ERGM tutorial. In RSiena, a value of 10 (in the input matrix) is a special value, indicating a structural zero is present. This is already properly coded in the friendship matrices. The model will thus be conditioned on the structural zeros present in the data. Similarly, we will not exclude cases with missing data, keeping the missing values as NAs (these values will be treated as non-informative during estimation). Let's begin by turning all of the 2s into 1s, so the matrix is just friend or no friend (i.e., we no longer make a distinction between best friend and friend). Values of 10 (structural zeros) will be kept at 10. friendship.1[friendship.1 == 2] &lt;- 1 friendship.2[friendship.2 == 2] &lt;- 1 friendship.3[friendship.3 == 2] &lt;- 1 Here we take those matrices and put them into a single array. net_size_ex2 &lt;- nrow(friendship.1) net_array &lt;- array(c(friendship.1, friendship.2, friendship.3), dim = c(net_size_ex2, net_size_ex2, 3)) dim(net_array) ## [1] 160 160 3 We have a 160 x 160 X 3 array. We can now create a siena object based on the array of network data. friendship &lt;- sienaDependent(netarray = net_array, type = &quot;oneMode&quot;) friendship ## Type oneMode ## Observations 3 ## Nodeset Actors (160 elements) Now we deal with the behavioral outcome of interest, alcohol use. Here we create the behavioral siena object. alcohol_depvar &lt;- sienaDependent(netarray = alcohol, type = &quot;behavior&quot;) alcohol_depvar ## Type behavior ## Observations 3 ## Nodeset Actors (160 elements) Let's also create some covariates to be included in the model. Here we create a variable for gender. Gender is housed as sex.F. 1 = boys; 2 = girls head(sex.F) ## s001 s002 s003 s004 s005 s006 ## 2 1 2 1 1 1 Let’s create a covariate object: gender &lt;- coCovar(sex.F) Here we put the constructed objects together using a sienaDataCreate() function: friend_behavior_covar &lt;- sienaDataCreate(friendship, alcohol_depvar, gender) friend_behavior_covar ## Dependent variables: friendship, alcohol_depvar ## Number of observations: 3 ## ## Nodeset Actors ## Number of nodes 160 ## ## Dependent variable friendship ## Type oneMode ## Observations 3 ## Nodeset Actors ## Densities 0.023 0.021 0.02 ## ## Dependent variable alcohol_depvar ## Type behavior ## Observations 3 ## Nodeset Actors ## Range 1 - 5 ## ## Constant covariates: gender 15.7.1 Model Specification and Estimation Now, we specify our model of interest. We want to run a similar model as above, where the focus is on selection/influence effects, here for drinking behavior. We start with the base model. siena_effects_3wave_example &lt;- getEffects(friend_behavior_covar) siena_effects_3wave_example ## name effectName include fix test initialValue parm ## 1 friendship constant friendship rate (period 1) TRUE FALSE FALSE 7.41692 0 ## 2 friendship constant friendship rate (period 2) TRUE FALSE FALSE 6.16651 0 ## 3 friendship outdegree (density) TRUE FALSE FALSE -1.66821 0 ## 4 friendship reciprocity TRUE FALSE FALSE 0.00000 0 ## 5 alcohol_depvar rate alcohol_depvar (period 1) TRUE FALSE FALSE 0.76577 0 ## 6 alcohol_depvar rate alcohol_depvar (period 2) TRUE FALSE FALSE 1.15233 0 ## 7 alcohol_depvar alcohol_depvar linear shape TRUE FALSE FALSE 0.35525 0 ## 8 alcohol_depvar alcohol_depvar quadratic shape TRUE FALSE FALSE 0.00000 0 Note that we now have rate parameters (controlling for change in ties over time) for T1 to T2 and T2 to T3. Now we add a term for peer influence, as before, using avSim (do people tend to move towards their friends drinking behavior?) siena_effects_3wave_example &lt;- includeEffects(siena_effects_3wave_example, avSim, interaction1 = &quot;friendship&quot;, name = &quot;alcohol_depvar&quot;) Also, let's include a term that controls for the main effect of gender on drinking behavior. siena_effects_3wave_example &lt;- includeEffects(siena_effects_3wave_example, effFrom, interaction1 = &quot;gender&quot;, name = &quot;alcohol_depvar&quot;) Now, we include terms that predict friendship ties as the outcome of interest, where students form and drop ties between T1 and T2 and T2 and T3. We will include outdegree and indegree effects for drinking (do people who drink send out/receive more ties?). siena_effects_3wave_example &lt;- includeEffects(siena_effects_3wave_example, egoX, interaction1 = &quot;alcohol_depvar&quot;, name = &quot;friendship&quot;) siena_effects_3wave_example &lt;- includeEffects(siena_effects_3wave_example, altX, interaction1 = &quot;alcohol_depvar&quot;, name = &quot;friendship&quot;) Now we include a term for selection on drinking behavior (do people tend to form and maintain ties with people with similar drinking behavior?) siena_effects_3wave_example &lt;- includeEffects(siena_effects_3wave_example, simX, interaction1 = &quot;alcohol_depvar&quot;, name = &quot;friendship&quot;) And now we add a term capturing the tendency for transitive closure in tie formation. siena_effects_3wave_example &lt;- includeEffects(siena_effects_3wave_example, transTrip, name = &quot;friendship&quot;) Let's take a look at the model effects: siena_effects_3wave_example ## name effectName include fix test initialValue parm ## 1 friendship constant friendship rate (period 1) TRUE FALSE FALSE 7.41692 0 ## 2 friendship constant friendship rate (period 2) TRUE FALSE FALSE 6.16651 0 ## 3 friendship outdegree (density) TRUE FALSE FALSE -1.66821 0 ## 4 friendship reciprocity TRUE FALSE FALSE 0.00000 0 ## 5 friendship transitive triplets TRUE FALSE FALSE 0.00000 0 ## 6 friendship alcohol_depvar alter TRUE FALSE FALSE 0.00000 0 ## 7 friendship alcohol_depvar ego TRUE FALSE FALSE 0.00000 0 ## 8 friendship alcohol_depvar similarity TRUE FALSE FALSE 0.00000 0 ## 9 alcohol_depvar rate alcohol_depvar (period 1) TRUE FALSE FALSE 0.76577 0 ## 10 alcohol_depvar rate alcohol_depvar (period 2) TRUE FALSE FALSE 1.15233 0 ## 11 alcohol_depvar alcohol_depvar linear shape TRUE FALSE FALSE 0.35525 0 ## 12 alcohol_depvar alcohol_depvar quadratic shape TRUE FALSE FALSE 0.00000 0 ## 13 alcohol_depvar alcohol_depvar average similarity TRUE FALSE FALSE 0.00000 0 ## 14 alcohol_depvar alcohol_depvar: effect from gender TRUE FALSE FALSE 0.00000 0 Here we set up our control inputs. We set the max degree to 6 as students were limited to naming 6 friends. input_options_3wave &lt;- sienaAlgorithmCreate(MaxDegree = c(friendship = 6), n3 = 3000, seed = 5000) And now we estimate the model. mod1_3wave_example &lt;- siena07(input_options_3wave, data = friend_behavior_covar, effects = siena_effects_3wave_example, returnDeps = TRUE, initC = TRUE, useCluster = TRUE, nbrNodes = 2) mod1_3wave_example ## Estimates, standard errors and convergence t-ratios ## ## Estimate Standard Convergence ## Error t-ratio ## Network Dynamics ## 1. rate constant friendship rate (period 1) 15.2825 ( 1.7375 ) -0.0032 ## 2. rate constant friendship rate (period 2) 11.8442 ( 1.2779 ) -0.0213 ## 3. eval outdegree (density) -2.5172 ( 0.0492 ) -0.0171 ## 4. eval reciprocity 2.0224 ( 0.0939 ) -0.0151 ## 5. eval transitive triplets 0.5646 ( 0.0232 ) -0.0366 ## 6. eval alcohol_depvar alter -0.0407 ( 0.0612 ) 0.0092 ## 7. eval alcohol_depvar ego 0.0349 ( 0.0651 ) 0.0384 ## 8. eval alcohol_depvar similarity 0.7936 ( 0.3594 ) -0.0434 ## ## Behavior Dynamics ## 9. rate rate alcohol_depvar (period 1) 1.6392 ( 0.2958 ) 0.0331 ## 10. rate rate alcohol_depvar (period 2) 2.7722 ( 0.5513 ) 0.0551 ## 11. eval alcohol_depvar linear shape 0.4561 ( 0.1467 ) 0.0343 ## 12. eval alcohol_depvar quadratic shape 0.0623 ( 0.0997 ) 0.0315 ## 13. eval alcohol_depvar average similarity 7.1645 ( 2.2501 ) -0.0708 ## 14. eval alcohol_depvar: effect from gender 0.0497 ( 0.2012 ) -0.0220 ## ## Overall maximum convergence ratio: 0.1194 ## ## ## Degrees constrained to maximum values: ## friendship : 6 ## ## ## Total of 5224 iteration steps. The first thing we see is that the model appears to have converged (looking at the overall convergence ratio and the t-ratios for each term). Looking at the estimated model, we can see that a model with three time periods looks better behaved. The standard errors tend to be lower, most noticeably for the peer influence (average similarity) coefficient. The estimate on average similarity (7.1645) is higher than what we saw above and the standard error is over 3 times smaller. The effect for peer influence is clearly significant here (with a t-statistic of: 3.184, 7.1645 / 2.2501). We also see that reciprocity is quite strong, as is the tendency for transitive relations. We also see that there is selection on alcohol use (a coefficient of 0.7936 and a standard error of 0.3594 for similarity), suggesting that not only do students tend to converge to the behaviors of their peers, they also tend to seek out those with similar drinking behavior. Again, we can use the influenceMatrix() function to get the full picture of how influence operates in the network in terms of alcohol use. influence_matrix_3wave &lt;- influenceMatrix(x = mod1_3wave_example, xd = friend_behavior_covar, netname = &quot;friendship&quot;, behname = &quot;alcohol_depvar&quot;, levls = 1:5) influence_matrix_3wave ## 1 2 3 4 5 ## 1 1.3703344 -0.1540429 -1.5537744 -2.8288600 -3.9792997 ## 2 -0.4208021 1.6370936 0.2373621 -1.0377235 -2.1881631 ## 3 -2.2119386 -0.1540429 2.0284986 0.7534131 -0.3970266 ## 4 -4.0030751 -1.9451794 0.2373621 2.5445496 1.3941099 ## 5 -5.7942116 -3.7363160 -1.5537744 0.7534131 3.1852464 We can also take a quick look at goodness of fit, here in terms of the indegree distribution. gof1 &lt;- sienaGOF(mod1_3wave_example, IndegreeDistribution, verbose = TRUE, join = TRUE, varName = &quot;friendship&quot;) plot(gof1) Here we look at the overall fit of the behavior distribution. gof_behaviour &lt;- sienaGOF(mod1_3wave_example, BehaviorDistribution, varName = &quot;alcohol_depvar&quot;) plot(gof_behaviour) 15.7.2 Time Heterogeneity Finally, when we have more than two time periods, it is important to test for time heterogeneity, asking if the estimates are different from T1 to T2 compared to T2 and T3 (in the case of three waves). Let's test whether the estimates differ from T1 to T2 vs. T2 to T3 using the sienaTimeTest() function. The input is the estimated model. timetest &lt;- sienaTimeTest(mod1_3wave_example) summary(timetest) ## Joint significance test of time heterogeneity: ## chi-squared = 10.25, d.f. = 10, p= 0.4185, ## where H0: The following parameters are zero: ## ( 1) (*)Dummy2:outdegree (density) ## ( 2) (*)Dummy2:reciprocity ## ( 3) (*)Dummy2:transitive triplets ## ( 4) (*)Dummy2:alcohol_depvar alter ## ( 5) (*)Dummy2:alcohol_depvar ego ## ( 6) (*)Dummy2:alcohol_depvar similarity ## ( 7) (*)Dummy2:alcohol_depvar linear shape ## ( 8) (*)Dummy2:alcohol_depvar quadratic shape ## ( 9) (*)Dummy2:alcohol_depvar average similarity ## (10) (*)Dummy2:alcohol_depvar: effect from gender ## ## Individual significance tests and one-step estimators: ## Initial Est. One Step Est. p-Value ## outdegree (density) -2.5172 -2.4831 0.0000 ## reciprocity 2.0224 1.9256 0.0000 ## transitive triplets 0.5646 0.5222 0.0000 ## alcohol_depvar alter -0.0407 -0.0021 0.5060 ## alcohol_depvar ego 0.0349 -0.0385 0.5920 ## alcohol_depvar similarity 0.7936 0.7868 0.0270 ## alcohol_depvar linear shape 0.4561 0.4929 0.0020 ## alcohol_depvar quadratic shape 0.0623 0.1869 0.5320 ## alcohol_depvar average similarity 7.1645 8.8445 0.0010 ## alcohol_depvar: effect from gender 0.0497 -0.0320 0.8050 ## (*)Dummy2:outdegree (density) 0.0000 -0.0936 0.1250 ## (*)Dummy2:reciprocity 0.0000 0.1965 0.0200 ## (*)Dummy2:transitive triplets 0.0000 0.1061 0.0080 ## (*)Dummy2:alcohol_depvar alter 0.0000 -0.0836 0.8830 ## (*)Dummy2:alcohol_depvar ego 0.0000 0.1449 0.3850 ## (*)Dummy2:alcohol_depvar similarity 0.0000 -0.0245 0.7640 ## (*)Dummy2:alcohol_depvar linear shape 0.0000 0.0097 0.9560 ## (*)Dummy2:alcohol_depvar quadratic shape 0.0000 -0.2384 0.4400 ## (*)Dummy2:alcohol_depvar average similarity 0.0000 -2.8301 0.9200 ## (*)Dummy2:alcohol_depvar: effect from gender 0.0000 0.0721 0.9670 ## ## Effect-wise joint significance tests ## (i.e. each effect across all dummies): ## chi-sq. df p-value ## outdegree (density) 2.36 1 0.124 ## reciprocity 5.41 1 0.020 ## transitive triplets 7.07 1 0.008 ## alcohol_depvar alter 0.02 1 0.888 ## alcohol_depvar ego 0.75 1 0.386 ## alcohol_depvar similarity 0.09 1 0.764 ## alcohol_depvar linear shape 0.00 1 1.000 ## alcohol_depvar quadratic shape 0.60 1 0.439 ## alcohol_depvar average similarity 0.01 1 0.920 ## alcohol_depvar: effect from gender 0.00 1 1.000 ## ## Period-wise joint significance tests ## (i.e. each period across all parameters): ## chi-sq. df p-value ## Period 1 10.25 10 0.419 ## Period 2 10.25 10 0.419 ## ## Use the following indices for plotting: ## (1) outdegree (density) ## (2) reciprocity ## (3) transitive triplets ## (4) alcohol_depvar alter ## (5) alcohol_depvar ego ## (6) alcohol_depvar similarity ## (7) alcohol_depvar linear shape ## (8) alcohol_depvar quadratic shape ## (9) alcohol_depvar average similarity ## (10) alcohol_depvar: effect from gender ## ## If you would like to fit time dummies to your model, ## use the includeTimeDummy function. ## Type &quot;?sienaTimeTest&quot; for more information on this output. The main test we are interested in is the 'joint significance' test. The null hypothesis is that the effects are the same across the waves. A p value &lt; .05 would offer evidence to reject that null (meaning the effects may differ across waves). Here there is not strong evidence of time differences. If we had found significant differences we may want to include terms that allow the coefficients (at least some of them) to vary by time. In particular, we would want to look at the significance tests on the specific coefficients interacted with time; here we would focus on the series of coefficients starting with 'Dummy 2', looking at which cases there is a significant t-test (suggesting that the coefficient for T2 - T3, for example, is different than the coefficient in T1 - T2). We would then use an includeTimeDummy() function to include the desired terms, allowing those coefficients to vary across time. For example, let's run through a quick example where we allow the coefficients on density and transitive triplets to vary between the first period of change (T1 to T2) and the second (T2 to T3). siena_effects_3wave_example &lt;- includeTimeDummy(siena_effects_3wave_example, density, transTrip, timeDummy = &quot;2&quot;) siena_effects_3wave_example ## name effectName include fix test initialValue parm timeDummy ## 1 friendship constant friendship rate (period 1) TRUE FALSE FALSE 7.41692 0 , ## 2 friendship constant friendship rate (period 2) TRUE FALSE FALSE 6.16651 0 , ## 3 friendship outdegree (density) TRUE FALSE FALSE -1.66821 0 2 ## 4 friendship reciprocity TRUE FALSE FALSE 0.00000 0 , ## 5 friendship transitive triplets TRUE FALSE FALSE 0.00000 0 2 ## 6 friendship alcohol_depvar alter TRUE FALSE FALSE 0.00000 0 , ## 7 friendship alcohol_depvar ego TRUE FALSE FALSE 0.00000 0 , ## 8 friendship alcohol_depvar similarity TRUE FALSE FALSE 0.00000 0 , ## 9 alcohol_depvar rate alcohol_depvar (period 1) TRUE FALSE FALSE 0.76577 0 , ## 10 alcohol_depvar rate alcohol_depvar (period 2) TRUE FALSE FALSE 1.15233 0 , ## 11 alcohol_depvar alcohol_depvar linear shape TRUE FALSE FALSE 0.35525 0 , ## 12 alcohol_depvar alcohol_depvar quadratic shape TRUE FALSE FALSE 0.00000 0 , ## 13 alcohol_depvar alcohol_depvar average similarity TRUE FALSE FALSE 0.00000 0 , ## 14 alcohol_depvar alcohol_depvar: effect from gender TRUE FALSE FALSE 0.00000 0 , Note that we now have a timeDummy column and we see a 2 in the density and transitive triplets rows. Now let's run the model and take a look at the results. mod2_3wave_example &lt;- siena07(input_options_3wave, data = friend_behavior_covar, effects = siena_effects_3wave_example, returnDeps = TRUE, initC = TRUE, useCluster = TRUE, nbrNodes = 2) mod2_3wave_example ## Estimates, standard errors and convergence t-ratios ## ## Estimate Standard Convergence ## Error t-ratio ## Network Dynamics ## 1. rate constant friendship rate (period 1) 14.6715 ( 1.4518 ) 0.0103 ## 2. rate constant friendship rate (period 2) 11.9149 ( 1.0703 ) 0.0100 ## 3. eval outdegree (density) -2.5164 ( 0.0498 ) -0.0219 ## 4. eval reciprocity 2.0263 ( 0.0954 ) -0.0207 ## 5. eval transitive triplets 0.5675 ( 0.0234 ) -0.0120 ## 6. eval alcohol_depvar alter -0.0378 ( 0.0542 ) -0.0039 ## 7. eval alcohol_depvar ego 0.0168 ( 0.0674 ) 0.0219 ## 8. eval alcohol_depvar similarity 0.7463 ( 0.3329 ) 0.0177 ## 9. eval Dummy2:friendship ego x transitive triplets 0.1116 ( 0.0448 ) 0.0598 ## 10. eval Dummy2:friendship ego -0.0240 ( 0.0846 ) 0.0541 ## ## Behavior Dynamics ## 11. rate rate alcohol_depvar (period 1) 1.6302 ( 0.3225 ) -0.0225 ## 12. rate rate alcohol_depvar (period 2) 2.7327 ( 0.5251 ) -0.0555 ## 13. eval alcohol_depvar linear shape 0.4639 ( 0.1310 ) -0.0091 ## 14. eval alcohol_depvar quadratic shape 0.0641 ( 0.0984 ) 0.0054 ## 15. eval alcohol_depvar average similarity 7.2544 ( 2.1290 ) 0.0079 ## 16. eval alcohol_depvar: effect from gender 0.0576 ( 0.1965 ) -0.0018 ## ## Overall maximum convergence ratio: 0.1001 ## ## ## Degrees constrained to maximum values: ## friendship : 6 ## ## ## Total of 5129 iteration steps. We see that there are now interactions between time and two terms (density and transitive triplets). These two are labeled as 'friendship ego' and 'friendship ego X transitive triplets'. For example, we can see that there is some evidence that the effect of transitivity is higher in later time periods than in earlier ones. Overall, this tutorial has offered two examples of running SIENA models in R. We have covered only a portion of the full range of applications that the RSiena package can handle. For example, it is possible to use the SIENA framework to do dynamic simulations (in an analogous fashion as we saw in Chapter 13), making it possible to explore hypothetical scenarios and test theories related to the coevolution of networks and behaviors. See http://www.stats.ox.ac.uk/~snijders/siena/ for many helpful scripts and tutorials. This serves as the final tutorial for the book. Throughout the tutorials, we have included in-depth analyses of different network topics and models. Of course, a single book cannot hope to cover everything a researcher may want to do, and we encourage readers to explore the packages discussed in this book more thoroughly with their own data and project in mind. "],["how-to-cite.html", "How to Cite", " How to Cite You can cite these tutorials as: Rawlings, Craig M., Jeffrey A. Smith, James Moody, and Daniel A. McFarland 2023. Network Analysis: Integrating Social Network Theory, Method, and Application with R. New York: Cambridge University Press. You can download the reference as BibTeX or RIS. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
